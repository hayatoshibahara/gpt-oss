{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "160b36f6",
   "metadata": {},
   "source": [
    "# gpt-oss\n",
    "\n",
    "- [ãƒ¢ãƒ‡ãƒ«ã‚«ãƒ¼ãƒ‰][1]\n",
    "- [GitHub][2]\n",
    "- [ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆ][3]\n",
    "\n",
    "[1]: https://arxiv.org/abs/2508.10925\n",
    "[2]: https://github.com/openai/gpt-oss\n",
    "[3]: https://openai.com/index/introducing-gpt-oss/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d9e846",
   "metadata": {},
   "source": [
    "## å°Žå…¥"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3d83cd",
   "metadata": {},
   "source": [
    "gpt-ossã¯ã€120bã¨20bã®2ã¤ã®ã‚ªãƒ¼ãƒ—ãƒ³ã‚¦ã‚§ã‚¤ãƒˆæŽ¨è«–ãƒ¢ãƒ‡ãƒ«"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fbd37f",
   "metadata": {},
   "source": [
    "Response APIã¨äº’æ›æ€§ãŒã‚ã‚Šã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã«çµ„ã¿è¾¼ã‚ã‚‹:\n",
    "\n",
    "- é«˜ã„æŒ‡ç¤ºè¿½å¾“æ€§\n",
    "- ã‚¦ã‚§ãƒ–æ¤œç´¢ã‚„ã‚³ãƒ¼ãƒ‰å®Ÿè¡Œãªã©ã®ãƒ„ãƒ¼ãƒ«ä»•æ§˜\n",
    "- æŽ¨è«–ã®æ·±ã•ã‚’è¨­å®šã§ãã‚‹æ©Ÿèƒ½\n",
    "- CoTï¼ˆchain-of-thought, æ€è€ƒã®é€£éŽ–ï¼‰å¯¾å¿œ\n",
    "- JSONå½¢å¼ãªã©ã®å‡ºåŠ›ã«å¯¾å¿œ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98363bd6",
   "metadata": {},
   "source": [
    "Preparedness Frameworkã§å±é™ºæ€§ã‚’è©•ä¾¡:\n",
    "\n",
    "- Preparedness Frameworkã¯ã€ãƒ¢ãƒ‡ãƒ«ã®é«˜åº¦ã®èƒ½åŠ›ã‚’è¿½è·¡ã—ãƒªã‚¹ã‚¯ã‚’è¦‹ç©ã‚‚ã‚‹å·¥ç¨‹\n",
    "- ä»¥ä¸‹ã®åˆ†é‡Žã§é«˜ãƒªã‚¹ã‚¯ã§ãªã„ã“ã¨ã‚’ç¢ºèª\n",
    "    - ç”Ÿç‰©ãƒ»åŒ–å­¦ï¼ˆBiological and Chemical capabilityï¼‰\n",
    "        - ãƒ¢ãƒ‡ãƒ«ãŒç”Ÿç‰©å…µå™¨ãƒ»åŒ–å­¦å…µå™¨ã‚’åŠ©é•·ã™ã‚‹èƒ½åŠ›ã‚’æŒã¤ã‹ã©ã†ã‹\n",
    "    - ã‚µã‚¤ãƒãƒ¼ï¼ˆCyber capabilityï¼‰\n",
    "        - ã‚·ã‚¹ãƒ†ãƒ ã®æ©Ÿå¯†æ€§ãƒ»å®Œå…¨æ€§ãƒ»å¯ç”¨æ€§ã‚’ç ´å£Šã™ã‚‹èƒ½åŠ›ã‚’æŒã¤ã‹ã©ã†ã‹\n",
    "    - AIè‡ªå·±æ”¹å–„ï¼ˆAI Self-Improvementï¼‰\n",
    "        - ãƒ¢ãƒ‡ãƒ«ãŒå†å¸°çš„ãªèƒ½åŠ›æ”¹å–„ã‚’å¼•ãèµ·ã“ã™èƒ½åŠ›ã‚’æŒã¤ã‹ã©ã†ã‹"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07058729",
   "metadata": {},
   "source": [
    "## ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb70b35",
   "metadata": {},
   "source": [
    "ç‰¹å¾´:\n",
    "\n",
    "- GPT-2ãƒ»GPT-3ã‚’åŸºã«æ§‹ç¯‰\n",
    "- è‡ªå·±å›žå¸°åž‹ã®Mixture-of-Expertsï¼ˆMoEï¼‰Transformerãƒ¢ãƒ‡ãƒ«\n",
    "- gpt-oss-120b: 36å±¤\n",
    "- gpt-oss-20b: 24å±¤\n",
    "\n",
    "![](image/arch.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38eb11e4",
   "metadata": {},
   "source": [
    "äº‹å¾Œå­¦ç¿’ã§ã€MoEã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’MXFP4ã«é‡å­åŒ–ã—ã¦ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°:\n",
    "\n",
    "- MoEã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã®90%ä»¥ä¸Šã‚’å ã‚ã¦ã„ã‚‹\n",
    "- é‡å­åŒ–ã§ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚ãŸã‚Š4.25ãƒ“ãƒƒãƒˆã«å‰Šæ¸›\n",
    "    - 120bãƒ¢ãƒ‡ãƒ«ã¯å˜ä¸€ã®80GBã®VRAMã§æŽ¨è«–å¯èƒ½\n",
    "    - 20bãƒ¢ãƒ‡ãƒ«ã¯16GBã®VRAMã§æŽ¨è«–å¯èƒ½"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b560961f",
   "metadata": {},
   "source": [
    "ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è©³ç´°:\n",
    "\n",
    "- 2880æ¬¡å…ƒã®æ®‹å·®æŽ¥ç¶šï¼ˆresidual streamï¼‰ã‚’æŒã¤\n",
    "- ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãŠã‚ˆã³MoEã‚’é©ç”¨ã™ã‚‹å‰ã«RMSNormã‚’é©ç”¨ï¼ˆPre-LNï¼‰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cfd9bf",
   "metadata": {
    "vscode": {
     "languageId": "ini"
    }
   },
   "source": [
    "MoEãƒ–ãƒ­ãƒƒã‚¯:\n",
    "\n",
    "- ä¸€èˆ¬çš„ãªç·šå½¢ãƒ«ãƒ¼ã‚¿ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆlinear router projectionï¼‰ã‚’æŽ¡ç”¨\n",
    "    - 120bãƒ¢ãƒ‡ãƒ«ã¯128å€‹ã®ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆ\n",
    "    - 20bãƒ¢ãƒ‡ãƒ«ã¯32å€‹ã®ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆ\n",
    "- ãƒˆãƒ¼ã‚¯ãƒ³ã”ã¨ã«4ã¤ã®ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã‚’é¸æŠžã—ã€Softmaxã§é‡ã¿ä»˜ã‘\n",
    "- MoEãƒ–ãƒ­ãƒƒã‚¯ã¯ã€Gated SwiGLUæ´»æ€§åŒ–é–¢æ•°ã‚’æŽ¡ç”¨\n",
    "    - å€¤ã‚’ã‚¯ãƒªãƒƒãƒ—ã—ãŸã‚Šï¼ˆclampingï¼‰ã€æ®‹å·®æŽ¥ç¶šã‚’åŠ ãˆã‚‹ãªã©ã‚«ã‚¹ã‚¿ãƒžã‚¤ã‚ºã—ãŸSwiGLUã‚’ä½¿ç”¨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9ee2cb",
   "metadata": {
    "vscode": {
     "languageId": "ini"
    }
   },
   "source": [
    "ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³æ©Ÿæ§‹:\n",
    "\n",
    "- 2ç¨®é¡žã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³æ©Ÿæ§‹ã‚’äº¤äº’ã«ç¹°ã‚Šè¿”ã™\n",
    "    - ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚ºãŒ128ãƒˆãƒ¼ã‚¯ãƒ³ã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ï¼ˆbanded window patternï¼‰\n",
    "    - ãƒ•ãƒ«ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ï¼ˆfully dense patternï¼‰\n",
    "- GQAï¼ˆGrouped Query Attentionï¼‰ã§ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã¨æŽ¨è«–æ™‚é–“ã‚’å‰Šæ¸›\n",
    "    - ãƒ˜ãƒƒãƒ‰ã¯64æ¬¡å…ƒ\n",
    "    - ã‚¯ã‚¨ãƒªãƒ˜ãƒƒãƒ‰ã¯64å€‹ \n",
    "    - ã‚­ãƒ¼ãƒ»ãƒãƒªãƒ¥ãƒ¼ãƒ˜ãƒƒãƒ‰ã¯8å€‹\n",
    "- RoPEï¼ˆRotary Position Embeddingsï¼‰ã‚’ä½¿ç”¨\n",
    "- YaRNï¼ˆYet another Rope extentioNï¼‰ã§ãƒ•ãƒ«ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·ã‚’131,072ãƒˆãƒ¼ã‚¯ãƒ³ã«æ‹¡å¼µ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902bd931",
   "metadata": {},
   "source": [
    "å„ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ˜ãƒƒãƒ‰ã®Softmaxã®åˆ†æ¯ã«å­¦ç¿’å¯èƒ½ãªãƒã‚¤ã‚¢ã‚¹é …ã‚’è¿½åŠ :\n",
    "\n",
    "- [off-by-one attention][1]ã‚„[attention sinks][2]ã‹ã‚‰ç€æƒ³\n",
    "    - å¾“æ¥ã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ˜ãƒƒãƒ‰ã¯ã€Œã©ã®ãƒˆãƒ¼ã‚¯ãƒ³ã«ã‚‚æ³¨æ„ã‚’æ‰•ã‚ãªã„ã€é¸æŠžãŒã§ããªã‹ã£ãŸ\n",
    "    - ãƒã‚¤ã‚¢ã‚¹é …ã‚’åŠ ãˆã‚‹ã“ã¨ã§æ³¨æ„ã‚’æ‰•ã‚ãªã„é¸æŠžãŒå¯èƒ½ã«ãªã‚Šã€å­¦ç¿’ãŒå®‰å®šåŒ–ã™ã‚‹\n",
    "\n",
    "[1]: https://www.evanmiller.org/attention-is-off-by-one.html\n",
    "[2]: https://arxiv.org/abs/2309.17453"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23905181",
   "metadata": {},
   "source": [
    "AIMEãƒ»GPQA Diamondãƒ»HLEãƒ»MMLUãƒ™ãƒ³ãƒãƒžãƒ¼ã‚¯ã§o4-miniã«åŒ¹æ•µã™ã‚‹æ€§èƒ½:\n",
    "\n",
    "![](image/fig1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee363f9",
   "metadata": {},
   "source": [
    "## ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca44ef97",
   "metadata": {},
   "source": [
    "TikTokenãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§äº¤éš›ã•ã‚Œã¦ã„ã‚‹[o200k_harmony][1]ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ä½¿ç”¨:\n",
    "\n",
    "- BPEï¼ˆByte Pair Encodingï¼‰\n",
    "- GPT-4oã‚„o4-miniã¨åŒæ§˜\n",
    "- Harmonyãƒãƒ£ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆã«äº’æ›\n",
    "- èªžå½™æ•°ã¯201,088ãƒˆãƒ¼ã‚¯ãƒ³\n",
    "\n",
    "![](image/fig17.png)\n",
    "\n",
    "[1]: https://github.com/openai/tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c9402f",
   "metadata": {},
   "source": [
    "## äº‹å‰å­¦ç¿’"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183ee789",
   "metadata": {},
   "source": [
    "è¨“ç·´ã«æ•°å…†ãƒˆãƒ¼ã‚¯ãƒ³ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½¿ç”¨:\n",
    "\n",
    "- STEMï¼ˆç§‘å­¦ãƒ»æŠ€è¡“ãƒ»å·¥å­¦ãƒ»æ•°å­¦ï¼‰ã€ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã€ä¸€èˆ¬å¸¸è­˜\n",
    "- GPT-4oã®CBRNäº‹å‰å­¦ç¿’ãƒ•ã‚£ãƒ«ã‚¿ã‚’å†åˆ©ç”¨ã—å®‰å…¨æ€§ã‚’å‘ä¸Š\n",
    "    - CBRN: Chemicalãƒ»Biologicalãƒ»Radiodicalãƒ»Nuclear\n",
    "- çŸ¥è­˜ã‚«ãƒƒãƒˆã‚ªãƒ•ã¯2024å¹´6æœˆ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50af7c4c",
   "metadata": {},
   "source": [
    "NVIDIA H100 GPUã§è¨“ç·´:\n",
    "\n",
    "- PyTorchã¨ã€ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®æœ€é©åŒ–ã«Tritonã‚’ä½¿ç”¨\n",
    "- Flash Attentionã§ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’å‰Šæ¸›ã—ã€è¨“ç·´ã‚’é«˜é€ŸåŒ–\n",
    "- gpt-oss-120bã®è¨“ç·´ã¯ã€H100ã§210ä¸‡æ™‚é–“ã«ç›¸å½“ã™ã‚‹è¨ˆç®—é‡\n",
    "- gpt-oss-20bã®è¨“ç·´ã¯ã€H100ã§21ä¸‡æ™‚é–“ã«ç›¸å½“ã™ã‚‹è¨ˆç®—é‡"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef076066",
   "metadata": {},
   "source": [
    "## äº‹å¾Œå­¦ç¿’"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab514cd",
   "metadata": {},
   "source": [
    "äº‹å¾Œå­¦ç¿’ã¯ã€OpenAI O3ã¨åŒæ§˜ã®CoT RLæ‰‹æ³•ã§å®Ÿæ–½:\n",
    "\n",
    "- CoTã«ã‚ˆã‚‹æŽ¨è«–ã®å¼·åŒ–\n",
    "- ãƒ„ãƒ¼ãƒ«ã®ä½¿ã„æ–¹ã‚’ç¿’å¾—\n",
    "- ChatGPTã¨ä¼¼ãŸå€‹æ€§\n",
    "- ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãƒ»æ•°å­¦ãƒ»ç§‘å­¦ãªã©ã®å¹…åºƒã„åˆ†é‡Ž"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f254255",
   "metadata": {},
   "source": [
    "Harmonyãƒãƒ£ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆã‚’ä½¿ç”¨ã—ã€OpenAI APIã¨äº’æ›:\n",
    "\n",
    "- Systemãƒ­ãƒ¼ãƒ«ã‚„Developerãƒ­ãƒ¼ãƒ«ãªã©ã®ãƒ­ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã®ãƒãƒ£ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆ\n",
    "- æŒ‡ç¤ºãŒç«¶åˆã—ãŸå ´åˆã€System > Developer > User > Assistant > Toolã¨ã„ã†å„ªå…ˆé †ä½ã§å‡¦ç†\n",
    "- CoTç”¨ã®analysisãƒãƒ£ãƒãƒ«ã€é–¢æ•°ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ç”¨ã®commentaryãƒãƒ£ãƒãƒ«ã€æœ€çµ‚è§£ç­”ç”¨ã®finalãƒãƒ£ãƒãƒ«ãªã©ã‚‚å«ã‚€\n",
    "- è©³ç´°ã¯[openai/harmony](https://github.com/openai/harmony)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bc4e5c",
   "metadata": {},
   "source": [
    "å¯å¤‰ãªæŽ¨è«–ãƒ¬ãƒ™ãƒ«ã®è¨“ç·´:\n",
    "\n",
    "- `Reasoning: low` ã®ã‚ˆã†ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«è¿½åŠ ã™ã‚‹ã¨ã€CoTã®é•·ã•ã‚’å¤‰æ›´ã§ãã‚‹"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d9b0b5",
   "metadata": {},
   "source": [
    "ãƒ¢ãƒ‡ãƒ«ã«è‡ªå¾‹çš„ã«ãƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹ã‚ˆã†ã«è¨“ç·´:\n",
    "\n",
    "- ãƒ–ãƒ©ã‚¦ã‚¸ãƒ³ã‚°ãƒ„ãƒ¼ãƒ«\n",
    "    - `search`ãŠã‚ˆã³`open`é–¢æ•°ã‚’å‘¼ã³å‡ºã—ã€è§£ç­”ã®äº‹å®Ÿæ€§ã‚„çŸ¥è­˜ã®ã‚«ãƒƒãƒˆã‚ªãƒ•ã«å¯¾å‡¦\n",
    "- Pythonãƒ„ãƒ¼ãƒ«\n",
    "    - ã‚¹ãƒ†ãƒ¼ãƒˆãƒ•ãƒ«ãªJupyter Notebookç’°å¢ƒã§ã‚³ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œ\n",
    "- ä»»æ„ã®ã‚¹ã‚­ãƒ¼ãƒžã®ãƒ„ãƒ¼ãƒ«\n",
    "    - OpenAI APIã¨åŒæ§˜ã«ä»»æ„ã®é–¢æ•°ã®å®Ÿè¡Œ\n",
    "    - è©³ç´°ã¯[openai/gpt-oss](https://github.com/openai/gpt-oss)\n",
    "\n",
    "![](image/fig18.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7624c9",
   "metadata": {},
   "source": [
    "## è©•ä¾¡"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4832533",
   "metadata": {},
   "source": [
    "æŽ¨è«–ãƒ¬ãƒ™ãƒ«ã‚’ã€Œé«˜ã€ã«ã—ã¦pass@1ã§è©•ä¾¡:\n",
    "\n",
    "- æŽ¨è«–ã¨äº‹å®Ÿæ€§\n",
    "    - AIMEï¼ˆæ•°å­¦ã‚³ãƒ³ãƒ†ã‚¹ãƒˆï¼‰\n",
    "    - GPQAï¼ˆåšå£«ãƒ¬ãƒ™ãƒ«ã®ç§‘å­¦ï¼‰\n",
    "    - MMLUï¼ˆå¤§å­¦ãƒ¬ãƒ™ãƒ«è©¦é¨“ï¼‰\n",
    "    - HLEï¼ˆå°‚é–€å®¶ãƒ¬ãƒ™ãƒ«è©¦é¨“ï¼‰\n",
    "- ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°\n",
    "    - Codex CLIã¨ä¼¼ãŸãƒ„ãƒ¼ãƒ«ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹ã®æœ‰ç„¡ã‚’åˆ†ã‘ã¦è©•ä¾¡\n",
    "    - Codeforces Eloï¼ˆç«¶æŠ€ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ï¼‰\n",
    "    - SWE-bench Verifiedï¼ˆã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢å·¥å­¦ï¼‰\n",
    "- ãƒ„ãƒ¼ãƒ«ä½¿ç”¨\n",
    "    - $\\tau$-Bench Retailï¼ˆå°å£²æ¥­ã‚¿ã‚¹ã‚¯ï¼‰\n",
    "- ãã®ä»–\n",
    "    - MMMLUï¼ˆå¤šè¨€èªžã®å¤§å­¦ãƒ¬ãƒ™ãƒ«è©¦é¨“ï¼‰\n",
    "    - HealthBenchï¼ˆå¥åº·åˆ†é‡Žï¼‰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201d61d3",
   "metadata": {},
   "source": [
    "ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã¨ãƒ„ãƒ¼ãƒ«ä½¿ç”¨ã«ãŠã‘ã‚‹ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã¯o4-miniã«åŒ¹æ•µã™ã‚‹æ€§èƒ½:\n",
    "\n",
    "![](image/fig2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624a550a",
   "metadata": {},
   "source": [
    "æŽ¨è«–ãƒ¬ãƒ™ãƒ«ã‚’é«˜ãã™ã‚‹ã¨ã€CoTãŒé•·ããªã‚Šå›žç­”ã®ç²¾åº¦ãŒä¸ŠãŒã‚‹:\n",
    "\n",
    "![](image/fig3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2d3b99",
   "metadata": {},
   "source": [
    "æŽ¨è«–ãƒ¬ãƒ™ãƒ«ãŒé«˜ã„gpt-oss-120bã¯ã€OpenAI o3ã«åŒ¹æ•µã™ã‚‹æ€§èƒ½:\n",
    "\n",
    "![](image/fig4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83d67aa",
   "metadata": {},
   "source": [
    "å¤šè¨€èªžæ€§èƒ½ã¯æŽ¨è«–ãƒ¬ãƒ™ãƒ«ã®é«˜ã„gpt-oss-120bã§ã€o3-miniç¨‹åº¦ã®æ€§èƒ½:\n",
    "\n",
    "![](image/table2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d948518c",
   "metadata": {},
   "source": [
    "å…¨ä½“çš„ã«ã¯æŽ¨è«–ãƒ¬ãƒ™ãƒ«ãŒä¸ŠãŒã‚‹ã¨æ€§èƒ½ãŒå‘ä¸Š:\n",
    "\n",
    "![](image/table3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9942a82",
   "metadata": {},
   "source": [
    "## å®‰å…¨æ€§ãƒ†ã‚¹ãƒˆã¨ãƒªã‚¹ã‚¯ç·©å’Œ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8491d486",
   "metadata": {},
   "source": [
    "ã€Œæ‚ªæ„ã®ã‚ã‚‹é–‹ç™ºè€…ãŒãƒ¢ãƒ‡ãƒ«ã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹å¯èƒ½æ€§ã€ã‚’è€ƒæ…®ã—ã¦ãƒ†ã‚¹ãƒˆ\n",
    "\n",
    "Preparedness Frameworkã§ã€ç”Ÿç‰©ãƒ»ç§‘å­¦ã€ã‚µã‚¤ãƒãƒ¼ã€è‡ªå·±æ”¹å–„ã®åˆ†é‡Žã§é«˜ã„ãƒªã‚¹ã‚¯ã§ã¯ãªã„ã“ã¨ã‚’ç¢ºèª\n",
    "\n",
    "æ›´ã«2ã¤ã®è¦³ç‚¹ã§èª¿æŸ»:\n",
    "\n",
    "1. æ‚ªæ„ã®ã‚ã‚‹é–‹ç™ºè€…ãŒgpt-oss-120bã‚’è¿½åŠ å­¦ç¿’ã—ã¦ã€ç”Ÿç‰©ãƒ»ç§‘å­¦ã€ã‚µã‚¤ãƒãƒ¼é ˜åŸŸã§é«˜ã„èƒ½åŠ›ã‚’å®Ÿç¾ã§ãã‚‹ã‹\n",
    "    - SAGï¼ˆSafety Advisory Groupï¼‰ãŒãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’è¡Œã£ã¦ã‚‚å•é¡Œãªã‹ã£ãŸ\n",
    "2. gpt-oss-120bã‚’ãƒªãƒªãƒ¼ã‚¹ã™ã‚‹ã“ã¨ã§ã€ç”Ÿç‰©åˆ†é‡Žã®ç ”ç©¶ã‚’è‘—ã—ãå‰é€²ã§ãã‚‹ã‹\n",
    "    - åŒç­‰æ€§èƒ½ã®ã‚ªãƒ¼ãƒ—ãƒ³ã‚¦ã‚§ã‚¤ãƒˆãƒ¢ãƒ‡ãƒ«ãŒã™ã§ã«å­˜åœ¨ã™ã‚‹ã®ã§å®Ÿç¾å¯èƒ½æ€§ã¯ä½Žã„"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20945183",
   "metadata": {},
   "source": [
    "## ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦ã„ãªã„ãƒ¢ãƒ‡ãƒ«ã®å®‰å…¨æ€§"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3f41a6",
   "metadata": {},
   "source": [
    "æ¨™æº–çš„ãªãƒ™ãƒ³ãƒãƒžãƒ¼ã‚¯ã«ã‚ˆã‚‹ä¸è¨±å¯ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã«å¯¾ã™ã‚‹å®‰å…¨æ€§ã¯é£½å’Œ:\n",
    "\n",
    "![](image/table4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a05eb7",
   "metadata": {},
   "source": [
    "ãƒžãƒ«ãƒã‚¿ãƒ¼ãƒ³ã®ãƒ™ãƒ³ãƒãƒžãƒ¼ã‚¯ã§ã¯ã€o4-miniä»¥ä¸Šã®æ€§èƒ½:\n",
    "\n",
    "![](image/table5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28016c32",
   "metadata": {},
   "source": [
    "[StrongReject][1]ã«ã‚ˆã‚‹ã‚¸ã‚§ã‚¤ãƒ«ãƒ–ãƒ¬ã‚¤ã‚¯ã«å¯¾ã™ã‚‹ãƒ™ãƒ³ãƒãƒžãƒ¼ã‚¯ã§ã¯ã€o4-miniã¨åŒç­‰ã®æ€§èƒ½:\n",
    "\n",
    "![](image/table6.png)\n",
    "\n",
    "[1]: https://proceedings.neurips.cc/paper_files/paper/2024/hash/e2e06adf560b0706d3b1ddfca9f29756-Abstract-Datasets_and_Benchmarks_Track.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f31bf31",
   "metadata": {},
   "source": [
    "ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æŠ½å‡ºã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¤ãƒ³ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã«å¯¾ã™ã‚‹å …ç‰¢æ€§ã¯o4-miniã‚ˆã‚Šå¼±ã„:\n",
    "\n",
    "![](image/table7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a7b4a3",
   "metadata": {},
   "source": [
    "é–‹ç™ºè€…ãƒ­ãƒ¼ãƒ«ã¨ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ­ãƒ¼ãƒ«é–“ã®ãƒ•ãƒ¬ãƒ¼ã‚ºä¿è­·ã«å¯¾ã™ã‚‹å …ç‰¢æ€§ã¯ä½Žã„:\n",
    "\n",
    "![](image/table8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e082072",
   "metadata": {},
   "source": [
    "ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³ã¯ã€å†…éƒ¨ã«ä¸–ç•ŒçŸ¥è­˜ãŒå°‘ãªã„gpt-oss-20bã¯èµ·ã“ã—ã‚„ã™ã„:\n",
    "\n",
    "![](image/table9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d893a5c",
   "metadata": {},
   "source": [
    "åè¦‹ã«å¯¾ã™ã‚‹è©•ä¾¡ã¯o4-miniã¨åŒç­‰:\n",
    "\n",
    "![](image/table10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a494cdae",
   "metadata": {},
   "source": [
    "## Preparedness Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221194c2",
   "metadata": {},
   "source": [
    "Preparedness Frameworkã¯ã€ãƒ¢ãƒ‡ãƒ«ã‚’å…¬é–‹ã™ã‚‹ã“ã¨ã«ã‚ˆã‚‹æ·±åˆ»ãªãƒªã‚¹ã‚¯ã‚’è¿½è·¡ã—æœ€å°é™ã«ã™ã‚‹ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯\n",
    "\n",
    "ç”Ÿç‰©ãƒ»ç§‘å­¦é ˜åŸŸã¨ã‚µã‚¤ãƒãƒ¼é ˜åŸŸã§ã¯é«˜ãƒªã‚¹ã‚¯ã®å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚ã€æ•µå¯¾çš„ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆAFTï¼‰ã‚’å®Ÿæ–½:\n",
    "\n",
    "- Helpful-onlyè¨“ç·´\n",
    "    - ãƒ¢ãƒ‡ãƒ«ã®ã‚¬ãƒ¼ãƒ‰ãƒ¬ãƒ¼ãƒ«ã‚’æ„å›³çš„ã«è§£é™¤ã™ã‚‹ãŸã‚ã®å¼·åŒ–å­¦ç¿’\n",
    "- ç”Ÿç‰©ãƒ»ã‚µã‚¤ãƒãƒ¼é ˜åŸŸã«ãŠã‘ã‚‹èƒ½åŠ›ã®æœ€å¤§åŒ–\n",
    "    - ç”Ÿç‰©é ˜åŸŸã¯ã€äººé–“ã®å°‚é–€å®¶ã‚’æ´»ç”¨ã—ã¦è¨“ç·´\n",
    "        - è³ªå•å›žç­”ã§ã¯é«˜ã„æ€§èƒ½ã«é”ã—ãŸãŒã€å®Ÿé¨“ã®ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ãƒ»å®Ÿé¨“ã®æš—é»™çŸ¥ã«é–¢ã—ã¦ã¯é”ã—ãªã‹ã£ãŸ\n",
    "        - Qwen3ã‚„Kimi K2ã¯ãã‚Œã¨åŒç­‰ã®èƒ½åŠ›ã‚’æŒã£ã¦ã„ã‚‹ãŸã‚ã€ãƒªãƒªãƒ¼ã‚¹ã—ã¦ã‚‚ãƒªã‚¹ã‚¯ã¯å°‘ãªã„\n",
    "    - ã‚µã‚¤ãƒãƒ¼é ˜åŸŸã¯ã€CTFï¼ˆã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ç«¶æŠ€ï¼‰ãƒ‡ãƒ¼ã‚¿ã‚’æ´»ç”¨ã—ã¦è¨“ç·´"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808538b9",
   "metadata": {},
   "source": [
    "ç”Ÿç‰©é ˜åŸŸã§ã®å±é™ºæ€§ã®5æ®µéšŽã§è©•ä¾¡ã—ãŸã¨ã“ã‚ã€Helpful-onlyè¨“ç·´ã—ãŸãƒ¢ãƒ‡ãƒ«ã¯ä»–ã®ãƒ¢ãƒ‡ãƒ«ã¨åŒç­‰ã®æ€§èƒ½:\n",
    "\n",
    "1. Ideationï¼ˆã‚¢ã‚¤ãƒ‡ã‚¢å‡ºã—ï¼‰\n",
    "2. Acquisitionï¼ˆå…¥æ‰‹ï¼‰\n",
    "3. Magnificationï¼ˆå¢—å¼·ï¼‰\n",
    "4. Formulationï¼ˆè£½å‰¤åŒ–ï¼‰\n",
    "5. Releaseï¼ˆæ”¾å‡ºï¼‰\n",
    "\n",
    "![](image/fig5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c32df2",
   "metadata": {},
   "source": [
    "ãƒžãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãªã‚¦ã‚£ãƒ«ã‚¹å­¦åˆ†é‡Žã§ã®æ€§èƒ½ã¯ã€o3ã«åŠ£ã‚‹æ€§èƒ½ï¼ˆãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ–ãƒ­ãƒƒã‚¯ãƒ»ãƒ–ãƒ©ã‚¦ã‚¸ãƒ³ã‚°ã‚ã‚Šï¼‰:\n",
    "\n",
    "![](image/fig6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310ed3b5",
   "metadata": {},
   "source": [
    "å®Ÿé¨“ã®ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã«é–¢ã™ã‚‹ãƒ™ãƒ³ãƒãƒžãƒ¼ã‚¯ã§ã¯ã€o3ã«åŠ£ã‚‹æ€§èƒ½:\n",
    "\n",
    "![](image/fig7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d00177f",
   "metadata": {},
   "source": [
    "å®Ÿé¨“ã®æš—é»™çŸ¥ã¨ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã«é–¢ã™ã‚‹åˆ¥ã®ãƒ™ãƒ³ãƒãƒžãƒ¼ã‚¯ã§ã¯ã€o3ã«åŠ£ã‚‹æ€§èƒ½:\n",
    "\n",
    "![](image/fig8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69a7925",
   "metadata": {},
   "source": [
    "å¤§å­¦ç”Ÿãƒ¬ãƒ™ãƒ«ã®CTFï¼ˆCapture the Flag, ã‚µã‚¤ãƒãƒ¼ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ï¼‰ã§ã¯ã€o3ã«åŠ£ã‚‹æ€§èƒ½:\n",
    "\n",
    "![](image/fig10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d724ed",
   "metadata": {},
   "source": [
    "ãƒ—ãƒ­ãƒ¬ãƒ™ãƒ«ã®CTFãƒ™ãƒ³ãƒãƒžãƒ¼ã‚¯ã§ã¯ã€CTFå•é¡Œã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸãƒ¢ãƒ‡ãƒ«ãŒo3ã«åŒ¹æ•µã™ã‚‹æ€§èƒ½:\n",
    "\n",
    "![](image/fig11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac62eb6",
   "metadata": {},
   "source": [
    "ã‚µã‚¤ãƒãƒ¼æ”»æ’ƒã«é–¢ã™ã‚‹ãƒ™ãƒ³ãƒãƒžãƒ¼ã‚¯ã§ã¯ã€ã„ãšã‚Œã®ã‚·ãƒŠãƒªã‚ªã®è§£æ±ºã«ã‚‚è‡³ã‚‰ãªã‹ã£ãŸ:\n",
    "\n",
    "![](image/fig12.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92e3c30",
   "metadata": {},
   "source": [
    "AIã®è‡ªå·±æ”¹å–„ãƒªã‚¹ã‚¯ã§ã€GitHubã®ãƒã‚°ä¿®æ­£ã¯o4-miniã«åŠ£ã‚‹æ€§èƒ½:\n",
    "\n",
    "![](image/fig19.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128b59af",
   "metadata": {},
   "source": [
    "æœ€å…ˆç«¯ã®AIã®ç ”ç©¶ã®å†ç¾ã™ã‚‹èƒ½åŠ›ã¯ã€o4-miniã«åŠ£ã‚‹æ€§èƒ½:\n",
    "\n",
    "![](image/fig16.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e40d14",
   "metadata": {},
   "source": [
    "## å®Ÿè£…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9780cbdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸŸ© Transformers version: 4.57.1\n",
      "ðŸŸ© Numpy version: 2.1.2\n",
      "ðŸŸ© BitsAndBytes version: 0.48.1\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU transformers==4.57.1 kernels\n",
    "\n",
    "import os\n",
    "import logging as logging_\n",
    "import transformers\n",
    "import bitsandbytes\n",
    "from transformers import PretrainedConfig\n",
    "from transformers.utils import logging\n",
    "\n",
    "from typing import Callable, Optional, Union\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼\n",
    "\n",
    "import os\n",
    "from shutil import copyfile\n",
    "from typing import TYPE_CHECKING, Any, Optional\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«\n",
    "\n",
    "from typing import Callable, Optional, Union\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "assert torch.cuda.is_available(), \"CUDAã‚’ä½¿ç”¨ã§ãã¾ã›ã‚“\"\n",
    "\n",
    "from typing import Callable, Optional, Union\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from transformers.cache_utils import Cache, DynamicCache\n",
    "from transformers.generation import GenerationMixin\n",
    "from transformers.integrations.hub_kernels import use_kernel_forward_from_hub\n",
    "from transformers.masking_utils import create_causal_mask, create_sliding_window_causal_mask\n",
    "from transformers.modeling_layers import (\n",
    "    GenericForSequenceClassification,\n",
    "    GenericForTokenClassification,\n",
    "    GradientCheckpointingLayer,\n",
    ")\n",
    "from transformers.modeling_outputs import MoeCausalLMOutputWithPast, MoeModelOutputWithPast\n",
    "from transformers.modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n",
    "from transformers.modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n",
    "from transformers.processing_utils import Unpack\n",
    "from transformers.utils import TransformersKwargs, auto_docstring, can_return_tuple\n",
    "from transformers.utils.deprecation import deprecate_kwarg\n",
    "from transformers.utils.generic import OutputRecorder, check_model_inputs\n",
    "from transformers.models.gpt_oss.configuration_gpt_oss import GptOssConfig\n",
    "\n",
    "# ãƒ‡ãƒã‚¤ã‚¹è¨­å®š\n",
    "\n",
    "torch.set_default_device(\"cuda\")\n",
    "\n",
    "# ãƒ­ã‚°è¨­å®š\n",
    "\n",
    "if os.path.exists('debug.log'):\n",
    "    os.remove('debug.log')\n",
    "\n",
    "def custom_format(record):\n",
    "    match record.levelno:\n",
    "        case logging_.DEBUG:\n",
    "            level = 'ðŸŸ¦'\n",
    "        case logging_.INFO:\n",
    "            level = 'ðŸŸ©'\n",
    "        case logging_.WARNING:\n",
    "            level = 'ðŸŸ¨'\n",
    "        case logging_.ERROR:\n",
    "            level = 'ðŸŸ¥'\n",
    "        case logging_.CRITICAL:\n",
    "            level = 'ðŸ›‘'\n",
    "    return f\"{level} {record.getMessage()}\"\n",
    "\n",
    "logging.set_verbosity_debug()\n",
    "logger = logging.get_logger()\n",
    "\n",
    "for handler in logger.handlers:\n",
    "    logger.removeHandler(handler)\n",
    "\n",
    "formatter = logging_.Formatter()\n",
    "formatter.format = custom_format\n",
    "\n",
    "file_handler = logging_.FileHandler('debug.log')\n",
    "file_handler.setFormatter(formatter)\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "stream_handler = logging_.StreamHandler()\n",
    "stream_handler.setFormatter(formatter)\n",
    "logger.addHandler(stream_handler)\n",
    "\n",
    "logger.info(f\"Transformers version: {transformers.__version__}\")\n",
    "logger.info(f\"Numpy version: {np.__version__}\")\n",
    "logger.info(f\"BitsAndBytes version: {bitsandbytes.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81e8b37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@use_kernel_forward_from_hub(\"RMSNorm\")\n",
    "class GptOssRMSNorm(nn.Module):\n",
    "    def __init__(self, hidden_size, eps=1e-6):\n",
    "        \"\"\"\n",
    "        GptOssRMSNorm is equivalent to T5LayerNorm\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(hidden_size))\n",
    "        self.variance_epsilon = eps\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        input_dtype = hidden_states.dtype\n",
    "        hidden_states = hidden_states.to(torch.float32)\n",
    "        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
    "        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
    "        return (self.weight * hidden_states).to(input_dtype)  # main diff with Llama\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5399eb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GptOssExperts(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.intermediate_size = config.intermediate_size\n",
    "        self.num_experts = config.num_local_experts\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.expert_dim = self.intermediate_size\n",
    "        self.gate_up_proj = nn.Parameter(torch.empty(self.num_experts, self.hidden_size, 2 * self.expert_dim))\n",
    "        self.gate_up_proj_bias = nn.Parameter(torch.empty(self.num_experts, 2 * self.expert_dim))\n",
    "        self.down_proj = nn.Parameter(torch.empty((self.num_experts, self.expert_dim, self.hidden_size)))\n",
    "        self.down_proj_bias = nn.Parameter(torch.empty(self.num_experts, self.hidden_size))\n",
    "        self.alpha = 1.702\n",
    "        self.limit = 7.0\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor, router_indices=None, routing_weights=None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        When training it is more efficient to just loop over the experts and compute the output for each expert\n",
    "        as otherwise the memory would explode.\n",
    "\n",
    "        For inference we can sacrifice some memory and compute the output for all experts at once. By repeating the inputs.\n",
    "\n",
    "        Args:\n",
    "            hidden_states (torch.Tensor): (batch_size, seq_len, hidden_size)\n",
    "            selected_experts (torch.Tensor): (batch_size * token_num, top_k)\n",
    "            routing_weights (torch.Tensor): (batch_size * token_num, num_experts)\n",
    "        Returns:\n",
    "            torch.Tensor\n",
    "        \"\"\"\n",
    "        batch_size = hidden_states.shape[0]\n",
    "        hidden_states = hidden_states.reshape(-1, self.hidden_size)  # (num_tokens, hidden_size)\n",
    "        num_experts = routing_weights.shape[1]\n",
    "        if hidden_states.device.type == \"cpu\" or self.training:\n",
    "            next_states = torch.zeros_like(hidden_states, dtype=hidden_states.dtype, device=hidden_states.device)\n",
    "            with torch.no_grad():\n",
    "                expert_mask = torch.nn.functional.one_hot(\n",
    "                    router_indices, num_classes=num_experts + 1\n",
    "                )  # masking is also a class\n",
    "                expert_mask = expert_mask.permute(2, 1, 0)\n",
    "                # we sum on the top_k and on the sequence length to get which experts\n",
    "                # are hit this time around\n",
    "                expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n",
    "            for expert_idx in expert_hit[:]:\n",
    "                # expert_idx only have 1 element, so we can use scale for fast indexing\n",
    "                expert_idx = expert_idx[0]\n",
    "                # skip masking index\n",
    "                if expert_idx == num_experts:\n",
    "                    continue\n",
    "                with torch.no_grad():\n",
    "                    _, token_idx = torch.where(expert_mask[expert_idx])\n",
    "                current_state = hidden_states[token_idx]\n",
    "                gate_up = current_state @ self.gate_up_proj[expert_idx] + self.gate_up_proj_bias[expert_idx]\n",
    "                gate, up = gate_up[..., ::2], gate_up[..., 1::2]\n",
    "                gate = gate.clamp(min=None, max=self.limit)\n",
    "                up = up.clamp(min=-self.limit, max=self.limit)\n",
    "                glu = gate * torch.sigmoid(gate * self.alpha)\n",
    "                gated_output = (up + 1) * glu\n",
    "                out = gated_output @ self.down_proj[expert_idx] + self.down_proj_bias[expert_idx]\n",
    "                weighted_output = out * routing_weights[token_idx, expert_idx, None]\n",
    "                next_states.index_add_(0, token_idx, weighted_output.to(hidden_states.dtype))\n",
    "            next_states = next_states.view(batch_size, -1, self.hidden_size)\n",
    "        else:\n",
    "            hidden_states = hidden_states.repeat(num_experts, 1)\n",
    "            hidden_states = hidden_states.view(num_experts, -1, self.hidden_size)\n",
    "            gate_up = torch.bmm(hidden_states, self.gate_up_proj) + self.gate_up_proj_bias[..., None, :]\n",
    "            gate, up = gate_up[..., ::2], gate_up[..., 1::2]\n",
    "            gate = gate.clamp(min=None, max=self.limit)\n",
    "            up = up.clamp(min=-self.limit, max=self.limit)\n",
    "            glu = gate * torch.sigmoid(gate * self.alpha)\n",
    "            next_states = torch.bmm(((up + 1) * glu), self.down_proj)\n",
    "            next_states = next_states + self.down_proj_bias[..., None, :]\n",
    "            next_states = next_states.view(num_experts, batch_size, -1, self.hidden_size)\n",
    "            next_states = next_states * routing_weights.transpose(0, 1).view(num_experts, batch_size, -1)[..., None]\n",
    "            next_states = next_states.sum(dim=0)\n",
    "        return next_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b6b8a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GptOssTopKRouter(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.top_k = config.num_experts_per_tok\n",
    "        self.num_experts = config.num_local_experts\n",
    "        self.hidden_dim = config.hidden_size\n",
    "        self.weight = nn.Parameter(torch.empty(self.num_experts, self.hidden_dim))\n",
    "        self.bias = nn.Parameter(torch.empty(self.num_experts))\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = hidden_states.reshape(-1, self.hidden_dim)\n",
    "        router_logits = F.linear(hidden_states, self.weight, self.bias)  # (seq_len, num_experts)\n",
    "        router_top_value, router_indices = torch.topk(router_logits, self.top_k, dim=-1)  # (seq_len, top_k)\n",
    "        router_top_value = torch.nn.functional.softmax(router_top_value, dim=1, dtype=router_top_value.dtype)\n",
    "        router_scores = torch.zeros_like(router_logits).scatter_(1, router_indices, router_top_value)\n",
    "        return router_scores, router_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2bc91ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "@use_kernel_forward_from_hub(\"MegaBlocksMoeMLP\")\n",
    "class GptOssMLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.router = GptOssTopKRouter(config)\n",
    "        self.experts = GptOssExperts(config)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        router_scores, router_indices = self.router(hidden_states)  # (num_experts, seq_len)\n",
    "        routed_out = self.experts(hidden_states, router_indices=router_indices, routing_weights=router_scores)\n",
    "        return routed_out, router_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62bc22db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GptOssRotaryEmbedding(nn.Module):\n",
    "    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n",
    "\n",
    "    def __init__(self, config: GptOssConfig, device=None):\n",
    "        super().__init__()\n",
    "        # BC: \"rope_type\" was originally \"type\"\n",
    "        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n",
    "            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n",
    "        else:\n",
    "            self.rope_type = \"default\"\n",
    "        self.max_seq_len_cached = config.max_position_embeddings\n",
    "        self.original_max_seq_len = config.max_position_embeddings\n",
    "\n",
    "        self.config = config\n",
    "        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n",
    "\n",
    "        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n",
    "        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n",
    "        self.original_inv_freq = self.inv_freq\n",
    "\n",
    "    @torch.no_grad()\n",
    "    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n",
    "    def forward(self, x, position_ids):\n",
    "        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n",
    "        position_ids_expanded = position_ids[:, None, :].float()\n",
    "\n",
    "        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n",
    "        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n",
    "            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n",
    "            emb = freqs\n",
    "            cos = emb.cos() * self.attention_scaling\n",
    "            sin = emb.sin() * self.attention_scaling\n",
    "\n",
    "        return cos.to(x.dtype), sin.to(x.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5bdc1dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n",
    "    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n",
    "    \"\"\"\n",
    "    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n",
    "    if n_rep == 1:\n",
    "        return hidden_states\n",
    "    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n",
    "    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28c149ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _apply_rotary_emb(\n",
    "    x: torch.Tensor,\n",
    "    cos: torch.Tensor,\n",
    "    sin: torch.Tensor,\n",
    ") -> torch.Tensor:\n",
    "    first_half, second_half = torch.chunk(x, 2, dim=-1)\n",
    "    first_ = first_half * cos - second_half * sin\n",
    "    second_ = second_half * cos + first_half * sin\n",
    "    return torch.cat((first_, second_), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14294bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n",
    "    cos = cos.unsqueeze(unsqueeze_dim)\n",
    "    sin = sin.unsqueeze(unsqueeze_dim)\n",
    "    q_embed = _apply_rotary_emb(q, cos, sin)\n",
    "    k_embed = _apply_rotary_emb(k, cos, sin)\n",
    "    return q_embed, k_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e916936",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eager_attention_forward(\n",
    "    module: nn.Module,\n",
    "    query: torch.Tensor,\n",
    "    key: torch.Tensor,\n",
    "    value: torch.Tensor,\n",
    "    attention_mask: Optional[torch.Tensor],\n",
    "    scaling: float,\n",
    "    dropout: float = 0.0,\n",
    "    **kwargs,\n",
    "):\n",
    "    key_states = repeat_kv(key, module.num_key_value_groups)\n",
    "    value_states = repeat_kv(value, module.num_key_value_groups)\n",
    "    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling\n",
    "    if attention_mask is not None:\n",
    "        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n",
    "        attn_weights = attn_weights + causal_mask\n",
    "\n",
    "    sinks = module.sinks.reshape(1, -1, 1, 1).expand(query.shape[0], -1, query.shape[-2], -1)\n",
    "    combined_logits = torch.cat([attn_weights, sinks], dim=-1)\n",
    "\n",
    "    # This was not in the original implementation and slightly affect results; it prevents overflow in BF16/FP16\n",
    "    # when training with bsz>1 we clamp max values.\n",
    "\n",
    "    combined_logits = combined_logits - combined_logits.max(dim=-1, keepdim=True).values\n",
    "    probs = F.softmax(combined_logits, dim=-1, dtype=combined_logits.dtype)\n",
    "    scores = probs[..., :-1]  # we drop the sink here\n",
    "    attn_weights = nn.functional.dropout(scores, p=dropout, training=module.training)\n",
    "    attn_output = torch.matmul(attn_weights, value_states)\n",
    "    attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "    return attn_output, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "103b2cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GptOssAttention(nn.Module):\n",
    "    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n",
    "\n",
    "    def __init__(self, config: GptOssConfig, layer_idx: int):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.layer_idx = layer_idx\n",
    "        self.head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n",
    "        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads\n",
    "        self.scaling = self.head_dim**-0.5\n",
    "        self.attention_dropout = config.attention_dropout\n",
    "        self.is_causal = True\n",
    "        self.q_proj = nn.Linear(\n",
    "            config.hidden_size, config.num_attention_heads * self.head_dim, bias=config.attention_bias\n",
    "        )\n",
    "        self.k_proj = nn.Linear(\n",
    "            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n",
    "        )\n",
    "        self.v_proj = nn.Linear(\n",
    "            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n",
    "        )\n",
    "        self.o_proj = nn.Linear(\n",
    "            config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n",
    "        )\n",
    "        self.sliding_window = config.sliding_window if config.layer_types[layer_idx] == \"sliding_attention\" else None\n",
    "        self.sinks = nn.Parameter(torch.empty(config.num_attention_heads))\n",
    "\n",
    "    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n",
    "        attention_mask: Optional[torch.Tensor],\n",
    "        past_key_values: Optional[Cache] = None,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "        **kwargs: Unpack[TransformersKwargs],\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        input_shape = hidden_states.shape[:-1]\n",
    "        hidden_shape = (*input_shape, -1, self.head_dim)\n",
    "\n",
    "        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
    "        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
    "        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
    "\n",
    "        cos, sin = position_embeddings\n",
    "        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n",
    "\n",
    "        if past_key_values is not None:\n",
    "            cache_kwargs = {\"cache_position\": cache_position}\n",
    "            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n",
    "\n",
    "        attention_interface: Callable = eager_attention_forward\n",
    "        if self.config._attn_implementation != \"eager\":\n",
    "            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n",
    "\n",
    "        attn_output, attn_weights = attention_interface(\n",
    "            self,\n",
    "            query_states,\n",
    "            key_states,\n",
    "            value_states,\n",
    "            attention_mask,\n",
    "            dropout=0.0 if not self.training else self.attention_dropout,\n",
    "            scaling=self.scaling,\n",
    "            sliding_window=self.sliding_window,\n",
    "            s_aux=self.sinks,  # diff with Llama\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n",
    "        attn_output = self.o_proj(attn_output)\n",
    "        return attn_output, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd90c5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GptOssDecoderLayer(GradientCheckpointingLayer):\n",
    "    def __init__(self, config: GptOssConfig, layer_idx: int):\n",
    "        super().__init__()\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.self_attn = GptOssAttention(config=config, layer_idx=layer_idx)\n",
    "        self.mlp = GptOssMLP(config)\n",
    "        self.input_layernorm = GptOssRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "        self.post_attention_layernorm = GptOssRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "        self.attention_type = config.layer_types[layer_idx]\n",
    "\n",
    "    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[Cache] = None,\n",
    "        use_cache: Optional[bool] = False,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n",
    "        **kwargs: Unpack[TransformersKwargs],\n",
    "    ) -> torch.Tensor:\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.input_layernorm(hidden_states)\n",
    "        # Self Attention\n",
    "        hidden_states, _ = self.self_attn(\n",
    "            hidden_states=hidden_states,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            use_cache=use_cache,\n",
    "            cache_position=cache_position,\n",
    "            position_embeddings=position_embeddings,\n",
    "            **kwargs,\n",
    "        )\n",
    "        hidden_states = residual + hidden_states\n",
    "\n",
    "        # Fully Connected\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.post_attention_layernorm(hidden_states)\n",
    "        hidden_states, _ = self.mlp(hidden_states)  # diff with llama: router scores\n",
    "        hidden_states = residual + hidden_states\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d2813bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GptOssPreTrainedModel(PreTrainedModel):\n",
    "    config: GptOssConfig\n",
    "    base_model_prefix = \"model\"\n",
    "    supports_gradient_checkpointing = True\n",
    "    _no_split_modules = [\"GptOssDecoderLayer\"]\n",
    "    _skip_keys_device_placement = [\"past_key_values\"]\n",
    "    _supports_flash_attn = True\n",
    "    _supports_sdpa = False\n",
    "    _supports_flex_attn = True\n",
    "\n",
    "    _can_compile_fullgraph = True\n",
    "    _supports_attention_backend = True\n",
    "    _can_record_outputs = {\n",
    "        \"router_logits\": OutputRecorder(GptOssTopKRouter, index=0),\n",
    "        \"hidden_states\": GptOssDecoderLayer,\n",
    "        \"attentions\": GptOssAttention,\n",
    "    }\n",
    "    _keep_in_fp32_modules = [\"post_attention_layernorm\", \"input_layernorm\", \"norm\"]\n",
    "    _supports_flash_attention = False\n",
    "    _supports_flex_attention = False\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        std = self.config.initializer_range\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=std)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Parameter):\n",
    "            module.data.normal_(mean=0.0, std=std)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=std)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, GptOssRMSNorm):\n",
    "            module.weight.data.fill_(1.0)\n",
    "        elif isinstance(module, GptOssExperts):\n",
    "            module.gate_up_proj.data.normal_(mean=0.0, std=std)\n",
    "            module.gate_up_proj_bias.data.zero_()\n",
    "            module.down_proj.data.normal_(mean=0.0, std=std)\n",
    "            module.down_proj_bias.data.zero_()\n",
    "        elif isinstance(module, GptOssAttention):\n",
    "            module.sinks.data.normal_(mean=0.0, std=std)\n",
    "        elif isinstance(module, GptOssTopKRouter):\n",
    "            module.weight.data.normal_(mean=0.0, std=std)\n",
    "            module.bias.data.normal_(mean=0.0, std=std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f0d7305e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GptOssModel(GptOssPreTrainedModel):\n",
    "    _no_split_modules = [\"GptOssDecoderLayer\"]\n",
    "\n",
    "    def __init__(self, config: GptOssConfig):\n",
    "        super().__init__(config)\n",
    "        self.padding_idx = config.pad_token_id\n",
    "        self.vocab_size = config.vocab_size\n",
    "\n",
    "        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n",
    "        self.layers = nn.ModuleList(\n",
    "            [GptOssDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n",
    "        )\n",
    "        self.norm = GptOssRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "        self.rotary_emb = GptOssRotaryEmbedding(config=config)\n",
    "        self.gradient_checkpointing = False\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    @check_model_inputs\n",
    "    @auto_docstring\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.LongTensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[Cache] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "        **kwargs: Unpack[TransformersKwargs],\n",
    "    ) -> MoeModelOutputWithPast:\n",
    "        if (input_ids is None) ^ (inputs_embeds is not None):\n",
    "            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n",
    "\n",
    "        if use_cache and past_key_values is None:\n",
    "            past_key_values = DynamicCache(config=self.config)\n",
    "\n",
    "        if inputs_embeds is None:\n",
    "            inputs_embeds = self.embed_tokens(input_ids)\n",
    "\n",
    "        if cache_position is None:\n",
    "            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n",
    "            cache_position = torch.arange(\n",
    "                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n",
    "            )\n",
    "        if position_ids is None:\n",
    "            position_ids = cache_position.unsqueeze(0)\n",
    "\n",
    "        # It may already have been prepared by e.g. `generate`\n",
    "        if not isinstance(causal_mask_mapping := attention_mask, dict):\n",
    "            mask_kwargs = {\n",
    "                \"config\": self.config,\n",
    "                \"input_embeds\": inputs_embeds,\n",
    "                \"attention_mask\": attention_mask,\n",
    "                \"cache_position\": cache_position,\n",
    "                \"past_key_values\": past_key_values,\n",
    "            }\n",
    "            causal_mask_mapping = {\n",
    "                \"full_attention\": create_causal_mask(**mask_kwargs),\n",
    "                \"sliding_attention\": create_sliding_window_causal_mask(**mask_kwargs),\n",
    "            }\n",
    "\n",
    "        hidden_states = inputs_embeds\n",
    "        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n",
    "\n",
    "        for decoder_layer in self.layers:\n",
    "            hidden_states = decoder_layer(\n",
    "                hidden_states,\n",
    "                attention_mask=causal_mask_mapping[decoder_layer.attention_type],\n",
    "                position_ids=position_ids,\n",
    "                past_key_values=past_key_values,\n",
    "                use_cache=use_cache,\n",
    "                cache_position=cache_position,\n",
    "                position_embeddings=position_embeddings,\n",
    "                **kwargs,\n",
    "            )\n",
    "        hidden_states = self.norm(hidden_states)\n",
    "        return MoeModelOutputWithPast(\n",
    "            last_hidden_state=hidden_states,\n",
    "            past_key_values=past_key_values,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "adfcd06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_balancing_loss_func(\n",
    "    gate_logits: Union[torch.Tensor, tuple[torch.Tensor], None],\n",
    "    num_experts: Optional[int] = None,\n",
    "    top_k=2,\n",
    "    attention_mask: Optional[torch.Tensor] = None,\n",
    ") -> Union[torch.Tensor, int]:\n",
    "    r\"\"\"\n",
    "    Computes auxiliary load balancing loss as in Switch Transformer - implemented in Pytorch.\n",
    "\n",
    "    See Switch Transformer (https://huggingface.co/papers/2101.03961) for more details. This function implements the loss\n",
    "    function presented in equations (4) - (6) of the paper. It aims at penalizing cases where the routing between\n",
    "    experts is too unbalanced.\n",
    "\n",
    "    Args:\n",
    "        gate_logits:\n",
    "            Logits from the `gate`, should be a tuple of model.config.num_hidden_layers tensors of\n",
    "            shape [batch_size X sequence_length, num_experts].\n",
    "        num_experts:\n",
    "            Number of experts\n",
    "        top_k:\n",
    "            The number of experts to route per-token, can be also interpreted as the `top-k` routing\n",
    "            parameter.\n",
    "        attention_mask (`torch.Tensor`, *optional*):\n",
    "            The attention_mask used in forward function\n",
    "            shape [batch_size X sequence_length] if not None.\n",
    "\n",
    "    Returns:\n",
    "        The auxiliary loss.\n",
    "    \"\"\"\n",
    "    if gate_logits is None or not isinstance(gate_logits, tuple):\n",
    "        return 0\n",
    "\n",
    "    if isinstance(gate_logits, tuple):\n",
    "        compute_device = gate_logits[0].device\n",
    "        concatenated_gate_logits = torch.cat([layer_gate.to(compute_device) for layer_gate in gate_logits], dim=0)\n",
    "\n",
    "    routing_weights = torch.nn.functional.softmax(concatenated_gate_logits, dim=-1)\n",
    "\n",
    "    _, selected_experts = torch.topk(routing_weights, top_k, dim=-1)\n",
    "\n",
    "    expert_mask = torch.nn.functional.one_hot(selected_experts, num_experts)\n",
    "\n",
    "    if attention_mask is None:\n",
    "        # Compute the percentage of tokens routed to each experts\n",
    "        tokens_per_expert = torch.mean(expert_mask.float(), dim=0)\n",
    "\n",
    "        # Compute the average probability of routing to these experts\n",
    "        router_prob_per_expert = torch.mean(routing_weights, dim=0)\n",
    "    else:\n",
    "        batch_size, sequence_length = attention_mask.shape\n",
    "        num_hidden_layers = concatenated_gate_logits.shape[0] // (batch_size * sequence_length)\n",
    "\n",
    "        # Compute the mask that masks all padding tokens as 0 with the same shape of expert_mask\n",
    "        expert_attention_mask = (\n",
    "            attention_mask[None, :, :, None, None]\n",
    "            .expand((num_hidden_layers, batch_size, sequence_length, top_k, num_experts))\n",
    "            .reshape(-1, top_k, num_experts)\n",
    "            .to(compute_device)\n",
    "        )\n",
    "\n",
    "        # Compute the percentage of tokens routed to each experts\n",
    "        tokens_per_expert = torch.sum(expert_mask.float() * expert_attention_mask, dim=0) / torch.sum(\n",
    "            expert_attention_mask, dim=0\n",
    "        )\n",
    "\n",
    "        # Compute the mask that masks all padding tokens as 0 with the same shape of tokens_per_expert\n",
    "        router_per_expert_attention_mask = (\n",
    "            attention_mask[None, :, :, None]\n",
    "            .expand((num_hidden_layers, batch_size, sequence_length, num_experts))\n",
    "            .reshape(-1, num_experts)\n",
    "            .to(compute_device)\n",
    "        )\n",
    "\n",
    "        # Compute the average probability of routing to these experts\n",
    "        router_prob_per_expert = torch.sum(routing_weights * router_per_expert_attention_mask, dim=0) / torch.sum(\n",
    "            router_per_expert_attention_mask, dim=0\n",
    "        )\n",
    "\n",
    "    overall_loss = torch.sum(tokens_per_expert * router_prob_per_expert.unsqueeze(0))\n",
    "    return overall_loss * num_experts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bf01d5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GptOssForCausalLM(GptOssPreTrainedModel, GenerationMixin):\n",
    "    _tied_weights_keys = [\"lm_head.weight\"]\n",
    "    _tp_plan = {\"lm_head\": \"colwise_rep\"}\n",
    "    _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.model = GptOssModel(config)\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "        self.router_aux_loss_coef = config.router_aux_loss_coef\n",
    "        self.num_experts = config.num_local_experts\n",
    "        self.num_experts_per_tok = config.num_experts_per_tok\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    @can_return_tuple\n",
    "    @auto_docstring\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.LongTensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[Cache] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_router_logits: Optional[bool] = None,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "        logits_to_keep: Union[int, torch.Tensor] = 0,\n",
    "        **kwargs: Unpack[TransformersKwargs],\n",
    "    ) -> MoeCausalLMOutputWithPast:\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n",
    "            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n",
    "            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n",
    "\n",
    "        Example:\n",
    "\n",
    "        ```python\n",
    "        >>> from transformers import AutoTokenizer, GptOssForCausalLM\n",
    "\n",
    "        >>> model = GptOssForCausalLM.from_pretrained(\"mistralai/GptOss-8x7B-v0.1\")\n",
    "        >>> tokenizer = AutoTokenizer.from_pretrained(\"mistralai/GptOss-8x7B-v0.1\")\n",
    "\n",
    "        >>> prompt = \"Hey, are you conscious? Can you talk to me?\"\n",
    "        >>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "        >>> # Generate\n",
    "        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n",
    "        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "        \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\n",
    "        ```\"\"\"\n",
    "\n",
    "        output_router_logits = (\n",
    "            output_router_logits if output_router_logits is not None else self.config.output_router_logits\n",
    "        )\n",
    "\n",
    "        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n",
    "        outputs: MoeModelOutputWithPast = self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            use_cache=use_cache,\n",
    "            output_router_logits=output_router_logits,\n",
    "            cache_position=cache_position,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n",
    "        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n",
    "        logits = self.lm_head(hidden_states[:, slice_indices, :])\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = self.loss_function(logits, labels, self.vocab_size, **kwargs)\n",
    "\n",
    "        aux_loss = None\n",
    "        if output_router_logits:\n",
    "            aux_loss = load_balancing_loss_func(\n",
    "                outputs.router_logits,\n",
    "                self.num_experts,\n",
    "                self.num_experts_per_tok,\n",
    "                attention_mask,\n",
    "            )\n",
    "            if labels is not None:\n",
    "                loss += self.router_aux_loss_coef * aux_loss.to(loss.device)  # make sure to reside in the same device\n",
    "\n",
    "        return MoeCausalLMOutputWithPast(\n",
    "            loss=loss,\n",
    "            aux_loss=aux_loss,\n",
    "            logits=logits,\n",
    "            past_key_values=outputs.past_key_values,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "            router_logits=outputs.router_logits,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d03e5c25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸŸ© loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--openai--gpt-oss-20b/snapshots/6cee5e81ee83917806bbde320786a8fb61efebee/tokenizer.json\n",
      "ðŸŸ© loading file tokenizer.model from cache at None\n",
      "ðŸŸ© loading file added_tokens.json from cache at None\n",
      "ðŸŸ© loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--openai--gpt-oss-20b/snapshots/6cee5e81ee83917806bbde320786a8fb61efebee/special_tokens_map.json\n",
      "ðŸŸ© loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--openai--gpt-oss-20b/snapshots/6cee5e81ee83917806bbde320786a8fb61efebee/tokenizer_config.json\n",
      "ðŸŸ© loading file chat_template.jinja from cache at /root/.cache/huggingface/hub/models--openai--gpt-oss-20b/snapshots/6cee5e81ee83917806bbde320786a8fb61efebee/chat_template.jinja\n",
      "ðŸŸ© Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='openai/gpt-oss-20b', vocab_size=199998, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|startoftext|>', 'eos_token': '<|return|>', 'pad_token': '<|endoftext|>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t199998: AddedToken(\"<|startoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t199999: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t200000: AddedToken(\"<|reserved_200000|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t200001: AddedToken(\"<|reserved_200001|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t200002: AddedToken(\"<|return|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t200003: AddedToken(\"<|constrain|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t200004: AddedToken(\"<|reserved_200004|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t200005: AddedToken(\"<|channel|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t200006: AddedToken(\"<|start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t200007: AddedToken(\"<|end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t200008: AddedToken(\"<|message|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t200009: AddedToken(\"<|reserved_200009|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t200010: AddedToken(\"<|reserved_200010|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t200011: AddedToken(\"<|reserved_200011|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t200012: AddedToken(\"<|call|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t200013: AddedToken(\"<|reserved_200013|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t200014: AddedToken(\"<|reserved_200014|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t200015: AddedToken(\"<|reserved_200015|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t200016: AddedToken(\"<|reserved_200016|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t200017: AddedToken(\"<|reserved_200017|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t200018: AddedToken(\"<|endofprompt|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model_id = \"openai/gpt-oss-20b\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876725b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸŸ© loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--openai--gpt-oss-20b/snapshots/6cee5e81ee83917806bbde320786a8fb61efebee/config.json\n",
      "ðŸŸ© Model config GptOssConfig {\n",
      "  \"architectures\": [\n",
      "    \"GptOssForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": true,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"eos_token_id\": 200002,\n",
      "  \"experts_per_token\": 4,\n",
      "  \"head_dim\": 64,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2880,\n",
      "  \"initial_context_length\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 2880,\n",
      "  \"layer_types\": [\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\"\n",
      "  ],\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"model_type\": \"gpt_oss\",\n",
      "  \"num_attention_heads\": 64,\n",
      "  \"num_experts_per_tok\": 4,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"num_local_experts\": 32,\n",
      "  \"output_router_logits\": false,\n",
      "  \"pad_token_id\": 199999,\n",
      "  \"quantization_config\": {\n",
      "    \"modules_to_not_convert\": [\n",
      "      \"model.layers.*.self_attn\",\n",
      "      \"model.layers.*.mlp.router\",\n",
      "      \"model.embed_tokens\",\n",
      "      \"lm_head\"\n",
      "    ],\n",
      "    \"quant_method\": \"mxfp4\"\n",
      "  },\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"beta_fast\": 32.0,\n",
      "    \"beta_slow\": 1.0,\n",
      "    \"factor\": 32.0,\n",
      "    \"original_max_position_embeddings\": 4096,\n",
      "    \"rope_type\": \"yarn\",\n",
      "    \"truncate\": false\n",
      "  },\n",
      "  \"rope_theta\": 150000,\n",
      "  \"router_aux_loss_coef\": 0.9,\n",
      "  \"sliding_window\": 128,\n",
      "  \"swiglu_limit\": 7.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.57.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 201088\n",
      "}\n",
      "\n",
      "ðŸŸ© \n",
      "ðŸŸ© Overriding dtype=torch.bfloat16 with `dtype=torch.bfloat16` due to requirements of `fbgemm-gpu` to enable model loading in fp4. Pass your own dtype to specify the dtype of the remaining non-linear layers or pass dtype=torch.bfloat16 to remove this warning.\n",
      "ðŸŸ© loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--openai--gpt-oss-20b/snapshots/6cee5e81ee83917806bbde320786a8fb61efebee/model.safetensors.index.json\n",
      "ðŸŸ© Instantiating GptOssForCausalLM model under default dtype torch.bfloat16.\n",
      "ðŸŸ© Generate config GenerationConfig {\n",
      "  \"eos_token_id\": 200002,\n",
      "  \"pad_token_id\": 199999\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "543eea659b034ae0913b0e608f54f6ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 41 files:   0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb37b9a26b464220ac3004cfb4f00245",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52c3adbc97d34f268ff8c5183fa7a9d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 41 files:   0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸŸ© loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--openai--gpt-oss-20b/snapshots/6cee5e81ee83917806bbde320786a8fb61efebee/generation_config.json\n",
      "ðŸŸ© Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 199998,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    200002,\n",
      "    199999,\n",
      "    200012\n",
      "  ],\n",
      "  \"pad_token_id\": 199999\n",
      "}\n",
      "\n",
      "ðŸŸ© Could not locate the custom_generate/generate.py inside openai/gpt-oss-20b.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GptOssForCausalLM(\n",
       "  (model): GptOssModel(\n",
       "    (embed_tokens): Embedding(201088, 2880, padding_idx=199999)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x GptOssDecoderLayer(\n",
       "        (self_attn): GptOssAttention(\n",
       "          (q_proj): Linear(in_features=2880, out_features=4096, bias=True)\n",
       "          (k_proj): Linear(in_features=2880, out_features=512, bias=True)\n",
       "          (v_proj): Linear(in_features=2880, out_features=512, bias=True)\n",
       "          (o_proj): Linear(in_features=4096, out_features=2880, bias=True)\n",
       "        )\n",
       "        (mlp): GptOssMLP(\n",
       "          (router): GptOssTopKRouter()\n",
       "          (experts): Mxfp4GptOssExperts()\n",
       "        )\n",
       "        (input_layernorm): GptOssRMSNorm((2880,), eps=1e-05)\n",
       "        (post_attention_layernorm): GptOssRMSNorm((2880,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): GptOssRMSNorm((2880,), eps=1e-05)\n",
       "    (rotary_emb): GptOssRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2880, out_features=201088, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GptOssForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"cuda\",\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6c0c964e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸŸ¨ Device set to use cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'assistant', 'content': 'analysisWe need to explain quantum mechanics clearly and concisely. Provide overview, key concepts, principles, and some examples. Avoid excessive jargon. Should be accessible. Likely need to mention wave-particle duality, superposition, uncertainty principle, quantization, measurement, wavefunction, SchrÃ¶dinger equation, entanglement, probabilistic nature. Need to keep concise. So maybe a structured explanation with bullet points or brief paragraphs. Provide analogies like double-slit, electron in potential well. Ensure clarity. Provide key equations like SchrÃ¶dinger. Also mention historical development. Keep it concise.assistantfinal**Quantum mechanics in a nutshell**\\n\\n| What | Why it matters | Key idea |\\n|------|----------------|----------|\\n| **Waveâ€‘particle duality** | Light and matter behave like waves *and* particles. | Light shows interference (waves); photons are detected as individual packets (particles). |\\n| **Wavefunction (Î¨)** | Encodes *all* information about a system. | A mathematical function that gives the probability of finding a particle in each position. |\\n| **Superposition** | A particle can be in several states at once until measured. | A cat is both alive *and* dead in the famous'}\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Explain quantum mechanics clearly and concisely.\"},\n",
    "]\n",
    "\n",
    "outputs = pipe(\n",
    "    messages,\n",
    "    max_new_tokens=256,\n",
    ")\n",
    "print(outputs[0][\"generated_text\"][-1])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
