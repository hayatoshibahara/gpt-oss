{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "160b36f6",
   "metadata": {},
   "source": [
    "# gpt-oss\n",
    "\n",
    "- [ãƒ¢ãƒ‡ãƒ«ã‚«ãƒ¼ãƒ‰][1]\n",
    "- [GitHub][2]\n",
    "- [ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆ][3]\n",
    "\n",
    "[1]: https://arxiv.org/abs/2508.10925\n",
    "[2]: https://github.com/openai/gpt-oss\n",
    "[3]: https://openai.com/index/introducing-gpt-oss/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d9e846",
   "metadata": {},
   "source": [
    "## å°Žå…¥"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3d83cd",
   "metadata": {},
   "source": [
    "gpt-ossã¯ã€120bã¨20bã®2ã¤ã®ã‚ªãƒ¼ãƒ—ãƒ³ã‚¦ã‚§ã‚¤ãƒˆæŽ¨è«–ãƒ¢ãƒ‡ãƒ«"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fbd37f",
   "metadata": {},
   "source": [
    "Response APIã¨äº’æ›æ€§ãŒã‚ã‚Šã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã«çµ„ã¿è¾¼ã‚ã‚‹:\n",
    "\n",
    "- é«˜ã„æŒ‡ç¤ºè¿½å¾“æ€§\n",
    "- ã‚¦ã‚§ãƒ–æ¤œç´¢ã‚„ã‚³ãƒ¼ãƒ‰å®Ÿè¡Œãªã©ã®ãƒ„ãƒ¼ãƒ«ä»•æ§˜\n",
    "- æŽ¨è«–ã®æ·±ã•ã‚’è¨­å®šã§ãã‚‹æ©Ÿèƒ½\n",
    "- CoTï¼ˆchain-of-thought, æ€è€ƒã®é€£éŽ–ï¼‰å¯¾å¿œ\n",
    "- JSONå½¢å¼ãªã©ã®å‡ºåŠ›ã«å¯¾å¿œ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98363bd6",
   "metadata": {},
   "source": [
    "Preparedness Frameworkã§å±é™ºæ€§ã‚’è©•ä¾¡:\n",
    "\n",
    "- Preparedness Frameworkã¯ã€ãƒ¢ãƒ‡ãƒ«ã®é«˜åº¦ã®èƒ½åŠ›ã‚’è¿½è·¡ã—ãƒªã‚¹ã‚¯ã‚’è¦‹ç©ã‚‚ã‚‹å·¥ç¨‹\n",
    "- ä»¥ä¸‹ã®åˆ†é‡Žã§é«˜ãƒªã‚¹ã‚¯ã§ãªã„ã“ã¨ã‚’ç¢ºèª\n",
    "    - ç”Ÿç‰©ãƒ»åŒ–å­¦ï¼ˆBiological and Chemical capabilityï¼‰\n",
    "        - ãƒ¢ãƒ‡ãƒ«ãŒç”Ÿç‰©å…µå™¨ãƒ»åŒ–å­¦å…µå™¨ã‚’åŠ©é•·ã™ã‚‹èƒ½åŠ›ã‚’æŒã¤ã‹ã©ã†ã‹\n",
    "    - ã‚µã‚¤ãƒãƒ¼ï¼ˆCyber capabilityï¼‰\n",
    "        - ã‚·ã‚¹ãƒ†ãƒ ã®æ©Ÿå¯†æ€§ãƒ»å®Œå…¨æ€§ãƒ»å¯ç”¨æ€§ã‚’ç ´å£Šã™ã‚‹èƒ½åŠ›ã‚’æŒã¤ã‹ã©ã†ã‹\n",
    "    - AIè‡ªå·±æ”¹å–„ï¼ˆAI Self-Improvementï¼‰\n",
    "        - ãƒ¢ãƒ‡ãƒ«ãŒå†å¸°çš„ãªèƒ½åŠ›æ”¹å–„ã‚’å¼•ãèµ·ã“ã™èƒ½åŠ›ã‚’æŒã¤ã‹ã©ã†ã‹"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07058729",
   "metadata": {},
   "source": [
    "## ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb70b35",
   "metadata": {},
   "source": [
    "ç‰¹å¾´:\n",
    "\n",
    "- GPT-2ãƒ»GPT-3ã‚’åŸºã«æ§‹ç¯‰\n",
    "- è‡ªå·±å›žå¸°åž‹ã®Mixture-of-Expertsï¼ˆMoEï¼‰Transformerãƒ¢ãƒ‡ãƒ«\n",
    "- gpt-oss-120b: 36å±¤\n",
    "- gpt-oss-20b: 24å±¤\n",
    "\n",
    "![](image/arch.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38eb11e4",
   "metadata": {},
   "source": [
    "äº‹å¾Œå­¦ç¿’ã§ã€MoEã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’MXFP4ã«é‡å­åŒ–ã—ã¦ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°:\n",
    "\n",
    "- MoEã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã®90%ä»¥ä¸Šã‚’å ã‚ã¦ã„ã‚‹\n",
    "- é‡å­åŒ–ã§ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚ãŸã‚Š4.25ãƒ“ãƒƒãƒˆã«å‰Šæ¸›\n",
    "    - 120bãƒ¢ãƒ‡ãƒ«ã¯å˜ä¸€ã®80GBã®VRAMã§æŽ¨è«–å¯èƒ½\n",
    "    - 20bãƒ¢ãƒ‡ãƒ«ã¯16GBã®VRAMã§æŽ¨è«–å¯èƒ½"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b560961f",
   "metadata": {},
   "source": [
    "ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è©³ç´°:\n",
    "\n",
    "- 2880æ¬¡å…ƒã®æ®‹å·®æŽ¥ç¶šï¼ˆresidual streamï¼‰ã‚’æŒã¤\n",
    "- ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãŠã‚ˆã³MoEã‚’é©ç”¨ã™ã‚‹å‰ã«RMSNormã‚’é©ç”¨ï¼ˆPre-LNï¼‰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cfd9bf",
   "metadata": {
    "vscode": {
     "languageId": "ini"
    }
   },
   "source": [
    "MoEãƒ–ãƒ­ãƒƒã‚¯:\n",
    "\n",
    "- ä¸€èˆ¬çš„ãªç·šå½¢ãƒ«ãƒ¼ã‚¿ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆlinear router projectionï¼‰ã‚’æŽ¡ç”¨\n",
    "    - 120bãƒ¢ãƒ‡ãƒ«ã¯128å€‹ã®ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆ\n",
    "    - 20bãƒ¢ãƒ‡ãƒ«ã¯32å€‹ã®ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆ\n",
    "- ãƒˆãƒ¼ã‚¯ãƒ³ã”ã¨ã«4ã¤ã®ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã‚’é¸æŠžã—ã€Softmaxã§é‡ã¿ä»˜ã‘\n",
    "- MoEãƒ–ãƒ­ãƒƒã‚¯ã¯ã€Gated SwiGLUæ´»æ€§åŒ–é–¢æ•°ã‚’æŽ¡ç”¨\n",
    "    - å€¤ã‚’ã‚¯ãƒªãƒƒãƒ—ã—ãŸã‚Šï¼ˆclampingï¼‰ã€æ®‹å·®æŽ¥ç¶šã‚’åŠ ãˆã‚‹ãªã©ã‚«ã‚¹ã‚¿ãƒžã‚¤ã‚ºã—ãŸSwiGLUã‚’ä½¿ç”¨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9ee2cb",
   "metadata": {
    "vscode": {
     "languageId": "ini"
    }
   },
   "source": [
    "ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³æ©Ÿæ§‹:\n",
    "\n",
    "- 2ç¨®é¡žã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³æ©Ÿæ§‹ã‚’äº¤äº’ã«ç¹°ã‚Šè¿”ã™\n",
    "    - ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚ºãŒ128ãƒˆãƒ¼ã‚¯ãƒ³ã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ï¼ˆbanded window patternï¼‰\n",
    "    - ãƒ•ãƒ«ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ï¼ˆfully dense patternï¼‰\n",
    "- GQAï¼ˆGrouped Query Attentionï¼‰ã§ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã¨æŽ¨è«–æ™‚é–“ã‚’å‰Šæ¸›\n",
    "    - ãƒ˜ãƒƒãƒ‰ã¯64æ¬¡å…ƒ\n",
    "    - ã‚¯ã‚¨ãƒªãƒ˜ãƒƒãƒ‰ã¯64å€‹ \n",
    "    - ã‚­ãƒ¼ãƒ»ãƒãƒªãƒ¥ãƒ¼ãƒ˜ãƒƒãƒ‰ã¯8å€‹\n",
    "- RoPEï¼ˆRotary Position Embeddingsï¼‰ã‚’ä½¿ç”¨\n",
    "- YaRNï¼ˆYet another Rope extentioNï¼‰ã§ãƒ•ãƒ«ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·ã‚’131,072ãƒˆãƒ¼ã‚¯ãƒ³ã«æ‹¡å¼µ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902bd931",
   "metadata": {},
   "source": [
    "å„ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ˜ãƒƒãƒ‰ã®Softmaxã®åˆ†æ¯ã«å­¦ç¿’å¯èƒ½ãªãƒã‚¤ã‚¢ã‚¹é …ã‚’è¿½åŠ :\n",
    "\n",
    "- [off-by-one attention][1]ã‚„[attention sinks][2]ã‹ã‚‰ç€æƒ³\n",
    "    - å¾“æ¥ã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ˜ãƒƒãƒ‰ã¯ã€Œã©ã®ãƒˆãƒ¼ã‚¯ãƒ³ã«ã‚‚æ³¨æ„ã‚’æ‰•ã‚ãªã„ã€é¸æŠžãŒã§ããªã‹ã£ãŸ\n",
    "    - ãƒã‚¤ã‚¢ã‚¹é …ã‚’åŠ ãˆã‚‹ã“ã¨ã§æ³¨æ„ã‚’æ‰•ã‚ãªã„é¸æŠžãŒå¯èƒ½ã«ãªã‚Šã€å­¦ç¿’ãŒå®‰å®šåŒ–ã™ã‚‹\n",
    "\n",
    "[1]: https://www.evanmiller.org/attention-is-off-by-one.html\n",
    "[2]: https://arxiv.org/abs/2309.17453"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23905181",
   "metadata": {},
   "source": [
    "AIMEãƒ»GPQA Diamondãƒ»HLEãƒ»MMLUãƒ™ãƒ³ãƒãƒžãƒ¼ã‚¯ã§o4-miniã«åŒ¹æ•µã™ã‚‹æ€§èƒ½:\n",
    "\n",
    "![](image/fig1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee363f9",
   "metadata": {},
   "source": [
    "## ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca44ef97",
   "metadata": {},
   "source": [
    "TikTokenãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§äº¤éš›ã•ã‚Œã¦ã„ã‚‹[o200k_harmony][1]ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ä½¿ç”¨:\n",
    "\n",
    "- BPEï¼ˆByte Pair Encodingï¼‰\n",
    "- GPT-4oã‚„o4-miniã¨åŒæ§˜\n",
    "- Harmonyãƒãƒ£ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆã«äº’æ›\n",
    "- èªžå½™æ•°ã¯201,088ãƒˆãƒ¼ã‚¯ãƒ³\n",
    "\n",
    "![](image/fig17.png)\n",
    "\n",
    "[1]: https://github.com/openai/tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c9402f",
   "metadata": {},
   "source": [
    "## äº‹å‰å­¦ç¿’"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183ee789",
   "metadata": {},
   "source": [
    "è¨“ç·´ã«æ•°å…†ãƒˆãƒ¼ã‚¯ãƒ³ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½¿ç”¨:\n",
    "\n",
    "- STEMï¼ˆç§‘å­¦ãƒ»æŠ€è¡“ãƒ»å·¥å­¦ãƒ»æ•°å­¦ï¼‰ã€ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã€ä¸€èˆ¬å¸¸è­˜\n",
    "- GPT-4oã®CBRNäº‹å‰å­¦ç¿’ãƒ•ã‚£ãƒ«ã‚¿ã‚’å†åˆ©ç”¨ã—å®‰å…¨æ€§ã‚’å‘ä¸Š\n",
    "    - CBRN: Chemicalãƒ»Biologicalãƒ»Radiodicalãƒ»Nuclear\n",
    "- çŸ¥è­˜ã‚«ãƒƒãƒˆã‚ªãƒ•ã¯2024å¹´6æœˆ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50af7c4c",
   "metadata": {},
   "source": [
    "NVIDIA H100 GPUã§è¨“ç·´:\n",
    "\n",
    "- PyTorchã¨ã€ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®æœ€é©åŒ–ã«Tritonã‚’ä½¿ç”¨\n",
    "- Flash Attentionã§ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’å‰Šæ¸›ã—ã€è¨“ç·´ã‚’é«˜é€ŸåŒ–\n",
    "- gpt-oss-120bã®è¨“ç·´ã¯ã€H100ã§210ä¸‡æ™‚é–“ã«ç›¸å½“ã™ã‚‹è¨ˆç®—é‡\n",
    "- gpt-oss-20bã®è¨“ç·´ã¯ã€H100ã§21ä¸‡æ™‚é–“ã«ç›¸å½“ã™ã‚‹è¨ˆç®—é‡"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef076066",
   "metadata": {},
   "source": [
    "## äº‹å¾Œå­¦ç¿’"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab514cd",
   "metadata": {},
   "source": [
    "äº‹å¾Œå­¦ç¿’ã¯ã€OpenAI O3ã¨åŒæ§˜ã®CoT RLæ‰‹æ³•ã§å®Ÿæ–½:\n",
    "\n",
    "- CoTã«ã‚ˆã‚‹æŽ¨è«–ã®å¼·åŒ–\n",
    "- ãƒ„ãƒ¼ãƒ«ã®ä½¿ã„æ–¹ã‚’ç¿’å¾—\n",
    "- ChatGPTã¨ä¼¼ãŸå€‹æ€§\n",
    "- ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãƒ»æ•°å­¦ãƒ»ç§‘å­¦ãªã©ã®å¹…åºƒã„åˆ†é‡Ž"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f254255",
   "metadata": {},
   "source": [
    "Harmonyãƒãƒ£ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆã‚’ä½¿ç”¨ã—ã€OpenAI APIã¨äº’æ›:\n",
    "\n",
    "- Systemãƒ­ãƒ¼ãƒ«ã‚„Developerãƒ­ãƒ¼ãƒ«ãªã©ã®ãƒ­ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã®ãƒãƒ£ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆ\n",
    "- æŒ‡ç¤ºãŒç«¶åˆã—ãŸå ´åˆã€System > Developer > User > Assistant > Toolã¨ã„ã†å„ªå…ˆé †ä½ã§å‡¦ç†\n",
    "- CoTç”¨ã®analysisãƒãƒ£ãƒãƒ«ã€é–¢æ•°ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ç”¨ã®commentaryãƒãƒ£ãƒãƒ«ã€æœ€çµ‚è§£ç­”ç”¨ã®finalãƒãƒ£ãƒãƒ«ãªã©ã‚‚å«ã‚€\n",
    "- è©³ç´°ã¯[openai/harmony](https://github.com/openai/harmony)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bc4e5c",
   "metadata": {},
   "source": [
    "å¯å¤‰ãªæŽ¨è«–ãƒ¬ãƒ™ãƒ«ã®è¨“ç·´:\n",
    "\n",
    "- `Reasoning: low` ã®ã‚ˆã†ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«è¿½åŠ ã™ã‚‹ã¨ã€CoTã®é•·ã•ã‚’å¤‰æ›´ã§ãã‚‹"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d9b0b5",
   "metadata": {},
   "source": [
    "ãƒ¢ãƒ‡ãƒ«ã«è‡ªå¾‹çš„ã«ãƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹ã‚ˆã†ã«è¨“ç·´:\n",
    "\n",
    "- ãƒ–ãƒ©ã‚¦ã‚¸ãƒ³ã‚°ãƒ„ãƒ¼ãƒ«\n",
    "    - `search`ãŠã‚ˆã³`open`é–¢æ•°ã‚’å‘¼ã³å‡ºã—ã€è§£ç­”ã®äº‹å®Ÿæ€§ã‚„çŸ¥è­˜ã®ã‚«ãƒƒãƒˆã‚ªãƒ•ã«å¯¾å‡¦\n",
    "- Pythonãƒ„ãƒ¼ãƒ«\n",
    "    - ã‚¹ãƒ†ãƒ¼ãƒˆãƒ•ãƒ«ãªJupyter Notebookç’°å¢ƒã§ã‚³ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œ\n",
    "- ä»»æ„ã®ã‚¹ã‚­ãƒ¼ãƒžã®ãƒ„ãƒ¼ãƒ«\n",
    "    - OpenAI APIã¨åŒæ§˜ã«ä»»æ„ã®é–¢æ•°ã®å®Ÿè¡Œ\n",
    "    - è©³ç´°ã¯[openai/gpt-oss](https://github.com/openai/gpt-oss)\n",
    "\n",
    "![](image/fig18.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7624c9",
   "metadata": {},
   "source": [
    "## è©•ä¾¡"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4832533",
   "metadata": {},
   "source": [
    "æŽ¨è«–ãƒ¬ãƒ™ãƒ«ã‚’ã€Œé«˜ã€ã«ã—ã¦pass@1ã§è©•ä¾¡:\n",
    "\n",
    "- æŽ¨è«–ã¨äº‹å®Ÿæ€§\n",
    "    - AIMEï¼ˆæ•°å­¦ã‚³ãƒ³ãƒ†ã‚¹ãƒˆï¼‰\n",
    "    - GPQAï¼ˆåšå£«ãƒ¬ãƒ™ãƒ«ã®ç§‘å­¦ï¼‰\n",
    "    - MMLUï¼ˆå¤§å­¦ãƒ¬ãƒ™ãƒ«è©¦é¨“ï¼‰\n",
    "    - HLEï¼ˆå°‚é–€å®¶ãƒ¬ãƒ™ãƒ«è©¦é¨“ï¼‰\n",
    "- ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°\n",
    "    - Codex CLIã¨ä¼¼ãŸãƒ„ãƒ¼ãƒ«ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹ã®æœ‰ç„¡ã‚’åˆ†ã‘ã¦è©•ä¾¡\n",
    "    - Codeforces Eloï¼ˆç«¶æŠ€ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ï¼‰\n",
    "    - SWE-bench Verifiedï¼ˆã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢å·¥å­¦ï¼‰\n",
    "- ãƒ„ãƒ¼ãƒ«ä½¿ç”¨\n",
    "    - $\\tau$-Bench Retailï¼ˆå°å£²æ¥­ã‚¿ã‚¹ã‚¯ï¼‰\n",
    "- ãã®ä»–\n",
    "    - MMMLUï¼ˆå¤šè¨€èªžã®å¤§å­¦ãƒ¬ãƒ™ãƒ«è©¦é¨“ï¼‰\n",
    "    - HealthBenchï¼ˆå¥åº·åˆ†é‡Žï¼‰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201d61d3",
   "metadata": {},
   "source": [
    "ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã¨ãƒ„ãƒ¼ãƒ«ä½¿ç”¨ã«ãŠã‘ã‚‹ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã¯o4-miniã«åŒ¹æ•µã™ã‚‹æ€§èƒ½:\n",
    "\n",
    "![](image/fig2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624a550a",
   "metadata": {},
   "source": [
    "æŽ¨è«–ãƒ¬ãƒ™ãƒ«ã‚’é«˜ãã™ã‚‹ã¨ã€CoTãŒé•·ããªã‚Šå›žç­”ã®ç²¾åº¦ãŒä¸ŠãŒã‚‹:\n",
    "\n",
    "![](image/fig3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2d3b99",
   "metadata": {},
   "source": [
    "æŽ¨è«–ãƒ¬ãƒ™ãƒ«ãŒé«˜ã„gpt-oss-120bã¯ã€OpenAI o3ã«åŒ¹æ•µã™ã‚‹æ€§èƒ½:\n",
    "\n",
    "![](image/fig4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83d67aa",
   "metadata": {},
   "source": [
    "å¤šè¨€èªžæ€§èƒ½ã¯æŽ¨è«–ãƒ¬ãƒ™ãƒ«ã®é«˜ã„gpt-oss-120bã§ã€o3-miniç¨‹åº¦ã®æ€§èƒ½:\n",
    "\n",
    "![](image/table2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d948518c",
   "metadata": {},
   "source": [
    "å…¨ä½“çš„ã«ã¯æŽ¨è«–ãƒ¬ãƒ™ãƒ«ãŒä¸ŠãŒã‚‹ã¨æ€§èƒ½ãŒå‘ä¸Š:\n",
    "\n",
    "![](image/table3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9942a82",
   "metadata": {},
   "source": [
    "## å®‰å…¨æ€§ãƒ†ã‚¹ãƒˆã¨ãƒªã‚¹ã‚¯ç·©å’Œ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8491d486",
   "metadata": {},
   "source": [
    "ã€Œæ‚ªæ„ã®ã‚ã‚‹é–‹ç™ºè€…ãŒãƒ¢ãƒ‡ãƒ«ã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹å¯èƒ½æ€§ã€ã‚’è€ƒæ…®ã—ã¦ãƒ†ã‚¹ãƒˆ\n",
    "\n",
    "Preparedness Frameworkã§ã€ç”Ÿç‰©ãƒ»ç§‘å­¦ã€ã‚µã‚¤ãƒãƒ¼ã€è‡ªå·±æ”¹å–„ã®åˆ†é‡Žã§é«˜ã„ãƒªã‚¹ã‚¯ã§ã¯ãªã„ã“ã¨ã‚’ç¢ºèª\n",
    "\n",
    "æ›´ã«2ã¤ã®è¦³ç‚¹ã§èª¿æŸ»:\n",
    "\n",
    "1. æ‚ªæ„ã®ã‚ã‚‹é–‹ç™ºè€…ãŒgpt-oss-120bã‚’è¿½åŠ å­¦ç¿’ã—ã¦ã€ç”Ÿç‰©ãƒ»ç§‘å­¦ã€ã‚µã‚¤ãƒãƒ¼é ˜åŸŸã§é«˜ã„èƒ½åŠ›ã‚’å®Ÿç¾ã§ãã‚‹ã‹\n",
    "    - SAGï¼ˆSafety Advisory Groupï¼‰ãŒãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’è¡Œã£ã¦ã‚‚å•é¡Œãªã‹ã£ãŸ\n",
    "2. gpt-oss-120bã‚’ãƒªãƒªãƒ¼ã‚¹ã™ã‚‹ã“ã¨ã§ã€ç”Ÿç‰©åˆ†é‡Žã®ç ”ç©¶ã‚’è‘—ã—ãå‰é€²ã§ãã‚‹ã‹\n",
    "    - åŒç­‰æ€§èƒ½ã®ã‚ªãƒ¼ãƒ—ãƒ³ã‚¦ã‚§ã‚¤ãƒˆãƒ¢ãƒ‡ãƒ«ãŒã™ã§ã«å­˜åœ¨ã™ã‚‹ã®ã§å®Ÿç¾å¯èƒ½æ€§ã¯ä½Žã„"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20945183",
   "metadata": {},
   "source": [
    "## ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦ã„ãªã„ãƒ¢ãƒ‡ãƒ«ã®å®‰å…¨æ€§"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3f41a6",
   "metadata": {},
   "source": [
    "æ¨™æº–çš„ãªãƒ™ãƒ³ãƒãƒžãƒ¼ã‚¯ã«ã‚ˆã‚‹ä¸è¨±å¯ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã«å¯¾ã™ã‚‹å®‰å…¨æ€§ã¯é£½å’Œ:\n",
    "\n",
    "![](image/table4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a05eb7",
   "metadata": {},
   "source": [
    "ãƒžãƒ«ãƒã‚¿ãƒ¼ãƒ³ã®ãƒ™ãƒ³ãƒãƒžãƒ¼ã‚¯ã§ã¯ã€o4-miniä»¥ä¸Šã®æ€§èƒ½:\n",
    "\n",
    "![](image/table5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28016c32",
   "metadata": {},
   "source": [
    "[StrongReject][1]ã«ã‚ˆã‚‹ã‚¸ã‚§ã‚¤ãƒ«ãƒ–ãƒ¬ã‚¤ã‚¯ã«å¯¾ã™ã‚‹ãƒ™ãƒ³ãƒãƒžãƒ¼ã‚¯ã§ã¯ã€o4-miniã¨åŒç­‰ã®æ€§èƒ½:\n",
    "\n",
    "![](image/table6.png)\n",
    "\n",
    "[1]: https://proceedings.neurips.cc/paper_files/paper/2024/hash/e2e06adf560b0706d3b1ddfca9f29756-Abstract-Datasets_and_Benchmarks_Track.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f31bf31",
   "metadata": {},
   "source": [
    "ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æŠ½å‡ºã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¤ãƒ³ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã«å¯¾ã™ã‚‹å …ç‰¢æ€§ã¯o4-miniã‚ˆã‚Šå¼±ã„:\n",
    "\n",
    "![](image/table7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a7b4a3",
   "metadata": {},
   "source": [
    "é–‹ç™ºè€…ãƒ­ãƒ¼ãƒ«ã¨ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ­ãƒ¼ãƒ«é–“ã®ãƒ•ãƒ¬ãƒ¼ã‚ºä¿è­·ã«å¯¾ã™ã‚‹å …ç‰¢æ€§ã¯ä½Žã„:\n",
    "\n",
    "![](image/table8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e082072",
   "metadata": {},
   "source": [
    "ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³ã¯ã€å†…éƒ¨ã«ä¸–ç•ŒçŸ¥è­˜ãŒå°‘ãªã„gpt-oss-20bã¯èµ·ã“ã—ã‚„ã™ã„:\n",
    "\n",
    "![](image/table9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d893a5c",
   "metadata": {},
   "source": [
    "åè¦‹ã«å¯¾ã™ã‚‹è©•ä¾¡ã¯o4-miniã¨åŒç­‰:\n",
    "\n",
    "![](image/table10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a494cdae",
   "metadata": {},
   "source": [
    "## Preparedness Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221194c2",
   "metadata": {},
   "source": [
    "Preparedness Frameworkã¯ã€ãƒ¢ãƒ‡ãƒ«ã‚’å…¬é–‹ã™ã‚‹ã“ã¨ã«ã‚ˆã‚‹æ·±åˆ»ãªãƒªã‚¹ã‚¯ã‚’è¿½è·¡ã—æœ€å°é™ã«ã™ã‚‹ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯\n",
    "\n",
    "ç”Ÿç‰©ãƒ»ç§‘å­¦é ˜åŸŸã¨ã‚µã‚¤ãƒãƒ¼é ˜åŸŸã§ã¯é«˜ãƒªã‚¹ã‚¯ã®å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚ã€æ•µå¯¾çš„ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆAFTï¼‰ã‚’å®Ÿæ–½:\n",
    "\n",
    "- Helpful-onlyè¨“ç·´\n",
    "    - ãƒ¢ãƒ‡ãƒ«ã®ã‚¬ãƒ¼ãƒ‰ãƒ¬ãƒ¼ãƒ«ã‚’æ„å›³çš„ã«è§£é™¤ã™ã‚‹ãŸã‚ã®å¼·åŒ–å­¦ç¿’\n",
    "- ç”Ÿç‰©ãƒ»ã‚µã‚¤ãƒãƒ¼é ˜åŸŸã«ãŠã‘ã‚‹èƒ½åŠ›ã®æœ€å¤§åŒ–\n",
    "    - ç”Ÿç‰©é ˜åŸŸã¯ã€äººé–“ã®å°‚é–€å®¶ã‚’æ´»ç”¨ã—ã¦è¨“ç·´\n",
    "        - è³ªå•å›žç­”ã§ã¯é«˜ã„æ€§èƒ½ã«é”ã—ãŸãŒã€å®Ÿé¨“ã®ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ãƒ»å®Ÿé¨“ã®æš—é»™çŸ¥ã«é–¢ã—ã¦ã¯é”ã—ãªã‹ã£ãŸ\n",
    "        - Qwen3ã‚„Kimi K2ã¯ãã‚Œã¨åŒç­‰ã®èƒ½åŠ›ã‚’æŒã£ã¦ã„ã‚‹ãŸã‚ã€ãƒªãƒªãƒ¼ã‚¹ã—ã¦ã‚‚ãƒªã‚¹ã‚¯ã¯å°‘ãªã„\n",
    "    - ã‚µã‚¤ãƒãƒ¼é ˜åŸŸã¯ã€CTFï¼ˆã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ç«¶æŠ€ï¼‰ãƒ‡ãƒ¼ã‚¿ã‚’æ´»ç”¨ã—ã¦è¨“ç·´"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808538b9",
   "metadata": {},
   "source": [
    "ç”Ÿç‰©é ˜åŸŸã§ã®å±é™ºæ€§ã®5æ®µéšŽã§è©•ä¾¡ã—ãŸã¨ã“ã‚ã€Helpful-onlyè¨“ç·´ã—ãŸãƒ¢ãƒ‡ãƒ«ã¯ä»–ã®ãƒ¢ãƒ‡ãƒ«ã¨åŒç­‰ã®æ€§èƒ½:\n",
    "\n",
    "1. Ideationï¼ˆã‚¢ã‚¤ãƒ‡ã‚¢å‡ºã—ï¼‰\n",
    "2. Acquisitionï¼ˆå…¥æ‰‹ï¼‰\n",
    "3. Magnificationï¼ˆå¢—å¼·ï¼‰\n",
    "4. Formulationï¼ˆè£½å‰¤åŒ–ï¼‰\n",
    "5. Releaseï¼ˆæ”¾å‡ºï¼‰\n",
    "\n",
    "![](image/fig5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c32df2",
   "metadata": {},
   "source": [
    "ãƒžãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãªã‚¦ã‚£ãƒ«ã‚¹å­¦åˆ†é‡Žã§ã®æ€§èƒ½ã¯ã€o3ã«åŠ£ã‚‹æ€§èƒ½ï¼ˆãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ–ãƒ­ãƒƒã‚¯ãƒ»ãƒ–ãƒ©ã‚¦ã‚¸ãƒ³ã‚°ã‚ã‚Šï¼‰:\n",
    "\n",
    "![](image/fig6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310ed3b5",
   "metadata": {},
   "source": [
    "å®Ÿé¨“ã®ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã«é–¢ã™ã‚‹ãƒ™ãƒ³ãƒãƒžãƒ¼ã‚¯ã§ã¯ã€o3ã«åŠ£ã‚‹æ€§èƒ½:\n",
    "\n",
    "![](image/fig7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d00177f",
   "metadata": {},
   "source": [
    "å®Ÿé¨“ã®æš—é»™çŸ¥ã¨ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã«é–¢ã™ã‚‹åˆ¥ã®ãƒ™ãƒ³ãƒãƒžãƒ¼ã‚¯ã§ã¯ã€o3ã«åŠ£ã‚‹æ€§èƒ½:\n",
    "\n",
    "![](image/fig8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69a7925",
   "metadata": {},
   "source": [
    "å¤§å­¦ç”Ÿãƒ¬ãƒ™ãƒ«ã®CTFï¼ˆCapture the Flag, ã‚µã‚¤ãƒãƒ¼ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ï¼‰ã§ã¯ã€o3ã«åŠ£ã‚‹æ€§èƒ½:\n",
    "\n",
    "![](image/fig10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d724ed",
   "metadata": {},
   "source": [
    "ãƒ—ãƒ­ãƒ¬ãƒ™ãƒ«ã®CTFãƒ™ãƒ³ãƒãƒžãƒ¼ã‚¯ã§ã¯ã€CTFå•é¡Œã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸãƒ¢ãƒ‡ãƒ«ãŒo3ã«åŒ¹æ•µã™ã‚‹æ€§èƒ½:\n",
    "\n",
    "![](image/fig11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac62eb6",
   "metadata": {},
   "source": [
    "ã‚µã‚¤ãƒãƒ¼æ”»æ’ƒã«é–¢ã™ã‚‹ãƒ™ãƒ³ãƒãƒžãƒ¼ã‚¯ã§ã¯ã€ã„ãšã‚Œã®ã‚·ãƒŠãƒªã‚ªã®è§£æ±ºã«ã‚‚è‡³ã‚‰ãªã‹ã£ãŸ:\n",
    "\n",
    "![](image/fig12.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92e3c30",
   "metadata": {},
   "source": [
    "AIã®è‡ªå·±æ”¹å–„ãƒªã‚¹ã‚¯ã§ã€GitHubã®ãƒã‚°ä¿®æ­£ã¯o4-miniã«åŠ£ã‚‹æ€§èƒ½:\n",
    "\n",
    "![](image/fig19.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128b59af",
   "metadata": {},
   "source": [
    "æœ€å…ˆç«¯ã®AIã®ç ”ç©¶ã®å†ç¾ã™ã‚‹èƒ½åŠ›ã¯ã€o4-miniã«åŠ£ã‚‹æ€§èƒ½:\n",
    "\n",
    "![](image/fig16.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e40d14",
   "metadata": {},
   "source": [
    "## å®Ÿè£…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9780cbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU transformers==4.57.1 kernels\n",
    "\n",
    "import os\n",
    "import logging as logging_\n",
    "import transformers\n",
    "from transformers import PretrainedConfig\n",
    "from transformers.utils import logging\n",
    "\n",
    "from typing import Callable, Optional, Union\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼\n",
    "\n",
    "import os\n",
    "from shutil import copyfile\n",
    "from typing import TYPE_CHECKING, Any, Optional\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«\n",
    "\n",
    "from typing import Callable, Optional, Union\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "assert torch.cuda.is_available(), \"CUDAã‚’ä½¿ç”¨ã§ãã¾ã›ã‚“\"\n",
    "\n",
    "from typing import Callable, Optional, Union\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from transformers.cache_utils import Cache, DynamicCache\n",
    "from transformers.generation import GenerationMixin\n",
    "from transformers.integrations.hub_kernels import use_kernel_forward_from_hub\n",
    "from transformers.masking_utils import create_causal_mask, create_sliding_window_causal_mask\n",
    "from transformers.modeling_layers import (\n",
    "    GenericForSequenceClassification,\n",
    "    GenericForTokenClassification,\n",
    "    GradientCheckpointingLayer,\n",
    ")\n",
    "from transformers.modeling_outputs import MoeCausalLMOutputWithPast, MoeModelOutputWithPast\n",
    "from transformers.modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n",
    "from transformers.modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n",
    "from transformers.processing_utils import Unpack\n",
    "from transformers.utils import TransformersKwargs, auto_docstring, can_return_tuple\n",
    "from transformers.utils.deprecation import deprecate_kwarg\n",
    "from transformers.utils.generic import OutputRecorder, check_model_inputs\n",
    "from transformers.models.gpt_oss.configuration_gpt_oss import GptOssConfig\n",
    "\n",
    "# ãƒ‡ãƒã‚¤ã‚¹è¨­å®š\n",
    "\n",
    "torch.set_default_device(\"cuda\")\n",
    "\n",
    "# ãƒ­ã‚°è¨­å®š\n",
    "\n",
    "if os.path.exists('debug.log'):\n",
    "    os.remove('debug.log')\n",
    "\n",
    "def custom_format(record):\n",
    "    match record.levelno:\n",
    "        case logging_.DEBUG:\n",
    "            level = 'ðŸŸ¦'\n",
    "        case logging_.INFO:\n",
    "            level = 'ðŸŸ©'\n",
    "        case logging_.WARNING:\n",
    "            level = 'ðŸŸ¨'\n",
    "        case logging_.ERROR:\n",
    "            level = 'ðŸŸ¥'\n",
    "        case logging_.CRITICAL:\n",
    "            level = 'ðŸ›‘'\n",
    "    return f\"{level} {record.getMessage()}\"\n",
    "\n",
    "logging.set_verbosity_debug()\n",
    "logger = logging.get_logger()\n",
    "\n",
    "for handler in logger.handlers:\n",
    "    logger.removeHandler(handler)\n",
    "\n",
    "formatter = logging_.Formatter()\n",
    "formatter.format = custom_format\n",
    "\n",
    "file_handler = logging_.FileHandler('debug.log')\n",
    "file_handler.setFormatter(formatter)\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "stream_handler = logging_.StreamHandler()\n",
    "stream_handler.setFormatter(formatter)\n",
    "logger.addHandler(stream_handler)\n",
    "\n",
    "logger.info(f\"Transformers version: {transformers.__version__}\")\n",
    "logger.info(f\"Numpy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088abc52",
   "metadata": {},
   "source": [
    "### GptOssRMSNorm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51ec5b0",
   "metadata": {},
   "source": [
    "RMSNormï¼ˆRoot Mean Square Layer Normalizationï¼‰ã‚’å®Ÿè£…ã—ãŸã‚¯ãƒ©ã‚¹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e8b37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSNormã®ã‚«ãƒ¼ãƒãƒ«ã‚’ä½¿ç”¨ï¼ˆhttps://huggingface.co/docs/kernels/main/en/indexï¼‰\n",
    "@use_kernel_forward_from_hub(\"RMSNorm\")\n",
    "class GptOssRMSNorm(nn.Module):\n",
    "    def __init__(self, hidden_size, eps=1e-6):\n",
    "        \"\"\"\n",
    "        GptOssRMSNorm is equivalent to T5LayerNorï¼‰\n",
    "        \"\"\"\n",
    "        logger.info(f\"RMSNormã‚’åˆæœŸåŒ–é–‹å§‹ {hidden_size=}, {eps=}\")\n",
    "        super().__init__()\n",
    "        \n",
    "        # 1.0ã«åˆæœŸåŒ–\n",
    "        # 2880\n",
    "        self.weight = nn.Parameter(torch.ones(hidden_size))\n",
    "\n",
    "        # 1e-05\n",
    "        self.variance_epsilon = eps\n",
    "\n",
    "        logger.info(\"RMSNormã®åˆæœŸåŒ–å®Œäº†\")\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        logger.info(f\"RMSNormã®é †ä¼æ’­é–‹å§‹ {hidden_states.shape=}, {hidden_states.dtype=}\")\n",
    "\n",
    "        # 1. å˜ç²¾åº¦ã«ã‚¢ãƒƒãƒ—ã‚­ãƒ£ã‚¹ãƒˆ\n",
    "        # bfloat16 -> float32\n",
    "        input_dtype = hidden_states.dtype\n",
    "        hidden_states = hidden_states.to(torch.float32)\n",
    "        logger.debug(f\"ã‚¢ãƒƒãƒ—ã‚­ãƒ£ã‚¹ãƒˆ {input_dtype=} {hidden_states.dtype=}\")\n",
    "\n",
    "        # 2. äºŒä¹—å¹³å‡ã‚’è¨ˆç®—ï¼ˆåˆ†æ•£ï¼‰\n",
    "        # (1, 69, 2880) -> (1, 69, 1)\n",
    "        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
    "\n",
    "        # 3. å¹³æ–¹æ ¹ã®é€†æ•°ã§æ­£è¦åŒ–\n",
    "        # (1, 69, 2880) * (1, 69, 1) -> (1, 69, 2880)\n",
    "        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
    "\n",
    "        # 4. å­¦ç¿’å¯èƒ½ãªé‡ã¿ï¼ˆã‚²ã‚¤ãƒ³ï¼‰ã‚’é©ç”¨ã—ã€å…ƒã®ãƒ‡ãƒ¼ã‚¿åž‹ã«æˆ»ã™\n",
    "        # (1, 69, 2880) -> (1, 69, 2880)\n",
    "        # float32 -> bfloat16\n",
    "        res = (self.weight * hidden_states).to(input_dtype)  # main diff with Llama\n",
    "\n",
    "        logger.info(f\"RMSNormã®é †ä¼æ’­å®Œäº† {res.shape=}, {res.dtype=}\")\n",
    "        return res\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305cadf1",
   "metadata": {},
   "source": [
    "### GptOssExperts\n",
    "\n",
    "GptOssExpertsã¯ã€MoEã®ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆå±¤ã‚’å®Ÿè£…ã™ã‚‹ã‚¯ãƒ©ã‚¹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5399eb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GptOssExperts(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        logger.info(f\"GptOssExpertsã‚’åˆæœŸåŒ–é–‹å§‹ {config.intermediate_size=}, {config.num_local_experts=}, {config.hidden_size=}\")\n",
    "        super().__init__()\n",
    "\n",
    "        # 2880\n",
    "        self.intermediate_size = config.intermediate_size\n",
    "\n",
    "        # 32\n",
    "        self.num_experts = config.num_local_experts\n",
    "\n",
    "        # 2880\n",
    "        self.hidden_size = config.hidden_size\n",
    "\n",
    "        # 2880\n",
    "        self.expert_dim = self.intermediate_size\n",
    "\n",
    "        # ã‚²ãƒ¼ãƒˆãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã¨ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’åˆæœŸåŒ–\n",
    "        # (32, 2880, 5760)\n",
    "        self.gate_up_proj = nn.Parameter(torch.empty(self.num_experts, self.hidden_size, 2 * self.expert_dim))\n",
    "\n",
    "        # ã‚²ãƒ¼ãƒˆãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã¨ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã®ãƒã‚¤ã‚¢ã‚¹ã‚’åˆæœŸåŒ–\n",
    "        # (32, 5760)\n",
    "        self.gate_up_proj_bias = nn.Parameter(torch.empty(self.num_experts, 2 * self.expert_dim))\n",
    "\n",
    "        # ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’åˆæœŸåŒ–\n",
    "        # (32, 5760, 2880)\n",
    "        self.down_proj = nn.Parameter(torch.empty((self.num_experts, self.expert_dim, self.hidden_size)))\n",
    "\n",
    "        # ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã®ãƒã‚¤ã‚¢ã‚¹ã‚’åˆæœŸåŒ–\n",
    "        # (32, 2880)\n",
    "        self.down_proj_bias = nn.Parameter(torch.empty(self.num_experts, self.hidden_size))\n",
    "\n",
    "        # ã‚«ã‚¹ã‚¿ãƒ SwiGLUã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "        self.alpha = 1.702\n",
    "        self.limit = 7.0\n",
    "        logger.info(\"GptOssExpertsã®åˆæœŸåŒ–å®Œäº†\")\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor, router_indices=None, routing_weights=None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        When training it is more efficient to just loop over the experts and compute the output for each expert\n",
    "        as otherwise the memory would explode.\n",
    "\n",
    "        For inference we can sacrifice some memory and compute the output for all experts at once. By repeating the inputs.\n",
    "\n",
    "        Args:\n",
    "            hidden_states (torch.Tensor): (batch_size, seq_len, hidden_size)\n",
    "            selected_experts (torch.Tensor): (batch_size * token_num, top_k)\n",
    "            routing_weights (torch.Tensor): (batch_size * token_num, num_experts)\n",
    "        Returns:\n",
    "            torch.Tensor\n",
    "        \"\"\"\n",
    "        logger.info(f\"GptOssExpertsã®é †ä¼æ’­é–‹å§‹ {hidden_states.shape=}, {hidden_states.dtype=}, {router_indices.shape=}, {routing_weights.shape=}\")\n",
    "\n",
    "        batch_size = hidden_states.shape[0]\n",
    "        hidden_states = hidden_states.reshape(-1, self.hidden_size)  # (num_tokens, hidden_size)\n",
    "        logger.debug(f\"{hidden_states.shape=}\")\n",
    "\n",
    "        num_experts = routing_weights.shape[1]\n",
    "\n",
    "        # CPUã¾ãŸã¯è¨“ç·´æ™‚ã®å ´åˆ\n",
    "        if hidden_states.device.type == \"cpu\" or self.training:\n",
    "\n",
    "            # çµæžœã‚’æ ¼ç´ã™ã‚‹ã‚¢ã‚­ãƒ¥ãƒ¼ãƒ ãƒ¬ãƒ¼ã‚¿ã‚’åˆæœŸåŒ–\n",
    "            next_states = torch.zeros_like(hidden_states, dtype=hidden_states.dtype, device=hidden_states.device)\n",
    "            logger.debug(f\"{next_states.shape=}\")\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®å‰²ã‚Šå½“ã¦ã‚’ãƒ¯ãƒ³ãƒ›ãƒƒãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°\n",
    "                expert_mask = torch.nn.functional.one_hot(\n",
    "                    router_indices, num_classes=num_experts + 1\n",
    "                )  # masking is also a class\n",
    "                logger.debug(f\"{expert_mask.shape=}\")\n",
    "\n",
    "                expert_mask = expert_mask.permute(2, 1, 0)\n",
    "                logger.debug(f\"{expert_mask.shape=}\")\n",
    "\n",
    "                # å®Ÿéš›ã«1å›žã§ã‚‚é¸ã°ã‚ŒãŸã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã‚’å–å¾—\n",
    "                # we sum on the top_k and on the sequence length to get which experts\n",
    "                # are hit this time around\n",
    "                expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n",
    "                logger.debug(f\"{expert_hit.shape=}\")\n",
    "\n",
    "            # é¸ã°ã‚ŒãŸã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã”ã¨ã«ãƒ«ãƒ¼ãƒ—\n",
    "            for expert_idx in expert_hit[:]:\n",
    "                # expert_idx only have 1 element, so we can use scale for fast indexing\n",
    "                expert_idx = expert_idx[0]\n",
    "                logger.debug(f\"ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆå‡¦ç†ä¸­ {expert_idx=}\")\n",
    "\n",
    "                # skip masking index\n",
    "                if expert_idx == num_experts:\n",
    "                    continue\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    # å‡¦ç†ã™ã‚‹ãƒˆãƒ¼ã‚¯ãƒ³ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æŠ½å‡º\n",
    "                    _, token_idx = torch.where(expert_mask[expert_idx])\n",
    "                    logger.debug(f\"{token_idx.shape=}\")\n",
    "\n",
    "                # ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆãŒå‡¦ç†ã™ã‚‹ãƒˆãƒ¼ã‚¯ãƒ³ã‚’æŠ½å‡º\n",
    "                current_state = hidden_states[token_idx]\n",
    "                logger.debug(f\"{current_state.shape=}\")\n",
    "\n",
    "                # ã‚²ãƒ¼ãƒˆãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã¨ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’åŒæ™‚ã«é©ç”¨\n",
    "                gate_up = current_state @ self.gate_up_proj[expert_idx] + self.gate_up_proj_bias[expert_idx]\n",
    "                logger.debug(f\"{gate_up.shape=}\")\n",
    "                gate, up = gate_up[..., ::2], gate_up[..., 1::2]\n",
    "                logger.debug(f\"{gate.shape=}, {up.shape=}\")\n",
    "\n",
    "                # ã‚²ãƒ¼ãƒˆãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã®å€¤ã‚’ã‚¯ãƒªãƒƒãƒ—\n",
    "                gate = gate.clamp(min=None, max=self.limit)\n",
    "\n",
    "                # ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã®å€¤ã‚’ã‚¯ãƒªãƒƒãƒ—\n",
    "                up = up.clamp(min=-self.limit, max=self.limit)\n",
    "\n",
    "                # alphaã§ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã—ãŸã‚«ã‚¹ã‚¿ãƒ SwiGLUã®ã‚²ãƒ¼ãƒˆè¨ˆç®—ã‚’å®Ÿè¡Œ\n",
    "                glu = gate * torch.sigmoid(gate * self.alpha)\n",
    "\n",
    "                # ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã«1ã‚’è¶³ã—ã¦ã‹ã‚‰ã€ã‚²ãƒ¼ãƒˆã‚’æŽ›ã‘åˆã‚ã›ã‚‹ï¼ˆæ®‹å·®æŽ¥ç¶šã¨ç­‰ä¾¡ï¼‰\n",
    "                gated_output = (up + 1) * glu\n",
    "\n",
    "                # ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’é©ç”¨\n",
    "                out = gated_output @ self.down_proj[expert_idx] + self.down_proj_bias[expert_idx]\n",
    "                logger.debug(f\"{out.shape=}\")\n",
    "\n",
    "                # ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°é‡ã¿ã§å‡ºåŠ›ã‚’é‡ã¿ä»˜ã‘\n",
    "                weighted_output = out * routing_weights[token_idx, expert_idx, None]\n",
    "                logger.debug(f\"{weighted_output.shape=}\")\n",
    "\n",
    "                # ã‚¢ã‚­ãƒ¥ãƒ¼ãƒ ãƒ¬ãƒ¼ã‚¿ã«åŠ ç®—\n",
    "                next_states.index_add_(0, token_idx, weighted_output.to(hidden_states.dtype))\n",
    "\n",
    "            next_states = next_states.view(batch_size, -1, self.hidden_size)\n",
    "\n",
    "        # GPUã‹ã¤æŽ¨è«–æ™‚ã®å ´åˆ\n",
    "        else:\n",
    "            # å…¥åŠ›ã‚’ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆæ•°åˆ†ç¹°ã‚Šè¿”ã™\n",
    "            hidden_states = hidden_states.repeat(num_experts, 1)\n",
    "            logger.debug(f\"{hidden_states.shape=}\")\n",
    "\n",
    "            # ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã”ã¨ã«ãƒãƒƒãƒã‚’åˆ†å‰²\n",
    "            hidden_states = hidden_states.view(num_experts, -1, self.hidden_size)\n",
    "            logger.debug(f\"{hidden_states.shape=}\")\n",
    "\n",
    "            # ãƒãƒƒãƒè¡Œåˆ—ç©ï¼ˆbmmï¼‰ã‚’ä½¿ç”¨ã—ã¦å…¨ã¦ã®ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã‚’ä¸¦åˆ—å‡¦ç†\n",
    "            # ã‹ã¤ã€ã‚²ãƒ¼ãƒˆãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã¨ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’åŒæ™‚ã«é©ç”¨\n",
    "            gate_up = torch.bmm(hidden_states, self.gate_up_proj) + self.gate_up_proj_bias[..., None, :]\n",
    "            logger.debug(f\"{gate_up.shape=}\")\n",
    "            gate, up = gate_up[..., ::2], gate_up[..., 1::2]\n",
    "            logger.debug(f\"{gate.shape=}, {up.shape=}\")\n",
    "\n",
    "            # ã‚²ãƒ¼ãƒˆãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã®å€¤ã‚’ã‚¯ãƒªãƒƒãƒ—\n",
    "            gate = gate.clamp(min=None, max=self.limit)\n",
    "            logger.debug(f\"{gate.shape=}\")\n",
    "\n",
    "            # ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã®å€¤ã‚’ã‚¯ãƒªãƒƒãƒ—\n",
    "            up = up.clamp(min=-self.limit, max=self.limit)\n",
    "            logger.debug(f\"{up.shape=}\")\n",
    "\n",
    "            # alphaã§ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã—ãŸã‚«ã‚¹ã‚¿ãƒ SwiGLUã®ã‚²ãƒ¼ãƒˆè¨ˆç®—ã‚’å®Ÿè¡Œ\n",
    "            glu = gate * torch.sigmoid(gate * self.alpha)\n",
    "\n",
    "            # ã‚¢ãƒƒãƒ—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã«1ã‚’è¶³ã—ã¦ã‹ã‚‰ã€ã‚²ãƒ¼ãƒˆã‚’æŽ›ã‘åˆã‚ã›ã‚‹ï¼ˆæ®‹å·®æŽ¥ç¶šã¨ç­‰ä¾¡ï¼‰\n",
    "            # ãã—ã¦ãƒ€ã‚¦ãƒ³ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’é©ç”¨\n",
    "            next_states = torch.bmm(((up + 1) * glu), self.down_proj)\n",
    "            next_states = next_states + self.down_proj_bias[..., None, :]\n",
    "            logger.debug(f\"{next_states.shape=}\")\n",
    "\n",
    "            # ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã”ã¨ã«åˆ†å‰²\n",
    "            next_states = next_states.view(num_experts, batch_size, -1, self.hidden_size)\n",
    "            logger.debug(f\"{next_states.shape=}\")\n",
    "\n",
    "            # ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°é‡ã¿ã§ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®å‡ºåŠ›ã‚’é‡ã¿ä»˜ã‘\n",
    "            next_states = next_states * routing_weights.transpose(0, 1).view(num_experts, batch_size, -1)[..., None]\n",
    "            logger.debug(f\"{next_states.shape=}\")\n",
    "\n",
    "            # å…¨ã¦ã®ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®å‡ºåŠ›ã‚’åˆè¨ˆ\n",
    "            next_states = next_states.sum(dim=0)\n",
    "\n",
    "        logger.info(f\"GptOssExpertsã®é †ä¼æ’­å®Œäº† {next_states.shape=}, {next_states.dtype=}\")\n",
    "        return next_states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0c3f97",
   "metadata": {},
   "source": [
    "### GptOssTopKRouter\n",
    "\n",
    "GptOssTopKRouterã¯ã€ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã«ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å‰²ã‚Šå½“ã¦ã‚‹ãƒ«ãƒ¼ã‚¿ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å®Ÿè£…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6b8a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GptOssTopKRouter(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        logger.info(f\"GptOssTopKRouterã‚’åˆæœŸåŒ–é–‹å§‹ {config.num_experts_per_tok=}, {config.num_local_experts=}, {config.hidden_size=}\")\n",
    "        super().__init__()\n",
    "\n",
    "        # ä¸Šä½kå€‹ã®ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆæ•°ã‚’è¨­å®š\n",
    "        # 4\n",
    "        self.top_k = config.num_experts_per_tok\n",
    "\n",
    "        # ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆæ•°\n",
    "        # 32\n",
    "        self.num_experts = config.num_local_experts\n",
    "\n",
    "        # éš ã‚Œå±¤ã®æ¬¡å…ƒæ•°\n",
    "        # 2880\n",
    "        self.hidden_dim = config.hidden_size\n",
    "\n",
    "        # ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã®é‡ã¿ã‚’åˆæœŸåŒ–\n",
    "        # (32, 2880)\n",
    "        self.weight = nn.Parameter(torch.empty(self.num_experts, self.hidden_dim))\n",
    "\n",
    "        # ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã®ãƒã‚¤ã‚¢ã‚¹ã‚’åˆæœŸåŒ–\n",
    "        # (32,)\n",
    "        self.bias = nn.Parameter(torch.empty(self.num_experts))\n",
    "        logger.info(\"GptOssTopKRouterã®åˆæœŸåŒ–å®Œäº†\")\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        logger.info(f\"GptOssTopKRouterã®é †ä¼æ’­é–‹å§‹ {hidden_states.shape=}, {hidden_states.dtype=}\")\n",
    "\n",
    "        # (ãƒãƒƒãƒã‚µã‚¤ã‚º*ãƒˆãƒ¼ã‚¯ãƒ³æ•°, éš ã‚Œå±¤ã®æ¬¡å…ƒæ•°)ã«å¤‰å½¢\n",
    "        hidden_states = hidden_states.reshape(-1, self.hidden_dim)\n",
    "        logger.debug(f\"{hidden_states.shape=}\")\n",
    "\n",
    "        # ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã®ãƒ­ã‚¸ãƒƒãƒˆã‚’è¨ˆç®—\n",
    "        router_logits = F.linear(hidden_states, self.weight, self.bias)  # (seq_len, num_experts)\n",
    "        logger.debug(f\"{router_logits.shape=}\")\n",
    "\n",
    "        # ä¸Šä½kå€‹ã®ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã‚’é¸æŠž\n",
    "        router_top_value, router_indices = torch.topk(router_logits, self.top_k, dim=-1)  # (seq_len, top_k)\n",
    "        logger.debug(f\"{router_top_value.shape=}, {router_indices.shape=}\")\n",
    "\n",
    "        # ã‚½ãƒ•ãƒˆãƒžãƒƒã‚¯ã‚¹ã§ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã®é‡ã¿ã‚’è¨ˆç®—\n",
    "        router_top_value = torch.nn.functional.softmax(router_top_value, dim=1, dtype=router_top_value.dtype)\n",
    "        logger.debug(f\"{router_top_value.shape=}\")\n",
    "\n",
    "        # ã‚¹ãƒ‘ãƒ¼ã‚¹ãªãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã‚¹ã‚³ã‚¢ã‚’ä½œæˆ\n",
    "        router_scores = torch.zeros_like(router_logits).scatter_(1, router_indices, router_top_value)\n",
    "        logger.debug(f\"{router_scores.shape=}\")\n",
    "\n",
    "        logger.info(f\"GptOssTopKRouterã®é †ä¼æ’­å®Œäº† {router_scores.shape=}, {router_scores.dtype=}, {router_indices.shape=}, {router_indices.dtype=}\")\n",
    "        return router_scores, router_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50aa74b1",
   "metadata": {},
   "source": [
    "### GptOssMLP\n",
    "\n",
    "GptOssMLPã¯ã€MoEå±¤ã®å®Ÿè£…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bc91ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MegaBlocksMoeMLPã‚«ãƒ¼ãƒãƒ«ã‚’ä½¿ç”¨ï¼ˆhttps://huggingface.co/docs/kernels/main/en/indexï¼‰\n",
    "@use_kernel_forward_from_hub(\"MegaBlocksMoeMLP\")\n",
    "class GptOssMLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        logger.info(\"GptOssMLPã‚’åˆæœŸåŒ–é–‹å§‹\")\n",
    "        super().__init__()\n",
    "\n",
    "        # ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’åˆæœŸåŒ–\n",
    "        self.router = GptOssTopKRouter(config)\n",
    "\n",
    "        # è¤‡æ•°ã®ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã‚’åˆæœŸåŒ–\n",
    "        self.experts = GptOssExperts(config)\n",
    "        logger.info(\"GptOssMLPã®åˆæœŸåŒ–å®Œäº†\")\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        logger.info(f\"GptOssMLPã®é †ä¼æ’­é–‹å§‹ {hidden_states.shape=}, {hidden_states.dtype=}\")\n",
    "\n",
    "        # ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã‚¹ã‚³ã‚¢ã¨ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å–å¾—\n",
    "        router_scores, router_indices = self.router(hidden_states)  # (num_experts, seq_len)\n",
    "\n",
    "        # ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã«ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã—ã¦å‡ºåŠ›ã‚’å–å¾—\n",
    "        routed_out = self.experts(hidden_states, router_indices=router_indices, routing_weights=router_scores)\n",
    "\n",
    "        logger.info(f\"GptOssMLPã®é †ä¼æ’­å®Œäº† {routed_out.shape=}, {routed_out.dtype=}\")\n",
    "        return routed_out, router_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed39c481",
   "metadata": {},
   "source": [
    "### GptOssRotaryEmbedding\n",
    "\n",
    "GptOssRoutaryEmbeddingã¯ã€RoPEï¼ˆRotary Position Embedding, å›žè»¢ä½ç½®åŸ‹ã‚è¾¼ã¿ï¼‰ã®è¨ˆç®—ã«å¿…è¦ãªsinã¨cosã‚’è¨ˆç®—ã™ã‚‹ã‚¯ãƒ©ã‚¹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bc22db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GptOssRotaryEmbedding(nn.Module):\n",
    "\n",
    "    # é€†å‘¨æ³¢æ•°\n",
    "    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n",
    "\n",
    "    def __init__(self, config: GptOssConfig, device=None):\n",
    "        logger.info(f\"GptOssRotaryEmbeddingã‚’åˆæœŸåŒ–é–‹å§‹ {config.max_position_embeddings=} {config.rope_scaling=}\")\n",
    "\n",
    "        super().__init__()\n",
    "        # BC: \"rope_type\" was originally \"type\"\n",
    "        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n",
    "            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n",
    "        else:\n",
    "            self.rope_type = \"default\"\n",
    "\n",
    "        # 131072\n",
    "        self.max_seq_len_cached = config.max_position_embeddings\n",
    "\n",
    "        # 131072\n",
    "        self.original_max_seq_len = config.max_position_embeddings\n",
    "\n",
    "        self.config = config\n",
    "\n",
    "        # _compute_yarn_parameters\n",
    "        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n",
    "        logger.info(f\"RoPEã®åˆæœŸåŒ–é–¢æ•°: {self.rope_init_fn.__name__}\")\n",
    "\n",
    "        # 32, 1.3466\n",
    "        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n",
    "        logger.info(f\"é€†å‘¨æ³¢æ•°ã®è¨ˆç®— {inv_freq.shape=}, {inv_freq.dtype=}, {self.attention_scaling=}\")\n",
    "\n",
    "        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n",
    "\n",
    "        self.original_inv_freq = self.inv_freq\n",
    "        logger.info(\"GptOssRotaryEmbeddingã®åˆæœŸåŒ–å®Œäº†\")\n",
    "\n",
    "    @torch.no_grad()\n",
    "    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n",
    "    def forward(self, x, position_ids):\n",
    "        logger.info(f\"GptOssRotaryEmbeddingã®é †ä¼æ’­é–‹å§‹ {x.shape=}, {x.dtype=}, {position_ids.shape=}, {position_ids.dtype=}\")\n",
    "\n",
    "        # é€†å‘¨æ³¢æ•°ã‚’æ‹¡å¼µ\n",
    "        # (32,) -> (1, 32, 1) -> (1, 32, 1)\n",
    "        # float32\n",
    "        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n",
    "        logger.debug(f\"{inv_freq_expanded.shape=}, {inv_freq_expanded.dtype=}\")\n",
    "\n",
    "        # ä½ç½®IDã‚’æ‹¡å¼µ\n",
    "        # (1, 69) -> (1, 1, 69)\n",
    "        # int64 -> float32\n",
    "        position_ids_expanded = position_ids[:, None, :].float()\n",
    "        logger.debug(f\"{position_ids_expanded.shape=}, {position_ids_expanded.dtype=}\")\n",
    "\n",
    "        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n",
    "        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n",
    "            # ã‚¢ãƒƒãƒ—ã‚­ãƒ£ã‚¹ãƒˆã—ã¦è§’åº¦ã‚’è¨ˆç®—ï¼ˆå‘¨æ³¢æ•° * ä½ç½®ï¼‰\n",
    "            # (1, 32, 1) @ (1, 1, 69) -> (1, 32, 69) -> (1, 69, 32)\n",
    "            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n",
    "            emb = freqs\n",
    "\n",
    "            # ã‚³ã‚µã‚¤ãƒ³ã‚’è¨ˆç®—ã—ã€ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã‚’é©ç”¨\n",
    "            # (1, 69, 32)\n",
    "            cos = emb.cos() * self.attention_scaling\n",
    "\n",
    "            # ã‚µã‚¤ãƒ³ã‚’è¨ˆç®—ã—ã€ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã‚’é©ç”¨\n",
    "            # (1, 69, 32)\n",
    "            sin = emb.sin() * self.attention_scaling\n",
    "\n",
    "        logger.info(f\"GptOssRotaryEmbeddingã®é †ä¼æ’­å®Œäº† {cos.shape=}, {cos.dtype=}, {sin.shape=}, {sin.dtype=}\")\n",
    "        return cos.to(x.dtype), sin.to(x.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c149ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _apply_rotary_emb(\n",
    "    x: torch.Tensor,\n",
    "    cos: torch.Tensor,\n",
    "    sin: torch.Tensor,\n",
    ") -> torch.Tensor:\n",
    "    logger.info(f\"_apply_rotary_embã®é †ä¼æ’­é–‹å§‹ {x.shape=}, {x.dtype=}, {cos.shape=}, {cos.dtype=}, {sin.shape=}, {sin.dtype=}\")\n",
    "\n",
    "    first_half, second_half = torch.chunk(x, 2, dim=-1)\n",
    "    logger.debug(f\"{first_half.shape=}, {second_half.shape=}\")\n",
    "\n",
    "    first_ = first_half * cos - second_half * sin\n",
    "    logger.debug(f\"{first_.shape=}\")\n",
    "\n",
    "    second_ = second_half * cos + first_half * sin\n",
    "    logger.debug(f\"{second_.shape=}\")\n",
    "\n",
    "    res = torch.cat((first_, second_), dim=-1)\n",
    "    logger.info(f\"_apply_rotary_embã®é †ä¼æ’­å®Œäº† {res.shape=}, {res.dtype=}\")\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14294bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n",
    "    logger.info(f\"apply_rotary_pos_embã®é †ä¼æ’­é–‹å§‹ {q.shape=}, {q.dtype=}, {k.shape=}, {k.dtype=}, {cos.shape=}, {cos.dtype=}, {sin.shape=}, {sin.dtype=}, {position_ids=}, {unsqueeze_dim=}\")\n",
    "\n",
    "    cos = cos.unsqueeze(unsqueeze_dim)\n",
    "    logger.debug(f\"{cos.shape=}\")\n",
    "\n",
    "    sin = sin.unsqueeze(unsqueeze_dim)\n",
    "    logger.debug(f\"{sin.shape=}\")\n",
    "\n",
    "    q_embed = _apply_rotary_emb(q, cos, sin)\n",
    "\n",
    "    k_embed = _apply_rotary_emb(k, cos, sin)\n",
    "\n",
    "    logger.info(f\"apply_rotary_pos_embã®é †ä¼æ’­å®Œäº† {q_embed.shape=}, {q_embed.dtype=}, {k_embed.shape=}, {k_embed.dtype=}\")\n",
    "    return q_embed, k_embed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4637f65f",
   "metadata": {},
   "source": [
    "### GptOssAttention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a774ea9e",
   "metadata": {},
   "source": [
    "GptOssAttentionã¯ã€ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚·ãƒ³ã‚¯ã‚’æŒã¤ãƒžãƒ«ãƒãƒ˜ãƒƒãƒ‰ã‚»ãƒ«ãƒ•ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®å®Ÿè£…"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1b2516",
   "metadata": {},
   "source": [
    "repeat_kvã¯ã€GQAï¼ˆGrouped Query Attentionï¼‰ç”¨ã«ã‚­ãƒ¼ã¨ãƒãƒªãƒ¥ãƒ¼ã‚’è¤‡è£½ã™ã‚‹é–¢æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdc1dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n",
    "    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n",
    "    \"\"\"\n",
    "    logger.info(f\"repeat_kvã®é–‹å§‹ {hidden_states.shape=}, {hidden_states.dtype=}, {n_rep=}\")\n",
    "    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n",
    "    if n_rep == 1:\n",
    "        return hidden_states\n",
    "\n",
    "    # (1, 8, 69, 64) -> (1, 8, 8, 69, 64), bfloat16\n",
    "    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n",
    "    logger.debug(f\"{hidden_states.shape=}, {hidden_states.dtype=}\")\n",
    "\n",
    "    # (1, 8, 8, 69, 64) -> (1, 64, 69, 64), bfloat16\n",
    "    res = hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n",
    "    logger.info(f\"repeat_kvã®å®Œäº† {res.shape=}, {res.dtype=}\")\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e87d484",
   "metadata": {},
   "source": [
    "eager_attention_forwardã¯ã€ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚·ãƒ³ã‚¯ã‚’å®Ÿè£…ã—ãŸeagerãªï¼ˆåŽŸè‘—è«–æ–‡ã«å¿ å®Ÿã§éžåŠ¹çŽ‡ãªï¼‰å®Ÿè£…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e916936",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eager_attention_forward(\n",
    "    module: nn.Module,\n",
    "    query: torch.Tensor,\n",
    "    key: torch.Tensor,\n",
    "    value: torch.Tensor,\n",
    "    attention_mask: Optional[torch.Tensor],\n",
    "    scaling: float,\n",
    "    dropout: float = 0.0,\n",
    "    **kwargs,\n",
    "):\n",
    "    logger.info(f\"eager_attention_forwardã®é–‹å§‹ {query.shape=}, {key.shape=}, {value.shape=}, {scaling=}, {dropout=} {attention_mask.shape=}\")\n",
    "\n",
    "    # GQAç”¨ã«ã‚­ãƒ¼ã‚’è¤‡è£½\n",
    "    # (1, 8, 69, 64) -> (1, 64, 69, 64)\n",
    "    key_states = repeat_kv(key, module.num_key_value_groups)\n",
    "    logger.debug(f\"ã‚­ãƒ¼ã‚’è¤‡è£½ {key_states.shape=}, {key_states.dtype=}\")\n",
    "\n",
    "    # GQAç”¨ã«ãƒãƒªãƒ¥ãƒ¼ã‚’è¤‡è£½\n",
    "    # (1, 8, 69, 64) -> (1, 64, 69, 64)\n",
    "    value_states = repeat_kv(value, module.num_key_value_groups)\n",
    "    logger.debug(f\"ãƒãƒªãƒ¥ãƒ¼ã‚’è¤‡è£½ {value_states.shape=}, {value_states.dtype=}\")\n",
    "\n",
    "    # ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ­ã‚¸ãƒƒãƒˆã‚’è¨ˆç®—\n",
    "    # QK^T / sqrt(d_k) \n",
    "    # (1, 64, 69, 64) @ (1, 64, 64, 69) -> (1, 64, 69, 69)\n",
    "    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling\n",
    "    logger.debug(f\"ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ­ã‚¸ãƒƒãƒˆã‚’è¨ˆç®— {attn_weights.shape=}, {attn_weights.dtype=}\")\n",
    "\n",
    "    # å› æžœãƒžã‚¹ã‚¯ã‚’é©ç”¨\n",
    "    if attention_mask is not None:\n",
    "        # (1, 1, 69, 69) -> (1, 1, 69, 69)\n",
    "        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n",
    "        logger.debug(f\"{causal_mask.shape=}, {causal_mask.dtype=}\")\n",
    "        # (1, 64, 69, 69) + (1, 1, 69, 69) -> (1, 64, 69, 69), bfloat16\n",
    "        attn_weights = attn_weights + causal_mask\n",
    "        logger.debug(f\"å› æžœãƒžã‚¹ã‚¯é©ç”¨ {attn_weights.shape=}, {attn_weights.dtype=}\")\n",
    "\n",
    "    # ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚·ãƒ³ã‚¯ã‚’ä½œæˆ \n",
    "    # (1, 64, 69, 1), bfloat16\n",
    "    sinks = module.sinks.reshape(1, -1, 1, 1).expand(query.shape[0], -1, query.shape[-2], -1)\n",
    "    logger.debug(f\"{sinks.shape=}, {sinks.dtype=}\")\n",
    "\n",
    "    # ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ­ã‚¸ãƒƒãƒˆã«ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚·ãƒ³ã‚¯ã‚’è¿½åŠ \n",
    "    # (1, 64, 69, 69 + 1) -> (1, 64, 69, 70), bfloat16\n",
    "    combined_logits = torch.cat([attn_weights, sinks], dim=-1)\n",
    "    logger.debug(f\"ã‚·ãƒ³ã‚¯ã‚’è¿½åŠ  {combined_logits.shape=}, {combined_logits.dtype=}\")\n",
    "\n",
    "    # å®‰å®šåŒ–ã®ãŸã‚ã«æœ€å¤§å€¤ã‚’æ¸›ç®—\n",
    "    # (1, 65, 69, 70) - (1, 65, 69, 1) -> (1, 65, 69, 70), bfloat16\n",
    "    # This was not in the original implementation and slightly affect results; it prevents overflow in BF16/FP16\n",
    "    # when training with bsz>1 we clamp max values.\n",
    "    combined_logits = combined_logits - combined_logits.max(dim=-1, keepdim=True).values\n",
    "    logger.debug(f\"æœ€å¤§å€¤ã‚’æ¸›ç®— {combined_logits.shape=}, {combined_logits.dtype=}\")\n",
    "\n",
    "    # ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—\n",
    "    # (1, 64, 69, 70) -> (1, 64, 69, 70), bfloat16\n",
    "    probs = F.softmax(combined_logits, dim=-1, dtype=combined_logits.dtype)\n",
    "    logger.debug(f\"ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®— {probs.shape=}, {probs.dtype=}\")\n",
    "\n",
    "    # ã‚·ãƒ³ã‚¯ã®ç¢ºçŽ‡ã‚’é™¤åŽ»\n",
    "    # (1, 64, 69, 70) -> (1, 64, 69, 69), bfloat16\n",
    "    scores = probs[..., :-1]  # we drop the sink here\n",
    "    logger.debug(f\"ã‚·ãƒ³ã‚¯ã®ç¢ºçŽ‡ã‚’é™¤åŽ» {scores.shape=}, {scores.dtype=}\")\n",
    "\n",
    "    # ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆã‚’é©ç”¨\n",
    "    # (1, 64, 69, 69), bfloat16\n",
    "    attn_weights = nn.functional.dropout(scores, p=dropout, training=module.training)\n",
    "\n",
    "    # ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®—\n",
    "    # (1, 64, 69, 69) @ (1, 64, 69, 64) -> (1, 64, 69, 64)\n",
    "    attn_output = torch.matmul(attn_weights, value_states)\n",
    "    logger.debug(f\"ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ {attn_output.shape=}, {attn_output.dtype=}\")\n",
    "\n",
    "    # (1, 64, 69, 64) -> (1, 69, 64, 64)\n",
    "    attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "    logger.debug(f\"ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’è»¢ç½® {attn_output.shape=}, {attn_output.dtype=}\")\n",
    "\n",
    "    # (1, 69, 64, 64), bfloat16\n",
    "    logger.info(f\"eager_attention_forwardã®å®Œäº† {attn_output.shape=}, {attn_output.dtype=}\")\n",
    "    return attn_output, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103b2cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GptOssAttention(nn.Module):\n",
    "    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n",
    "\n",
    "    def __init__(self, config: GptOssConfig, layer_idx: int):\n",
    "        logger.info(f\"GptOssAttentionã‚’åˆæœŸåŒ–é–‹å§‹ {config.hidden_size=}, {config.num_attention_heads=}, {config.num_key_value_heads=}, {config.attention_bias=}, {config.attention_dropout=}, {layer_idx=}\")\n",
    "\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.layer_idx = layer_idx\n",
    "\n",
    "        self.head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n",
    "        logger.debug(f\"{self.head_dim=}\")\n",
    "\n",
    "        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads\n",
    "        logger.debug(f\"{self.num_key_value_groups=}\")\n",
    "\n",
    "        self.scaling = self.head_dim**-0.5\n",
    "        logger.debug(f\"{self.scaling=}\")\n",
    "\n",
    "        self.attention_dropout = config.attention_dropout\n",
    "        logger.debug(f\"{self.attention_dropout=}\")\n",
    "        \n",
    "        self.is_causal = True\n",
    "\n",
    "        # ã‚¯ã‚¨ãƒªã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³\n",
    "        # 2880 -> 64 * 64\n",
    "        self.q_proj = nn.Linear(\n",
    "            config.hidden_size, config.num_attention_heads * self.head_dim, bias=config.attention_bias\n",
    "        )\n",
    "\n",
    "        # ã‚­ãƒ¼ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³\n",
    "        self.k_proj = nn.Linear(\n",
    "            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n",
    "        )\n",
    "\n",
    "        # ãƒãƒªãƒ¥ãƒ¼ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³\n",
    "        self.v_proj = nn.Linear(\n",
    "            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n",
    "        )\n",
    "\n",
    "        # å‡ºåŠ›ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³\n",
    "        self.o_proj = nn.Linear(\n",
    "            config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n",
    "        )\n",
    "\n",
    "        # ã‚¹ãƒ©ã‚¤ãƒ‡ã‚£ãƒ³ã‚°ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã®è¨­å®š\n",
    "        self.sliding_window = config.sliding_window if config.layer_types[layer_idx] == \"sliding_attention\" else None\n",
    "        logger.debug(f\"{self.sliding_window=}\")\n",
    "\n",
    "        # ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚·ãƒ³ã‚¯ã®åˆæœŸåŒ–\n",
    "        self.sinks = nn.Parameter(torch.empty(config.num_attention_heads))\n",
    "\n",
    "        logger.info(\"GptOssAttentionã®åˆæœŸåŒ–å®Œäº†\")\n",
    "\n",
    "    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n",
    "        attention_mask: Optional[torch.Tensor],\n",
    "        past_key_values: Optional[Cache] = None,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "        **kwargs: Unpack[TransformersKwargs],\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        logger.info(f\"GptOssAttentionã®é †ä¼æ’­é–‹å§‹ {hidden_states.shape=}, {position_embeddings[0].shape=}, {position_embeddings[1].shape=}, {past_key_values=}, {cache_position.shape=}, {attention_mask.shape=}\")\n",
    "\n",
    "        # (1, 69, 2880) -> (1, 69)\n",
    "        input_shape = hidden_states.shape[:-1]\n",
    "        logger.debug(f\"{input_shape=}\")\n",
    "\n",
    "        # (1, 69, -1, 64)\n",
    "        hidden_shape = (*input_shape, -1, self.head_dim)\n",
    "        logger.debug(f\"{hidden_shape=}\")\n",
    "\n",
    "        # ã‚¯ã‚¨ãƒªã‚’è¨ˆç®—\n",
    "        # (1, 69, 2880) -> (1, 69, 4096) -> (1, 69, 64, 64) -> (1, 64, 69, 64)\n",
    "        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
    "        logger.debug(f\"ã‚¯ã‚¨ãƒªã‚’è¨ˆç®— {query_states.shape=}\")\n",
    "\n",
    "        # ã‚­ãƒ¼ã‚’è¨ˆç®—\n",
    "        # (1, 69, 2880) -> (1, 69, 4096) -> (1, 69, 64, 64) -> (1, 64, 69, 64)\n",
    "        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
    "        logger.debug(f\"ã‚­ãƒ¼ã‚’è¨ˆç®— {key_states.shape=}\")\n",
    "\n",
    "        # ãƒãƒªãƒ¥ãƒ¼ã‚’è¨ˆç®—\n",
    "        # (1, 69, 2880) -> (1, 69, 4096) -> (1, 69, 64, 64) -> (1, 64, 69, 64)\n",
    "        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
    "        logger.debug(f\"ãƒãƒªãƒ¥ãƒ¼ã‚’è¨ˆç®— {value_states.shape=}\")\n",
    "\n",
    "        # cosã¨sinã‚’å–å¾—\n",
    "        # (1, 69, 32), (1, 69, 32)\n",
    "        cos, sin = position_embeddings\n",
    "        logger.debug(f\"RoPEã®cosã¨sinã‚’å–å¾— {cos.shape=}, {sin.shape=}\")\n",
    "\n",
    "        # RoPEã‚’é©ç”¨\n",
    "        # (1, 64, 69, 64), (1, 64, 69, 64)\n",
    "        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n",
    "        logger.debug(f\"RoPEã‚’é©ç”¨ {query_states.shape=}, {key_states.shape=}\")\n",
    "\n",
    "        # ã‚­ãƒ¼ãƒãƒªãƒ¥ãƒ¼ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æ›´æ–°\n",
    "        if past_key_values is not None:\n",
    "            cache_kwargs = {\"cache_position\": cache_position}\n",
    "            # (1, 8, 69, 64), (1, 8, 69, 64)\n",
    "            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n",
    "            logger.debug(f\"ã‚­ãƒ¼ãƒãƒªãƒ¥ãƒ¼ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æ›´æ–° {key_states.shape=}, {value_states.shape=}\")\n",
    "\n",
    "        # ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®å®Ÿè£…ã‚’é¸æŠž\n",
    "        attention_interface: Callable = eager_attention_forward\n",
    "        if self.config._attn_implementation != \"eager\":\n",
    "            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n",
    "\n",
    "        # eager_attention_forward\n",
    "        logger.debug(f\"ä½¿ç”¨ã™ã‚‹ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å®Ÿè£…: {attention_interface.__name__}\")\n",
    "\n",
    "        # ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®—\n",
    "        # (1, 64, 69, 64), (1, 64, 69, 64), (1, 64, 69, 64) -> (1, 69, 64, 64), None\n",
    "        attn_output, attn_weights = attention_interface(\n",
    "            self,\n",
    "            query_states,\n",
    "            key_states,\n",
    "            value_states,\n",
    "            attention_mask,\n",
    "            dropout=0.0 if not self.training else self.attention_dropout,\n",
    "            scaling=self.scaling,\n",
    "            sliding_window=self.sliding_window,\n",
    "            s_aux=self.sinks,  # diff with Llama\n",
    "            **kwargs,\n",
    "        )\n",
    "        logger.debug(f\"ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ {attn_output.shape=}\")\n",
    "\n",
    "        # å‡ºåŠ›ã‚’å…ƒã®å½¢çŠ¶ã«æˆ»ã™\n",
    "        # (1, 69, 64, 64) -> (1, 69, 4096)\n",
    "        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n",
    "        logger.debug(f\"ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’å…ƒã®å½¢çŠ¶ã«æˆ»ã™ {attn_output.shape=}\")\n",
    "\n",
    "        # å‡ºåŠ›ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’é©ç”¨\n",
    "        # (1, 69, 4096) -> (1, 69, 2880)\n",
    "        attn_output = self.o_proj(attn_output)\n",
    "\n",
    "        logger.info(f\"GptOssAttentionã®é †ä¼æ’­å®Œäº† {attn_output.shape=}, {attn_output.dtype=}, {attn_weights.shape=}, {attn_weights.dtype=}\")\n",
    "        return attn_output, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c68a17f",
   "metadata": {},
   "source": [
    "### GptOssDecoderLayer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e1f709",
   "metadata": {},
   "source": [
    "GptOssDecoderLayerã¯ã€gpt-ossãƒ¢ãƒ‡ãƒ«ã®å˜ä¸€ã®Transformerã®ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼å±¤\n",
    "\n",
    "Pre-LNã‚’æŽ¡ç”¨ã—ã€ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã¨MoEå±¤ã®å‰ã«ãƒ¬ã‚¤ãƒ¤ãƒ¼æ­£è¦åŒ–å±¤ãŒå®Ÿè£…ã•ã‚Œã¦ã„ã‚‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd90c5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GptOssDecoderLayer(GradientCheckpointingLayer):\n",
    "    def __init__(self, config: GptOssConfig, layer_idx: int):\n",
    "        logger.info(f\"GptOssDecoderLayerã‚’åˆæœŸåŒ–é–‹å§‹ {config.hidden_size=}, {config.intermediate_size=}, {config.num_attention_heads=}, {config.num_key_value_heads=}, {config.attention_bias=}, {config.attention_dropout=}, {layer_idx=}\")\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # 2880\n",
    "        self.hidden_size = config.hidden_size\n",
    "\n",
    "        # ã‚»ãƒ«ãƒ•ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³\n",
    "        self.self_attn = GptOssAttention(config=config, layer_idx=layer_idx)\n",
    "\n",
    "        # MoEå±¤\n",
    "        self.mlp = GptOssMLP(config)\n",
    "\n",
    "        # 2ã¤ã®ãƒ¬ã‚¤ãƒ¤ãƒ¼æ­£è¦åŒ–å±¤\n",
    "        # 2880\n",
    "        self.input_layernorm = GptOssRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "        self.post_attention_layernorm = GptOssRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "\n",
    "        # ã‚¹ãƒ©ã‚¤ãƒ‡ã‚£ãƒ³ã‚°ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã¨ãƒ•ãƒ«ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãŒäº¤äº’ã«é¸æŠž\n",
    "        self.attention_type = config.layer_types[layer_idx]\n",
    "        logger.debug(f\"{self.attention_type=}\")\n",
    "\n",
    "        logger.info(\"GptOssDecoderLayerã®åˆæœŸåŒ–å®Œäº†\")\n",
    "\n",
    "    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[Cache] = None,\n",
    "        use_cache: Optional[bool] = False,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n",
    "        **kwargs: Unpack[TransformersKwargs],\n",
    "    ) -> torch.Tensor:\n",
    "\n",
    "        logger.info(f\"GptOssDecoderLayerã®é †ä¼æ’­é–‹å§‹ {hidden_states.shape=}, {hidden_states.dtype=}, {position_ids.shape=}, {past_key_values=}, {use_cache=}, {cache_position.shape=}, {position_embeddings[0].shape=}, {position_embeddings[1].shape=}, {attention_mask.shape=}\")\n",
    "\n",
    "        # æ®‹å·®æŽ¥ç¶šç”¨\n",
    "        # (1, 69, 2880)\n",
    "        residual = hidden_states\n",
    "\n",
    "        # ãƒ¬ã‚¤ãƒ¤ãƒ¼æ­£è¦åŒ–ã‚’é©ç”¨ï¼ˆPre-LNï¼‰\n",
    "        # (1, 69, 2880) -> (1, 69, 2880)\n",
    "        hidden_states = self.input_layernorm(hidden_states)\n",
    "\n",
    "        # ã‚»ãƒ«ãƒ•ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’é©ç”¨\n",
    "        # (1, 69, 2880) -> (1, 69, 2880)\n",
    "        hidden_states, _ = self.self_attn(\n",
    "            hidden_states=hidden_states, # (1, 69, 2880)\n",
    "            attention_mask=attention_mask, # (1, 1, 69, 69)\n",
    "            position_ids=position_ids, # (1, 69)\n",
    "            past_key_values=past_key_values,\n",
    "            use_cache=use_cache, # True\n",
    "            cache_position=cache_position, # (69,)\n",
    "            position_embeddings=position_embeddings, # ((1, 69, 32), (1, 69, 32))\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        # æ®‹å·®æŽ¥ç¶šã‚’é©ç”¨\n",
    "        # (1, 69, 2880) + (1, 69, 2880) -> (1, 69, 2880)\n",
    "        hidden_states = residual + hidden_states\n",
    "\n",
    "        # æ®‹å·®æŽ¥ç¶šç”¨\n",
    "        # (1, 69, 2880)\n",
    "        residual = hidden_states\n",
    "\n",
    "        # ãƒ¬ã‚¤ãƒ¤ãƒ¼æ­£è¦åŒ–ã‚’é©ç”¨ï¼ˆPre-LNï¼‰\n",
    "        # (1, 69, 2880) -> (1, 69, 2880)\n",
    "        hidden_states = self.post_attention_layernorm(hidden_states)\n",
    "\n",
    "        # MoEå±¤ã‚’é©ç”¨\n",
    "        # (1, 69, 2880) -> (1, 69, 2880)\n",
    "        logger.debug(f\"MoEå±¤ã®å…¥åŠ› {hidden_states.shape=}, {hidden_states.dtype=}\")\n",
    "        hidden_states, _ = self.mlp(hidden_states)  # diff with llama: router scores\n",
    "        logger.debug(f\"MoEå±¤ã®å‡ºåŠ› {hidden_states.shape=}, {hidden_states.dtype=}\")\n",
    "\n",
    "        # æ®‹å·®æŽ¥ç¶šã‚’é©ç”¨\n",
    "        # (1, 69, 2880) + (1, 69, 2880) -> (1, 69, 2880)\n",
    "        hidden_states = residual + hidden_states\n",
    "\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae49d90",
   "metadata": {},
   "source": [
    "### GptOssPreTrainedModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd20283",
   "metadata": {},
   "source": [
    "GptOssPreTrainedModelã¯ã€gpt-ossã®äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®è¨­å®šã‚¯ãƒ©ã‚¹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2813bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GptOssPreTrainedModel(PreTrainedModel):\n",
    "    config: GptOssConfig\n",
    "    base_model_prefix = \"model\"\n",
    "    supports_gradient_checkpointing = True\n",
    "    _no_split_modules = [\"GptOssDecoderLayer\"]\n",
    "    _skip_keys_device_placement = [\"past_key_values\"]\n",
    "    _supports_flash_attn = True\n",
    "    _supports_sdpa = False\n",
    "    _supports_flex_attn = True\n",
    "\n",
    "    _can_compile_fullgraph = True\n",
    "    _supports_attention_backend = True\n",
    "    _can_record_outputs = {\n",
    "        \"router_logits\": OutputRecorder(GptOssTopKRouter, index=0),\n",
    "        \"hidden_states\": GptOssDecoderLayer,\n",
    "        \"attentions\": GptOssAttention,\n",
    "    }\n",
    "    _keep_in_fp32_modules = [\"post_attention_layernorm\", \"input_layernorm\", \"norm\"]\n",
    "    _supports_flash_attention = False\n",
    "    _supports_flex_attention = False\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        std = self.config.initializer_range\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=std)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Parameter):\n",
    "            module.data.normal_(mean=0.0, std=std)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=std)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, GptOssRMSNorm):\n",
    "            module.weight.data.fill_(1.0)\n",
    "        elif isinstance(module, GptOssExperts):\n",
    "            module.gate_up_proj.data.normal_(mean=0.0, std=std)\n",
    "            module.gate_up_proj_bias.data.zero_()\n",
    "            module.down_proj.data.normal_(mean=0.0, std=std)\n",
    "            module.down_proj_bias.data.zero_()\n",
    "        elif isinstance(module, GptOssAttention):\n",
    "            module.sinks.data.normal_(mean=0.0, std=std)\n",
    "        elif isinstance(module, GptOssTopKRouter):\n",
    "            module.weight.data.normal_(mean=0.0, std=std)\n",
    "            module.bias.data.normal_(mean=0.0, std=std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af61303",
   "metadata": {},
   "source": [
    "### GptOssModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c273ead0",
   "metadata": {},
   "source": [
    "GptOssModelã¯ã€è¤‡æ•°ã®GptOssDecoderLayerã‚’é‡ã­ãŸãƒ™ãƒ¼ã‚¹ã‚¯ãƒ©ã‚¹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d7305e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GptOssModel(GptOssPreTrainedModel):\n",
    "    _no_split_modules = [\"GptOssDecoderLayer\"]\n",
    "\n",
    "    def __init__(self, config: GptOssConfig):\n",
    "        logger.info(f\"GptOssModelã‚’åˆæœŸåŒ–é–‹å§‹ {config.vocab_size=}, {config.hidden_size=}, {config.num_hidden_layers=}, {config.pad_token_id=}\")\n",
    "        super().__init__(config)\n",
    "\n",
    "        # 199999\n",
    "        self.padding_idx = config.pad_token_id\n",
    "\n",
    "        # 201088\n",
    "        self.vocab_size = config.vocab_size\n",
    "\n",
    "        # å…¥åŠ›ã®åŸ‹ã‚è¾¼ã¿å±¤\n",
    "        # 202112 -> 2880\n",
    "        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n",
    "\n",
    "        # ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼å±¤ã‚’ã‚¹ã‚¿ãƒƒã‚¯\n",
    "        # 24å±¤\n",
    "        self.layers = nn.ModuleList(\n",
    "            [GptOssDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n",
    "        )\n",
    "\n",
    "        # æœ€çµ‚ãƒ¬ã‚¤ãƒ¤ãƒ¼æ­£è¦åŒ–å±¤\n",
    "        # 2880\n",
    "        self.norm = GptOssRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "\n",
    "        # RoPEã®sinã¨cosã®è¨ˆç®—ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«\n",
    "        self.rotary_emb = GptOssRotaryEmbedding(config=config)\n",
    "\n",
    "        self.gradient_checkpointing = False\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "        logger.info(\"GptOssModelã®åˆæœŸåŒ–å®Œäº†\")\n",
    "\n",
    "    @check_model_inputs\n",
    "    @auto_docstring\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.LongTensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[Cache] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "        **kwargs: Unpack[TransformersKwargs],\n",
    "    ) -> MoeModelOutputWithPast:\n",
    "        logger.info(f\"GptOssModelã®é †ä¼æ’­é–‹å§‹ {input_ids.shape=}, {position_ids.shape=}, {past_key_values=}, {inputs_embeds=}, {use_cache=}, {cache_position.shape=}, {attention_mask.shape=}\")\n",
    "\n",
    "        if (input_ids is None) ^ (inputs_embeds is not None):\n",
    "            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n",
    "\n",
    "        if use_cache and past_key_values is None:\n",
    "            past_key_values = DynamicCache(config=self.config)\n",
    "            logger.info(\"DynamicCacheã‚’åˆæœŸåŒ–\")\n",
    "\n",
    "        if inputs_embeds is None:\n",
    "            # (1, 69) -> (1, 69, 2880)\n",
    "            inputs_embeds = self.embed_tokens(input_ids)\n",
    "            logger.debug(f\"å…¥åŠ›ã®åŸ‹ã‚è¾¼ã¿ã‚’è¨ˆç®— {inputs_embeds.shape=}, {inputs_embeds.dtype=}\")\n",
    "\n",
    "        if cache_position is None:\n",
    "            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n",
    "            cache_position = torch.arange(\n",
    "                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n",
    "            )\n",
    "            logger.info(f\"cache_positionã‚’è¨ˆç®— {cache_position.shape=}, {cache_position.dtype=}\")\n",
    "\n",
    "        if position_ids is None:\n",
    "            position_ids = cache_position.unsqueeze(0)\n",
    "            logger.info(f\"position_idsã‚’è¨ˆç®— {position_ids.shape=}, {position_ids.dtype=}\")\n",
    "\n",
    "        # It may already have been prepared by e.g. `generate`\n",
    "        if not isinstance(causal_mask_mapping := attention_mask, dict):\n",
    "            mask_kwargs = {\n",
    "                \"config\": self.config,\n",
    "                \"input_embeds\": inputs_embeds,\n",
    "                \"attention_mask\": attention_mask,\n",
    "                \"cache_position\": cache_position,\n",
    "                \"past_key_values\": past_key_values,\n",
    "            }\n",
    "            causal_mask_mapping = {\n",
    "                \"full_attention\": create_causal_mask(**mask_kwargs), # (1, 1, 69, 69)\n",
    "                \"sliding_attention\": create_sliding_window_causal_mask(**mask_kwargs), # (1, 1, 69, 69)\n",
    "            }\n",
    "            logger.debug(f\"å› æžœãƒžã‚¹ã‚¯ã‚’æº–å‚™ {causal_mask_mapping['full_attention'].shape=}, {causal_mask_mapping['sliding_attention'].shape=}\")\n",
    "\n",
    "        hidden_states = inputs_embeds\n",
    "\n",
    "        # RoPEã®sinã¨cosã‚’è¨ˆç®—ï¼ˆã™ã¹ã¦ã®å±¤ã§å…±æœ‰ï¼‰\n",
    "        # (1, 1, 69), (1, 1, 69)\n",
    "        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n",
    "        logger.debug(f\"{position_embeddings[0].shape=}, {position_embeddings[1].shape=}\")\n",
    "\n",
    "        # ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼å±¤ã‚’é †ä¼æ’­\n",
    "        # (1, 69, 2880) -> (1, 69, 2880)\n",
    "        for decoder_layer in self.layers:\n",
    "            hidden_states = decoder_layer(\n",
    "                hidden_states, # (1, 69, 2880)\n",
    "                attention_mask=causal_mask_mapping[decoder_layer.attention_type], # (1, 1, 69, 69)\n",
    "                position_ids=position_ids, # (1, 69)\n",
    "                past_key_values=past_key_values,\n",
    "                use_cache=use_cache, # True\n",
    "                cache_position=cache_position, # (69,)\n",
    "                position_embeddings=position_embeddings, # ((1, 1, 69), (1, 1, 69))\n",
    "                **kwargs,\n",
    "            )\n",
    "        \n",
    "        # æœ€çµ‚ãƒ¬ã‚¤ãƒ¤ãƒ¼æ­£è¦åŒ–ã‚’é©ç”¨\n",
    "        # (1, 69, 2880) -> (1, 69, 2880)\n",
    "        hidden_states = self.norm(hidden_states)\n",
    "\n",
    "        logger.info(f\"GptOssModelã®é †ä¼æ’­å®Œäº† {hidden_states.shape=}, {hidden_states.dtype=}\")\n",
    "        return MoeModelOutputWithPast(\n",
    "            last_hidden_state=hidden_states,\n",
    "            past_key_values=past_key_values,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d40efc",
   "metadata": {},
   "source": [
    "### GptOssForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfcd06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_balancing_loss_func(\n",
    "    gate_logits: Union[torch.Tensor, tuple[torch.Tensor], None],\n",
    "    num_experts: Optional[int] = None,\n",
    "    top_k=2,\n",
    "    attention_mask: Optional[torch.Tensor] = None,\n",
    ") -> Union[torch.Tensor, int]:\n",
    "    r\"\"\"\n",
    "    Computes auxiliary load balancing loss as in Switch Transformer - implemented in Pytorch.\n",
    "\n",
    "    See Switch Transformer (https://huggingface.co/papers/2101.03961) for more details. This function implements the loss\n",
    "    function presented in equations (4) - (6) of the paper. It aims at penalizing cases where the routing between\n",
    "    experts is too unbalanced.\n",
    "\n",
    "    Args:\n",
    "        gate_logits:\n",
    "            Logits from the `gate`, should be a tuple of model.config.num_hidden_layers tensors of\n",
    "            shape [batch_size X sequence_length, num_experts].\n",
    "        num_experts:\n",
    "            Number of experts\n",
    "        top_k:\n",
    "            The number of experts to route per-token, can be also interpreted as the `top-k` routing\n",
    "            parameter.\n",
    "        attention_mask (`torch.Tensor`, *optional*):\n",
    "            The attention_mask used in forward function\n",
    "            shape [batch_size X sequence_length] if not None.\n",
    "\n",
    "    Returns:\n",
    "        The auxiliary loss.\n",
    "    \"\"\"\n",
    "    if gate_logits is None or not isinstance(gate_logits, tuple):\n",
    "        return 0\n",
    "\n",
    "    if isinstance(gate_logits, tuple):\n",
    "        compute_device = gate_logits[0].device\n",
    "        concatenated_gate_logits = torch.cat([layer_gate.to(compute_device) for layer_gate in gate_logits], dim=0)\n",
    "\n",
    "    routing_weights = torch.nn.functional.softmax(concatenated_gate_logits, dim=-1)\n",
    "\n",
    "    _, selected_experts = torch.topk(routing_weights, top_k, dim=-1)\n",
    "\n",
    "    expert_mask = torch.nn.functional.one_hot(selected_experts, num_experts)\n",
    "\n",
    "    if attention_mask is None:\n",
    "        # Compute the percentage of tokens routed to each experts\n",
    "        tokens_per_expert = torch.mean(expert_mask.float(), dim=0)\n",
    "\n",
    "        # Compute the average probability of routing to these experts\n",
    "        router_prob_per_expert = torch.mean(routing_weights, dim=0)\n",
    "    else:\n",
    "        batch_size, sequence_length = attention_mask.shape\n",
    "        num_hidden_layers = concatenated_gate_logits.shape[0] // (batch_size * sequence_length)\n",
    "\n",
    "        # Compute the mask that masks all padding tokens as 0 with the same shape of expert_mask\n",
    "        expert_attention_mask = (\n",
    "            attention_mask[None, :, :, None, None]\n",
    "            .expand((num_hidden_layers, batch_size, sequence_length, top_k, num_experts))\n",
    "            .reshape(-1, top_k, num_experts)\n",
    "            .to(compute_device)\n",
    "        )\n",
    "\n",
    "        # Compute the percentage of tokens routed to each experts\n",
    "        tokens_per_expert = torch.sum(expert_mask.float() * expert_attention_mask, dim=0) / torch.sum(\n",
    "            expert_attention_mask, dim=0\n",
    "        )\n",
    "\n",
    "        # Compute the mask that masks all padding tokens as 0 with the same shape of tokens_per_expert\n",
    "        router_per_expert_attention_mask = (\n",
    "            attention_mask[None, :, :, None]\n",
    "            .expand((num_hidden_layers, batch_size, sequence_length, num_experts))\n",
    "            .reshape(-1, num_experts)\n",
    "            .to(compute_device)\n",
    "        )\n",
    "\n",
    "        # Compute the average probability of routing to these experts\n",
    "        router_prob_per_expert = torch.sum(routing_weights * router_per_expert_attention_mask, dim=0) / torch.sum(\n",
    "            router_per_expert_attention_mask, dim=0\n",
    "        )\n",
    "\n",
    "    overall_loss = torch.sum(tokens_per_expert * router_prob_per_expert.unsqueeze(0))\n",
    "    return overall_loss * num_experts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf01d5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GptOssForCausalLM(GptOssPreTrainedModel, GenerationMixin):\n",
    "    _tied_weights_keys = [\"lm_head.weight\"]\n",
    "    _tp_plan = {\"lm_head\": \"colwise_rep\"}\n",
    "    _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n",
    "\n",
    "    def __init__(self, config):\n",
    "        logger.info(f\"GptOssForCausalLMã‚’åˆæœŸåŒ–é–‹å§‹ {config.vocab_size=}, {config.hidden_size=}, {config.num_hidden_layers=}, {config.pad_token_id=}, {config.router_aux_loss_coef=}, {config.num_local_experts=}, {config.num_experts_per_tok=}\")\n",
    "\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.model = GptOssModel(config)\n",
    "\n",
    "        # 201088\n",
    "        self.vocab_size = config.vocab_size\n",
    "\n",
    "        # 2880 -> 201088\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "\n",
    "        # 0.9\n",
    "        self.router_aux_loss_coef = config.router_aux_loss_coef\n",
    "\n",
    "        # 32\n",
    "        self.num_experts = config.num_local_experts\n",
    "\n",
    "        # 4\n",
    "        self.num_experts_per_tok = config.num_experts_per_tok\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "        logger.info(\"GptOssForCausalLMã®åˆæœŸåŒ–å®Œäº†\")\n",
    "\n",
    "    @can_return_tuple\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.LongTensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[Cache] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_router_logits: Optional[bool] = None,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "        logits_to_keep: Union[int, torch.Tensor] = 0,\n",
    "        **kwargs: Unpack[TransformersKwargs],\n",
    "    ) -> MoeCausalLMOutputWithPast:\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n",
    "            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n",
    "            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n",
    "\n",
    "        Example:\n",
    "\n",
    "        ```python\n",
    "        >>> from transformers import AutoTokenizer, GptOssForCausalLM\n",
    "\n",
    "        >>> model = GptOssForCausalLM.from_pretrained(\"mistralai/GptOss-8x7B-v0.1\")\n",
    "        >>> tokenizer = AutoTokenizer.from_pretrained(\"mistralai/GptOss-8x7B-v0.1\")\n",
    "\n",
    "        >>> prompt = \"Hey, are you conscious? Can you talk to me?\"\n",
    "        >>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "        >>> # Generate\n",
    "        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n",
    "        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "        \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\n",
    "        ```\"\"\"\n",
    "        logger.info(f\"GptOssForCausalLMã®é †ä¼æ’­é–‹å§‹ {input_ids.shape=}, {position_ids.shape=}, {past_key_values=}, {use_cache=}, {cache_position.shape=}, {logits_to_keep=}, {attention_mask.shape=}\")\n",
    "\n",
    "        # Falseï¼ˆãƒ«ãƒ¼ã‚¿ãƒ¼ã®ãƒ­ã‚¸ãƒƒãƒˆã‚’å‡ºåŠ›ã—ãªã„ï¼‰\n",
    "        output_router_logits = (\n",
    "            output_router_logits if output_router_logits is not None else self.config.output_router_logits\n",
    "        )\n",
    "        logger.debug(f\"{output_router_logits=}\")\n",
    "\n",
    "        # ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼ã‚’é †ä¼æ’­\n",
    "        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n",
    "        outputs: MoeModelOutputWithPast = self.model(\n",
    "            input_ids=input_ids, # (1, 69)\n",
    "            attention_mask=attention_mask, # (1, 69)\n",
    "            position_ids=position_ids, # (1, 69)\n",
    "            past_key_values=past_key_values,\n",
    "            inputs_embeds=inputs_embeds, # None\n",
    "            use_cache=use_cache, # True\n",
    "            output_router_logits=output_router_logits, # False\n",
    "            cache_position=cache_position, # (69,)\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "        logger.debug(f\"æœ€çµ‚éš ã‚ŒçŠ¶æ…‹ã‚’å–å¾— {hidden_states.shape=}, {hidden_states.dtype=}\")\n",
    "\n",
    "        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n",
    "        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n",
    "        logger.debug(f\"ã‚¹ãƒ©ã‚¤ã‚¹ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’è¨ˆç®— {slice_indices=}\")\n",
    "\n",
    "        logits = self.lm_head(hidden_states[:, slice_indices, :])\n",
    "        logger.debug(f\"ãƒ­ã‚¸ãƒƒãƒˆã‚’è¨ˆç®— {logits.shape=}, {logits.dtype=}\")\n",
    "\n",
    "        # è¨“ç·´æ™‚ã¯æå¤±ã‚’è¨ˆç®—\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = self.loss_function(logits, labels, self.vocab_size, **kwargs)\n",
    "\n",
    "        # ãƒ«ãƒ¼ã‚¿ãƒ¼ã®è£œåŠ©æå¤±ã‚’è¨ˆç®—\n",
    "        aux_loss = None\n",
    "        if output_router_logits:\n",
    "            aux_loss = load_balancing_loss_func(\n",
    "                outputs.router_logits,\n",
    "                self.num_experts,\n",
    "                self.num_experts_per_tok,\n",
    "                attention_mask,\n",
    "            )\n",
    "            if labels is not None:\n",
    "                loss += self.router_aux_loss_coef * aux_loss.to(loss.device)  # make sure to reside in the same device\n",
    "\n",
    "        return MoeCausalLMOutputWithPast(\n",
    "            loss=loss,\n",
    "            aux_loss=aux_loss,\n",
    "            logits=logits,\n",
    "            past_key_values=outputs.past_key_values,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "            router_logits=outputs.router_logits,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abff63ac",
   "metadata": {},
   "source": [
    "### 1ãƒˆãƒ¼ã‚¯ãƒ³ã®ã¿æŽ¨è«–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03e5c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_id = \"openai/gpt-oss-20b\"\n",
    "\n",
    "# ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’èª­ã¿è¾¼ã¿\n",
    "# å®Ÿè£…ã¯PreTrainedTokenizerFast\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876725b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿\n",
    "# ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã¯MXFP4å½¢å¼\n",
    "# ãã®ã¾ã¾èª­ã¿è¾¼ã‚€ã¨VRAMã‚’15GBä»¥ä¸‹ã«ç¯€ç´„ã§ãã‚‹\n",
    "# bfloat16ã§èª­ã¿è¾¼ã‚€ã¨VRAMãŒ48GBå¿…è¦ã«ãªã‚‹\n",
    "# FlashAttention3ã‚’æœ‰åŠ¹ã«ã™ã‚‹ã¨ã‚ˆã‚Šé«˜é€ŸåŒ–ãŒå¯èƒ½\n",
    "\n",
    "model = GptOssForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=\"auto\",\n",
    "    # attn_implementation=\"kernels-community/vllm-flash-attn3\",\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b8be59",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [{\"role\": \"user\", \"content\": \"Hello!\"}]\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75600ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    reasoning_effort=\"medium\",\n",
    "    return_tensors=\"pt\",\n",
    "    tokenize=False,\n",
    ")\n",
    "raw_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8f8ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    reasoning_effort=\"medium\",\n",
    "    return_tensors=\"pt\",\n",
    "    tokenize=True,\n",
    "    return_dict=True,\n",
    ")\n",
    "inputs, inputs[\"input_ids\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a5bbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated = model.generate(**inputs, max_new_tokens=1)\n",
    "generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd67052",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(generated[0][inputs[\"input_ids\"].shape[-1]:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd66a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(generated[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd0f283",
   "metadata": {},
   "source": [
    "### ãƒ­ã‚°ã‚’ã‚ªãƒ•ã«ã—ã¦æŽ¨è«–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c7d059",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.setLevel(logging.ERROR)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Tell me recommended tourist attractions in Fukuoka city.\"},\n",
    "]\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    reasoning_effort=\"medium\",\n",
    "    return_tensors=\"pt\",\n",
    "    tokenize=True,\n",
    "    return_dict=True,\n",
    ")\n",
    "\n",
    "generated = model.generate(**inputs, max_new_tokens=4096)\n",
    "\n",
    "tokenizer.decode(generated[0][inputs[\"input_ids\"].shape[-1]:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43143c75",
   "metadata": {},
   "source": [
    "The user wants recommended tourist attractions in Fukuoka City. Provide a list, grouping by type. Provide brief explanation, practical tips, and maybe mention accessibility, operation hours, admission, best times, etc. Also could mention off-the-beaten path attractions, local experience, food spots, etc. Should be tailored, maybe mention some must-visit highlights: Fukuoka Castle ruins, Ohori Park, Fukuoka Tower, Canal City, Tenjin, Nakasu, Hakata Shrine, Momochi Seaside Park, etc. Might also suggest some local experiences like yatai stalls, ramen stalls, maybe some cultural experience. The user didn't provide language preference, but we output in English likely. Provide bullet points. If the user wants more detailed? But just \"recommended tourist attractions\" is asked. Let's provide a list with brief descriptions and tips. Also include a short note on transportation. Provide also a suggestion for a day itinerary? Possibly optional. Could mention a recommended day or two itinerary.\n",
    "\n",
    "Let's craft the response: structured with categories: historical sites, parks & nature, modern attractions, shopping & nightlife, local experience. Provide key details: how to get there, admission, times, best times, recommended activities. Also mention things like \"don't miss\" for each. Provide in a concise but thorough manner, maybe 10-12 highlights.\n",
    "\n",
    "Let's proceed.\n",
    "\n",
    "## Highlights of Fukuoka City â€“ A Quickâ€‘Guide for Visitors  \n",
    "\n",
    "Below is a curated list of the most iconic, offâ€‘beat, and â€œmustâ€‘tasteâ€ experiences in Fukuoka.  Iâ€™ve grouped them by theme and thrown in practical details (transport, opening hours, tips) so you can plan without fuss.\n",
    "\n",
    "| **Theme** | **Attraction** | **Why Itâ€™s Worth It** | **Key Practical Notes** |\n",
    "|-----------|----------------|----------------------|------------------------|\n",
    "| **Historic & Cultural** | **Fukuoka Castle Ruins (Maizuruâ€‘tÅ)** | The only surviving part of an old castle â€“ great for sweeping city views. | 10â€¯amâ€“6â€¯pm, free entry. Open all year. |\n",
    "| | **Hakata Shrine (Hakata Jinja)** | One of Fukuokaâ€™s three major shrines; a quiet spot to experience Shinto rituals. | 6â€¯amâ€“5â€¯pm, free. Wear respectful clothing when entering. |\n",
    "| | **Nokonoshima Island (Kokuraâ€‘Jima)** | An 18â€‘hectare island with botanical gardens, ponds, and a lighthouse. | Bus + ferry, 7â€¯amâ€“10â€¯pm. Great for a day trip. |\n",
    "| **Parks & Nature** | **Ohori Park** | Classic Japanese strolling pond â€“ ideal for a leisurely walk or paddle boat. | 7â€¯amâ€“10â€¯pm, free. Great spot for lunch around the tea house. |\n",
    "| | **Momochi Seaside Park** | Sand + sea + modern architecture (Fukuoka Tower). | 24â€¯hr, free. Check sunset times for best photos. |\n",
    "| | **Fukuoka City Zoo & Botanical Garden** | Familyâ€‘friendly with diverse animal species and a huge butterfly house. | 10â€¯amâ€“5â€¯pm, Â¥550 (adult). |\n",
    "| **Contemporary & Entertainment** | **Fukuoka Tower** | Highest building in Kyushu â€“ offers panoramic views and an observation deck. | 10â€¯amâ€“9â€¯pm, Â¥590. Visit midâ€‘afternoon for sunset views. |\n",
    "| | **Canal City Hakata** | Shopping, dining, and a â€œriverâ€ inside a mall. Great for streetâ€‘style photographers. | 10â€¯amâ€“10â€¯pm. Free to wander, but some attractions cost extra. |\n",
    "| | **Daimyoâ€‘sanjÅ** (Dazaifu) | Though technically outside Fukuoka City, itâ€™s a 30â€‘minute train ride and the cityâ€™s spiritual heart. | 9â€¯amâ€“7â€¯pm, free (except for some temples). |\n",
    "| **Shopping & Nightlife** | **Tenjin** (central business district) | The hub for fashion, electronics and underground shopping. | 10â€¯amâ€“9â€¯pm (stores). |\n",
    "| | **Nakasu** (redâ€‘light district) | Best for yatai (street food stalls), izakayas, and nightlife. | Evening to midnight. |\n",
    "| | **Hakata Station** | Iconic underground shopping and a great place for ramen. | 7â€¯amâ€“9â€¯pm (shops). |\n",
    "| **Local Experience** | **Yatai Stalls** | Street food tents selling ramen, yakitori, gyoza, etc. | Late afternoon to midnight. Great for trying local cuisine. |\n",
    "| | **Matsusakaâ€‘tai (Japanese horse mackerel) at **Matsuzakaâ€‘Taiya** | Fresh seafood at a local market. | Earlyâ€‘morning. |\n",
    "| | **Fukuoka Asian Art Museum** | Focus on contemporary Asian art. | 10â€¯amâ€“5â€¯pm; free entry. |\n",
    "\n",
    "---\n",
    "\n",
    "### Suggested Miniâ€‘Itineraries\n",
    "\n",
    "| **Duration** | **Day 1** | **Day 2** |\n",
    "|--------------|-----------|-----------|\n",
    "| **1 Day** | 1. Morning: Ohori Park & Fukuoka Tower (take photos of the city from the top).  <br>2. Lunch at Yatai (try Hakata ramen).  <br>3. Afternoon: Tenjin shopping & Canal City.  <br>4. Evening: Nakasu yatai and a stroll along the canal. | 1. Quick visit to Hakata Shrine in the morning.  <br>2. Lunch at Hakata Station (classic Hakata ramen).  <br>3. Explore Kokuraâ€‘Jima (or Momochi Seaside).  <br>4. Evening: Walk around the Fukuoka City Zoo or dine in Tenjin. |\n",
    "| **2 Days** | **Day 1**: As above.  <br>**Day 2**: Take a short train (JR Kagoshima Line) to **Dazaifu**: <br>â€” Kawaraâ€‘jimaâ€™s Daidoâ€‘ji Temple, <br>â€” Dazaifu Tenmangu, <br>â€” E-kyu (Japanâ€™s first museum of art and design). Return to Fukuoka for a night of yatai and a quick visit to the Fukuoka Art Museum. |  |\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Quick Tips\n",
    "\n",
    "- **Transportation**  \n",
    "  - **Nipponâ€‘Rail Pass (JR Kyushu)** gives unlimited rides on JR lines, including Fukuoka Cityâ€™s rapid (Hakata line).  \n",
    "  - **Fukuoka City Bus or the Canal City â€œYuhikukanâ€ (inâ€‘mall metro)** can get you to many attractions on foot.  \n",
    "  - A **NFC phone** or a preâ€‘loaded *Fukuoka City Tourist Card* can get you free or discounted admission to many attractions.\n",
    "\n",
    "- **Money**  \n",
    "  - Accepts contactless cards everywhere; 10â€‘yen coins are handy for vending machines.  \n",
    "  - 1,500â€“2,500â€¯JPY per attraction usually covers entrance plus a photo / small souvenir.\n",
    "\n",
    "- **Language**  \n",
    "  - Most tourism signs are bilingual.  \n",
    "  - A little Japanese can go a long way, especially in yatai stalls.\n",
    "\n",
    "- **Best Time to Visit**  \n",
    "  - **Spring (Marchâ€‘May)** for cherry blossoms around Ohori Park.  \n",
    "  - **Autumn (Octâ€‘Nov)** for mild temperatures and autumn foliage.  \n",
    "  - Avoid **summer** (hot & humid) unless you love beach & nightlife.\n",
    "\n",
    "- **Dining**  \n",
    "  - **Hakata ramen** (squidâ€‘shell broth) is a signature.  \n",
    "  - **Motsunabe** (offal stew) is a local hearty choice; try it in Nakasu for an authentic vibe.  \n",
    "  - **Matsusakaâ€‘tai** for fresh mackerel sashimi.\n",
    "\n",
    "---\n",
    "\n",
    "Enjoy your time in Fukuoka! Itâ€™s a city that mixes ultraâ€‘modern vibes with deep tradition, all under one vibrant, accessible skyline. If you want more in-depth itinerariesâ€”like 3â€‘day plans, travel tips by budget, or unique niche experiencesâ€”just let me know!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abbda90",
   "metadata": {},
   "source": [
    "```sh\n",
    "<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.\n",
    "Knowledge cutoff: 2024-06\n",
    "Current date: 2025-11-04\n",
    "\n",
    "Reasoning: medium\n",
    "\n",
    "# Valid channels: analysis, commentary, final. Channel must be included for every message.<|end|><|start|>user<|message|>Tell me recommended tourist attractions in Fukuoka city.<|end|><|start|>assistant<|channel|>analysis<|message|>The user wants recommended tourist attractions in Fukuoka City. Provide a list, grouping by type. Provide brief explanation, practical tips, and maybe mention accessibility, operation hours, admission, best times, etc. Also could mention off-the-beaten path attractions, local experience, food spots, etc. Should be tailored, maybe mention some must-visit highlights: Fukuoka Castle ruins, Ohori Park, Fukuoka Tower, Canal City, Tenjin, Nakasu, Hakata Shrine, Momochi Seaside Park, etc. Might also suggest some local experiences like yatai stalls, ramen stalls, maybe some cultural experience. The user didn't provide language preference, but we output in English likely. Provide bullet points. If the user wants more detailed? But just \"recommended tourist attractions\" is asked. Let's provide a list with brief descriptions and tips. Also include a short note on transportation. Provide also a suggestion for a day itinerary? Possibly optional. Could mention a recommended day or two itinerary.\n",
    "\n",
    "Let's craft the response: structured with categories: historical sites, parks & nature, modern attractions, shopping & nightlife, local experience. Provide key details: how to get there, admission, times, best times, recommended activities. Also mention things like \"don't miss\" for each. Provide in a concise but thorough manner, maybe 10-12 highlights.\n",
    "\n",
    "Let's proceed.<|end|><|start|>assistant<|channel|>final<|message|>## Highlights of Fukuoka City â€“ A Quickâ€‘Guide for Visitors  \n",
    "\n",
    "Below is a curated list of the most iconic, offâ€‘beat, and â€œmustâ€‘tasteâ€ experiences in Fukuoka.  Iâ€™ve grouped them by theme and thrown in practical details (transport, opening hours, tips) so you can plan without fuss.\n",
    "\n",
    "| **Theme** | **Attraction** | **Why Itâ€™s Worth It** | **Key Practical Notes** |\n",
    "|-----------|----------------|----------------------|------------------------|\n",
    "| **Historic & Cultural** | **Fukuoka Castle Ruins (Maizuruâ€‘tÅ)** | The only surviving part of an old castle â€“ great for sweeping city views. | 10â€¯amâ€“6â€¯pm, free entry. Open all year. |\n",
    "| | **Hakata Shrine (Hakata Jinja)** | One of Fukuokaâ€™s three major shrines; a quiet spot to experience Shinto rituals. | 6â€¯amâ€“5â€¯pm, free. Wear respectful clothing when entering. |\n",
    "| | **Nokonoshima Island (Kokuraâ€‘Jima)** | An 18â€‘hectare island with botanical gardens, ponds, and a lighthouse. | Bus + ferry, 7â€¯amâ€“10â€¯pm. Great for a day trip. |\n",
    "| **Parks & Nature** | **Ohori Park** | Classic Japanese strolling pond â€“ ideal for a leisurely walk or paddle boat. | 7â€¯amâ€“10â€¯pm, free. Great spot for lunch around the tea house. |\n",
    "| | **Momochi Seaside Park** | Sand + sea + modern architecture (Fukuoka Tower). | 24â€¯hr, free. Check sunset times for best photos. |\n",
    "| | **Fukuoka City Zoo & Botanical Garden** | Familyâ€‘friendly with diverse animal species and a huge butterfly house. | 10â€¯amâ€“5â€¯pm, Â¥550 (adult). |\n",
    "| **Contemporary & Entertainment** | **Fukuoka Tower** | Highest building in Kyushu â€“ offers panoramic views and an observation deck. | 10â€¯amâ€“9â€¯pm, Â¥590. Visit midâ€‘afternoon for sunset views. |\n",
    "| | **Canal City Hakata** | Shopping, dining, and a â€œriverâ€ inside a mall. Great for streetâ€‘style photographers. | 10â€¯amâ€“10â€¯pm. Free to wander, but some attractions cost extra. |\n",
    "| | **Daimyoâ€‘sanjÅ** (Dazaifu) | Though technically outside Fukuoka City, itâ€™s a 30â€‘minute train ride and the cityâ€™s spiritual heart. | 9â€¯amâ€“7â€¯pm, free (except for some temples). |\n",
    "| **Shopping & Nightlife** | **Tenjin** (central business district) | The hub for fashion, electronics and underground shopping. | 10â€¯amâ€“9â€¯pm (stores). |\n",
    "| | **Nakasu** (redâ€‘light district) | Best for yatai (street food stalls), izakayas, and nightlife. | Evening to midnight. |\n",
    "| | **Hakata Station** | Iconic underground shopping and a great place for ramen. | 7â€¯amâ€“9â€¯pm (shops). |\n",
    "| **Local Experience** | **Yatai Stalls** | Street food tents selling ramen, yakitori, gyoza, etc. | Late afternoon to midnight. Great for trying local cuisine. |\n",
    "| | **Matsusakaâ€‘tai (Japanese horse mackerel) at **Matsuzakaâ€‘Taiya** | Fresh seafood at a local market. | Earlyâ€‘morning. |\n",
    "| | **Fukuoka Asian Art Museum** | Focus on contemporary Asian art. | 10â€¯amâ€“5â€¯pm; free entry. |\n",
    "\n",
    "---\n",
    "\n",
    "### Suggested Miniâ€‘Itineraries\n",
    "\n",
    "| **Duration** | **Day 1** | **Day 2** |\n",
    "|--------------|-----------|-----------|\n",
    "| **1 Day** | 1. Morning: Ohori Park & Fukuoka Tower (take photos of the city from the top).  <br>2. Lunch at Yatai (try Hakata ramen).  <br>3. Afternoon: Tenjin shopping & Canal City.  <br>4. Evening: Nakasu yatai and a stroll along the canal. | 1. Quick visit to Hakata Shrine in the morning.  <br>2. Lunch at Hakata Station (classic Hakata ramen).  <br>3. Explore Kokuraâ€‘Jima (or Momochi Seaside).  <br>4. Evening: Walk around the Fukuoka City Zoo or dine in Tenjin. |\n",
    "| **2 Days** | **Day 1**: As above.  <br>**Day 2**: Take a short train (JR Kagoshima Line) to **Dazaifu**: <br>â€” Kawaraâ€‘jimaâ€™s Daidoâ€‘ji Temple, <br>â€” Dazaifu Tenmangu, <br>â€” E-kyu (Japanâ€™s first museum of art and design). Return to Fukuoka for a night of yatai and a quick visit to the Fukuoka Art Museum. |  |\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Quick Tips\n",
    "\n",
    "- **Transportation**  \n",
    "  - **Nipponâ€‘Rail Pass (JR Kyushu)** gives unlimited rides on JR lines, including Fukuoka Cityâ€™s rapid (Hakata line).  \n",
    "  - **Fukuoka City Bus or the Canal City â€œYuhikukanâ€ (inâ€‘mall metro)** can get you to many attractions on foot.  \n",
    "  - A **NFC phone** or a preâ€‘loaded *Fukuoka City Tourist Card* can get you free or discounted admission to many attractions.\n",
    "\n",
    "- **Money**  \n",
    "  - Accepts contactless cards everywhere; 10â€‘yen coins are handy for vending machines.  \n",
    "  - 1,500â€“2,500â€¯JPY per attraction usually covers entrance plus a photo / small souvenir.\n",
    "\n",
    "- **Language**  \n",
    "  - Most tourism signs are bilingual.  \n",
    "  - A little Japanese can go a long way, especially in yatai stalls.\n",
    "\n",
    "- **Best Time to Visit**  \n",
    "  - **Spring (Marchâ€‘May)** for cherry blossoms around Ohori Park.  \n",
    "  - **Autumn (Octâ€‘Nov)** for mild temperatures and autumn foliage.  \n",
    "  - Avoid **summer** (hot & humid) unless you love beach & nightlife.\n",
    "\n",
    "- **Dining**  \n",
    "  - **Hakata ramen** (squidâ€‘shell broth) is a signature.  \n",
    "  - **Motsunabe** (offal stew) is a local hearty choice; try it in Nakasu for an authentic vibe.  \n",
    "  - **Matsusakaâ€‘tai** for fresh mackerel sashimi.\n",
    "\n",
    "---\n",
    "\n",
    "Enjoy your time in Fukuoka! Itâ€™s a city that mixes ultraâ€‘modern vibes with deep tradition, all under one vibrant, accessible skyline. If you want more in-depth itinerariesâ€”like 3â€‘day plans, travel tips by budget, or unique niche experiencesâ€”just let me know!<|return|>\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
