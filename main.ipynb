{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9780cbdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸŸ© Transformers version: 4.57.1\n",
      "ðŸŸ© Numpy version: 2.1.2\n",
      "ðŸŸ© BitsAndBytes version: 0.48.1\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU transformers==4.57.1 kernels\n",
    "\n",
    "import os\n",
    "import logging as logging_\n",
    "import transformers\n",
    "import bitsandbytes\n",
    "from transformers import PretrainedConfig\n",
    "from transformers.utils import logging\n",
    "\n",
    "from typing import Callable, Optional, Union\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼\n",
    "\n",
    "import os\n",
    "from shutil import copyfile\n",
    "from typing import TYPE_CHECKING, Any, Optional\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«\n",
    "\n",
    "from typing import Callable, Optional, Union\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "assert torch.cuda.is_available(), \"CUDAã‚’ä½¿ç”¨ã§ãã¾ã›ã‚“\"\n",
    "\n",
    "from typing import Callable, Optional, Union\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from transformers.cache_utils import Cache, DynamicCache\n",
    "from transformers.generation import GenerationMixin\n",
    "from transformers.integrations.hub_kernels import use_kernel_forward_from_hub\n",
    "from transformers.masking_utils import create_causal_mask, create_sliding_window_causal_mask\n",
    "from transformers.modeling_layers import (\n",
    "    GenericForSequenceClassification,\n",
    "    GenericForTokenClassification,\n",
    "    GradientCheckpointingLayer,\n",
    ")\n",
    "from transformers.modeling_outputs import MoeCausalLMOutputWithPast, MoeModelOutputWithPast\n",
    "from transformers.modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n",
    "from transformers.modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n",
    "from transformers.processing_utils import Unpack\n",
    "from transformers.utils import TransformersKwargs, auto_docstring, can_return_tuple\n",
    "from transformers.utils.deprecation import deprecate_kwarg\n",
    "from transformers.utils.generic import OutputRecorder, check_model_inputs\n",
    "from transformers.models.gpt_oss.configuration_gpt_oss import GptOssConfig\n",
    "\n",
    "# ãƒ‡ãƒã‚¤ã‚¹è¨­å®š\n",
    "\n",
    "torch.set_default_device(\"cuda\")\n",
    "\n",
    "# ãƒ­ã‚°è¨­å®š\n",
    "\n",
    "if os.path.exists('debug.log'):\n",
    "    os.remove('debug.log')\n",
    "\n",
    "def custom_format(record):\n",
    "    match record.levelno:\n",
    "        case logging_.DEBUG:\n",
    "            level = 'ðŸŸ¦'\n",
    "        case logging_.INFO:\n",
    "            level = 'ðŸŸ©'\n",
    "        case logging_.WARNING:\n",
    "            level = 'ðŸŸ¨'\n",
    "        case logging_.ERROR:\n",
    "            level = 'ðŸŸ¥'\n",
    "        case logging_.CRITICAL:\n",
    "            level = 'ðŸ›‘'\n",
    "    return f\"{level} {record.getMessage()}\"\n",
    "\n",
    "logging.set_verbosity_debug()\n",
    "logger = logging.get_logger()\n",
    "\n",
    "for handler in logger.handlers:\n",
    "    logger.removeHandler(handler)\n",
    "\n",
    "formatter = logging_.Formatter()\n",
    "formatter.format = custom_format\n",
    "\n",
    "file_handler = logging_.FileHandler('debug.log')\n",
    "file_handler.setFormatter(formatter)\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "stream_handler = logging_.StreamHandler()\n",
    "stream_handler.setFormatter(formatter)\n",
    "logger.addHandler(stream_handler)\n",
    "\n",
    "logger.info(f\"Transformers version: {transformers.__version__}\")\n",
    "logger.info(f\"Numpy version: {np.__version__}\")\n",
    "logger.info(f\"BitsAndBytes version: {bitsandbytes.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81e8b37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@use_kernel_forward_from_hub(\"RMSNorm\")\n",
    "class GptOssRMSNorm(nn.Module):\n",
    "    def __init__(self, hidden_size, eps=1e-6):\n",
    "        \"\"\"\n",
    "        GptOssRMSNorm is equivalent to T5LayerNorm\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(hidden_size))\n",
    "        self.variance_epsilon = eps\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        input_dtype = hidden_states.dtype\n",
    "        hidden_states = hidden_states.to(torch.float32)\n",
    "        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
    "        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
    "        return (self.weight * hidden_states).to(input_dtype)  # main diff with Llama\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5399eb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GptOssExperts(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.intermediate_size = config.intermediate_size\n",
    "        self.num_experts = config.num_local_experts\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.expert_dim = self.intermediate_size\n",
    "        self.gate_up_proj = nn.Parameter(torch.empty(self.num_experts, self.hidden_size, 2 * self.expert_dim))\n",
    "        self.gate_up_proj_bias = nn.Parameter(torch.empty(self.num_experts, 2 * self.expert_dim))\n",
    "        self.down_proj = nn.Parameter(torch.empty((self.num_experts, self.expert_dim, self.hidden_size)))\n",
    "        self.down_proj_bias = nn.Parameter(torch.empty(self.num_experts, self.hidden_size))\n",
    "        self.alpha = 1.702\n",
    "        self.limit = 7.0\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor, router_indices=None, routing_weights=None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        When training it is more efficient to just loop over the experts and compute the output for each expert\n",
    "        as otherwise the memory would explode.\n",
    "\n",
    "        For inference we can sacrifice some memory and compute the output for all experts at once. By repeating the inputs.\n",
    "\n",
    "        Args:\n",
    "            hidden_states (torch.Tensor): (batch_size, seq_len, hidden_size)\n",
    "            selected_experts (torch.Tensor): (batch_size * token_num, top_k)\n",
    "            routing_weights (torch.Tensor): (batch_size * token_num, num_experts)\n",
    "        Returns:\n",
    "            torch.Tensor\n",
    "        \"\"\"\n",
    "        batch_size = hidden_states.shape[0]\n",
    "        hidden_states = hidden_states.reshape(-1, self.hidden_size)  # (num_tokens, hidden_size)\n",
    "        num_experts = routing_weights.shape[1]\n",
    "        if hidden_states.device.type == \"cpu\" or self.training:\n",
    "            next_states = torch.zeros_like(hidden_states, dtype=hidden_states.dtype, device=hidden_states.device)\n",
    "            with torch.no_grad():\n",
    "                expert_mask = torch.nn.functional.one_hot(\n",
    "                    router_indices, num_classes=num_experts + 1\n",
    "                )  # masking is also a class\n",
    "                expert_mask = expert_mask.permute(2, 1, 0)\n",
    "                # we sum on the top_k and on the sequence length to get which experts\n",
    "                # are hit this time around\n",
    "                expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n",
    "            for expert_idx in expert_hit[:]:\n",
    "                # expert_idx only have 1 element, so we can use scale for fast indexing\n",
    "                expert_idx = expert_idx[0]\n",
    "                # skip masking index\n",
    "                if expert_idx == num_experts:\n",
    "                    continue\n",
    "                with torch.no_grad():\n",
    "                    _, token_idx = torch.where(expert_mask[expert_idx])\n",
    "                current_state = hidden_states[token_idx]\n",
    "                gate_up = current_state @ self.gate_up_proj[expert_idx] + self.gate_up_proj_bias[expert_idx]\n",
    "                gate, up = gate_up[..., ::2], gate_up[..., 1::2]\n",
    "                gate = gate.clamp(min=None, max=self.limit)\n",
    "                up = up.clamp(min=-self.limit, max=self.limit)\n",
    "                glu = gate * torch.sigmoid(gate * self.alpha)\n",
    "                gated_output = (up + 1) * glu\n",
    "                out = gated_output @ self.down_proj[expert_idx] + self.down_proj_bias[expert_idx]\n",
    "                weighted_output = out * routing_weights[token_idx, expert_idx, None]\n",
    "                next_states.index_add_(0, token_idx, weighted_output.to(hidden_states.dtype))\n",
    "            next_states = next_states.view(batch_size, -1, self.hidden_size)\n",
    "        else:\n",
    "            hidden_states = hidden_states.repeat(num_experts, 1)\n",
    "            hidden_states = hidden_states.view(num_experts, -1, self.hidden_size)\n",
    "            gate_up = torch.bmm(hidden_states, self.gate_up_proj) + self.gate_up_proj_bias[..., None, :]\n",
    "            gate, up = gate_up[..., ::2], gate_up[..., 1::2]\n",
    "            gate = gate.clamp(min=None, max=self.limit)\n",
    "            up = up.clamp(min=-self.limit, max=self.limit)\n",
    "            glu = gate * torch.sigmoid(gate * self.alpha)\n",
    "            next_states = torch.bmm(((up + 1) * glu), self.down_proj)\n",
    "            next_states = next_states + self.down_proj_bias[..., None, :]\n",
    "            next_states = next_states.view(num_experts, batch_size, -1, self.hidden_size)\n",
    "            next_states = next_states * routing_weights.transpose(0, 1).view(num_experts, batch_size, -1)[..., None]\n",
    "            next_states = next_states.sum(dim=0)\n",
    "        return next_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b6b8a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GptOssTopKRouter(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.top_k = config.num_experts_per_tok\n",
    "        self.num_experts = config.num_local_experts\n",
    "        self.hidden_dim = config.hidden_size\n",
    "        self.weight = nn.Parameter(torch.empty(self.num_experts, self.hidden_dim))\n",
    "        self.bias = nn.Parameter(torch.empty(self.num_experts))\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = hidden_states.reshape(-1, self.hidden_dim)\n",
    "        router_logits = F.linear(hidden_states, self.weight, self.bias)  # (seq_len, num_experts)\n",
    "        router_top_value, router_indices = torch.topk(router_logits, self.top_k, dim=-1)  # (seq_len, top_k)\n",
    "        router_top_value = torch.nn.functional.softmax(router_top_value, dim=1, dtype=router_top_value.dtype)\n",
    "        router_scores = torch.zeros_like(router_logits).scatter_(1, router_indices, router_top_value)\n",
    "        return router_scores, router_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2bc91ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "@use_kernel_forward_from_hub(\"MegaBlocksMoeMLP\")\n",
    "class GptOssMLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.router = GptOssTopKRouter(config)\n",
    "        self.experts = GptOssExperts(config)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        router_scores, router_indices = self.router(hidden_states)  # (num_experts, seq_len)\n",
    "        routed_out = self.experts(hidden_states, router_indices=router_indices, routing_weights=router_scores)\n",
    "        return routed_out, router_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62bc22db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GptOssRotaryEmbedding(nn.Module):\n",
    "    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n",
    "\n",
    "    def __init__(self, config: GptOssConfig, device=None):\n",
    "        super().__init__()\n",
    "        # BC: \"rope_type\" was originally \"type\"\n",
    "        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n",
    "            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n",
    "        else:\n",
    "            self.rope_type = \"default\"\n",
    "        self.max_seq_len_cached = config.max_position_embeddings\n",
    "        self.original_max_seq_len = config.max_position_embeddings\n",
    "\n",
    "        self.config = config\n",
    "        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n",
    "\n",
    "        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n",
    "        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n",
    "        self.original_inv_freq = self.inv_freq\n",
    "\n",
    "    @torch.no_grad()\n",
    "    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n",
    "    def forward(self, x, position_ids):\n",
    "        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n",
    "        position_ids_expanded = position_ids[:, None, :].float()\n",
    "\n",
    "        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n",
    "        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n",
    "            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n",
    "            emb = freqs\n",
    "            cos = emb.cos() * self.attention_scaling\n",
    "            sin = emb.sin() * self.attention_scaling\n",
    "\n",
    "        return cos.to(x.dtype), sin.to(x.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5bdc1dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n",
    "    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n",
    "    \"\"\"\n",
    "    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n",
    "    if n_rep == 1:\n",
    "        return hidden_states\n",
    "    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n",
    "    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28c149ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _apply_rotary_emb(\n",
    "    x: torch.Tensor,\n",
    "    cos: torch.Tensor,\n",
    "    sin: torch.Tensor,\n",
    ") -> torch.Tensor:\n",
    "    first_half, second_half = torch.chunk(x, 2, dim=-1)\n",
    "    first_ = first_half * cos - second_half * sin\n",
    "    second_ = second_half * cos + first_half * sin\n",
    "    return torch.cat((first_, second_), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14294bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n",
    "    cos = cos.unsqueeze(unsqueeze_dim)\n",
    "    sin = sin.unsqueeze(unsqueeze_dim)\n",
    "    q_embed = _apply_rotary_emb(q, cos, sin)\n",
    "    k_embed = _apply_rotary_emb(k, cos, sin)\n",
    "    return q_embed, k_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e916936",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eager_attention_forward(\n",
    "    module: nn.Module,\n",
    "    query: torch.Tensor,\n",
    "    key: torch.Tensor,\n",
    "    value: torch.Tensor,\n",
    "    attention_mask: Optional[torch.Tensor],\n",
    "    scaling: float,\n",
    "    dropout: float = 0.0,\n",
    "    **kwargs,\n",
    "):\n",
    "    key_states = repeat_kv(key, module.num_key_value_groups)\n",
    "    value_states = repeat_kv(value, module.num_key_value_groups)\n",
    "    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling\n",
    "    if attention_mask is not None:\n",
    "        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n",
    "        attn_weights = attn_weights + causal_mask\n",
    "\n",
    "    sinks = module.sinks.reshape(1, -1, 1, 1).expand(query.shape[0], -1, query.shape[-2], -1)\n",
    "    combined_logits = torch.cat([attn_weights, sinks], dim=-1)\n",
    "\n",
    "    # This was not in the original implementation and slightly affect results; it prevents overflow in BF16/FP16\n",
    "    # when training with bsz>1 we clamp max values.\n",
    "\n",
    "    combined_logits = combined_logits - combined_logits.max(dim=-1, keepdim=True).values\n",
    "    probs = F.softmax(combined_logits, dim=-1, dtype=combined_logits.dtype)\n",
    "    scores = probs[..., :-1]  # we drop the sink here\n",
    "    attn_weights = nn.functional.dropout(scores, p=dropout, training=module.training)\n",
    "    attn_output = torch.matmul(attn_weights, value_states)\n",
    "    attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "    return attn_output, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "103b2cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GptOssAttention(nn.Module):\n",
    "    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n",
    "\n",
    "    def __init__(self, config: GptOssConfig, layer_idx: int):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.layer_idx = layer_idx\n",
    "        self.head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n",
    "        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads\n",
    "        self.scaling = self.head_dim**-0.5\n",
    "        self.attention_dropout = config.attention_dropout\n",
    "        self.is_causal = True\n",
    "        self.q_proj = nn.Linear(\n",
    "            config.hidden_size, config.num_attention_heads * self.head_dim, bias=config.attention_bias\n",
    "        )\n",
    "        self.k_proj = nn.Linear(\n",
    "            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n",
    "        )\n",
    "        self.v_proj = nn.Linear(\n",
    "            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n",
    "        )\n",
    "        self.o_proj = nn.Linear(\n",
    "            config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n",
    "        )\n",
    "        self.sliding_window = config.sliding_window if config.layer_types[layer_idx] == \"sliding_attention\" else None\n",
    "        self.sinks = nn.Parameter(torch.empty(config.num_attention_heads))\n",
    "\n",
    "    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n",
    "        attention_mask: Optional[torch.Tensor],\n",
    "        past_key_values: Optional[Cache] = None,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "        **kwargs: Unpack[TransformersKwargs],\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        input_shape = hidden_states.shape[:-1]\n",
    "        hidden_shape = (*input_shape, -1, self.head_dim)\n",
    "\n",
    "        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
    "        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
    "        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
    "\n",
    "        cos, sin = position_embeddings\n",
    "        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n",
    "\n",
    "        if past_key_values is not None:\n",
    "            cache_kwargs = {\"cache_position\": cache_position}\n",
    "            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n",
    "\n",
    "        attention_interface: Callable = eager_attention_forward\n",
    "        if self.config._attn_implementation != \"eager\":\n",
    "            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n",
    "\n",
    "        attn_output, attn_weights = attention_interface(\n",
    "            self,\n",
    "            query_states,\n",
    "            key_states,\n",
    "            value_states,\n",
    "            attention_mask,\n",
    "            dropout=0.0 if not self.training else self.attention_dropout,\n",
    "            scaling=self.scaling,\n",
    "            sliding_window=self.sliding_window,\n",
    "            s_aux=self.sinks,  # diff with Llama\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n",
    "        attn_output = self.o_proj(attn_output)\n",
    "        return attn_output, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd90c5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GptOssDecoderLayer(GradientCheckpointingLayer):\n",
    "    def __init__(self, config: GptOssConfig, layer_idx: int):\n",
    "        super().__init__()\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.self_attn = GptOssAttention(config=config, layer_idx=layer_idx)\n",
    "        self.mlp = GptOssMLP(config)\n",
    "        self.input_layernorm = GptOssRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "        self.post_attention_layernorm = GptOssRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "        self.attention_type = config.layer_types[layer_idx]\n",
    "\n",
    "    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[Cache] = None,\n",
    "        use_cache: Optional[bool] = False,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n",
    "        **kwargs: Unpack[TransformersKwargs],\n",
    "    ) -> torch.Tensor:\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.input_layernorm(hidden_states)\n",
    "        # Self Attention\n",
    "        hidden_states, _ = self.self_attn(\n",
    "            hidden_states=hidden_states,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            use_cache=use_cache,\n",
    "            cache_position=cache_position,\n",
    "            position_embeddings=position_embeddings,\n",
    "            **kwargs,\n",
    "        )\n",
    "        hidden_states = residual + hidden_states\n",
    "\n",
    "        # Fully Connected\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.post_attention_layernorm(hidden_states)\n",
    "        hidden_states, _ = self.mlp(hidden_states)  # diff with llama: router scores\n",
    "        hidden_states = residual + hidden_states\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d2813bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GptOssPreTrainedModel(PreTrainedModel):\n",
    "    config: GptOssConfig\n",
    "    base_model_prefix = \"model\"\n",
    "    supports_gradient_checkpointing = True\n",
    "    _no_split_modules = [\"GptOssDecoderLayer\"]\n",
    "    _skip_keys_device_placement = [\"past_key_values\"]\n",
    "    _supports_flash_attn = True\n",
    "    _supports_sdpa = False\n",
    "    _supports_flex_attn = True\n",
    "\n",
    "    _can_compile_fullgraph = True\n",
    "    _supports_attention_backend = True\n",
    "    _can_record_outputs = {\n",
    "        \"router_logits\": OutputRecorder(GptOssTopKRouter, index=0),\n",
    "        \"hidden_states\": GptOssDecoderLayer,\n",
    "        \"attentions\": GptOssAttention,\n",
    "    }\n",
    "    _keep_in_fp32_modules = [\"post_attention_layernorm\", \"input_layernorm\", \"norm\"]\n",
    "    _supports_flash_attention = False\n",
    "    _supports_flex_attention = False\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        std = self.config.initializer_range\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=std)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Parameter):\n",
    "            module.data.normal_(mean=0.0, std=std)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=std)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, GptOssRMSNorm):\n",
    "            module.weight.data.fill_(1.0)\n",
    "        elif isinstance(module, GptOssExperts):\n",
    "            module.gate_up_proj.data.normal_(mean=0.0, std=std)\n",
    "            module.gate_up_proj_bias.data.zero_()\n",
    "            module.down_proj.data.normal_(mean=0.0, std=std)\n",
    "            module.down_proj_bias.data.zero_()\n",
    "        elif isinstance(module, GptOssAttention):\n",
    "            module.sinks.data.normal_(mean=0.0, std=std)\n",
    "        elif isinstance(module, GptOssTopKRouter):\n",
    "            module.weight.data.normal_(mean=0.0, std=std)\n",
    "            module.bias.data.normal_(mean=0.0, std=std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f0d7305e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GptOssModel(GptOssPreTrainedModel):\n",
    "    _no_split_modules = [\"GptOssDecoderLayer\"]\n",
    "\n",
    "    def __init__(self, config: GptOssConfig):\n",
    "        super().__init__(config)\n",
    "        self.padding_idx = config.pad_token_id\n",
    "        self.vocab_size = config.vocab_size\n",
    "\n",
    "        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n",
    "        self.layers = nn.ModuleList(\n",
    "            [GptOssDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n",
    "        )\n",
    "        self.norm = GptOssRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "        self.rotary_emb = GptOssRotaryEmbedding(config=config)\n",
    "        self.gradient_checkpointing = False\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    @check_model_inputs\n",
    "    @auto_docstring\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.LongTensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[Cache] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "        **kwargs: Unpack[TransformersKwargs],\n",
    "    ) -> MoeModelOutputWithPast:\n",
    "        if (input_ids is None) ^ (inputs_embeds is not None):\n",
    "            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n",
    "\n",
    "        if use_cache and past_key_values is None:\n",
    "            past_key_values = DynamicCache(config=self.config)\n",
    "\n",
    "        if inputs_embeds is None:\n",
    "            inputs_embeds = self.embed_tokens(input_ids)\n",
    "\n",
    "        if cache_position is None:\n",
    "            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n",
    "            cache_position = torch.arange(\n",
    "                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n",
    "            )\n",
    "        if position_ids is None:\n",
    "            position_ids = cache_position.unsqueeze(0)\n",
    "\n",
    "        # It may already have been prepared by e.g. `generate`\n",
    "        if not isinstance(causal_mask_mapping := attention_mask, dict):\n",
    "            mask_kwargs = {\n",
    "                \"config\": self.config,\n",
    "                \"input_embeds\": inputs_embeds,\n",
    "                \"attention_mask\": attention_mask,\n",
    "                \"cache_position\": cache_position,\n",
    "                \"past_key_values\": past_key_values,\n",
    "            }\n",
    "            causal_mask_mapping = {\n",
    "                \"full_attention\": create_causal_mask(**mask_kwargs),\n",
    "                \"sliding_attention\": create_sliding_window_causal_mask(**mask_kwargs),\n",
    "            }\n",
    "\n",
    "        hidden_states = inputs_embeds\n",
    "        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n",
    "\n",
    "        for decoder_layer in self.layers:\n",
    "            hidden_states = decoder_layer(\n",
    "                hidden_states,\n",
    "                attention_mask=causal_mask_mapping[decoder_layer.attention_type],\n",
    "                position_ids=position_ids,\n",
    "                past_key_values=past_key_values,\n",
    "                use_cache=use_cache,\n",
    "                cache_position=cache_position,\n",
    "                position_embeddings=position_embeddings,\n",
    "                **kwargs,\n",
    "            )\n",
    "        hidden_states = self.norm(hidden_states)\n",
    "        return MoeModelOutputWithPast(\n",
    "            last_hidden_state=hidden_states,\n",
    "            past_key_values=past_key_values,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "adfcd06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_balancing_loss_func(\n",
    "    gate_logits: Union[torch.Tensor, tuple[torch.Tensor], None],\n",
    "    num_experts: Optional[int] = None,\n",
    "    top_k=2,\n",
    "    attention_mask: Optional[torch.Tensor] = None,\n",
    ") -> Union[torch.Tensor, int]:\n",
    "    r\"\"\"\n",
    "    Computes auxiliary load balancing loss as in Switch Transformer - implemented in Pytorch.\n",
    "\n",
    "    See Switch Transformer (https://huggingface.co/papers/2101.03961) for more details. This function implements the loss\n",
    "    function presented in equations (4) - (6) of the paper. It aims at penalizing cases where the routing between\n",
    "    experts is too unbalanced.\n",
    "\n",
    "    Args:\n",
    "        gate_logits:\n",
    "            Logits from the `gate`, should be a tuple of model.config.num_hidden_layers tensors of\n",
    "            shape [batch_size X sequence_length, num_experts].\n",
    "        num_experts:\n",
    "            Number of experts\n",
    "        top_k:\n",
    "            The number of experts to route per-token, can be also interpreted as the `top-k` routing\n",
    "            parameter.\n",
    "        attention_mask (`torch.Tensor`, *optional*):\n",
    "            The attention_mask used in forward function\n",
    "            shape [batch_size X sequence_length] if not None.\n",
    "\n",
    "    Returns:\n",
    "        The auxiliary loss.\n",
    "    \"\"\"\n",
    "    if gate_logits is None or not isinstance(gate_logits, tuple):\n",
    "        return 0\n",
    "\n",
    "    if isinstance(gate_logits, tuple):\n",
    "        compute_device = gate_logits[0].device\n",
    "        concatenated_gate_logits = torch.cat([layer_gate.to(compute_device) for layer_gate in gate_logits], dim=0)\n",
    "\n",
    "    routing_weights = torch.nn.functional.softmax(concatenated_gate_logits, dim=-1)\n",
    "\n",
    "    _, selected_experts = torch.topk(routing_weights, top_k, dim=-1)\n",
    "\n",
    "    expert_mask = torch.nn.functional.one_hot(selected_experts, num_experts)\n",
    "\n",
    "    if attention_mask is None:\n",
    "        # Compute the percentage of tokens routed to each experts\n",
    "        tokens_per_expert = torch.mean(expert_mask.float(), dim=0)\n",
    "\n",
    "        # Compute the average probability of routing to these experts\n",
    "        router_prob_per_expert = torch.mean(routing_weights, dim=0)\n",
    "    else:\n",
    "        batch_size, sequence_length = attention_mask.shape\n",
    "        num_hidden_layers = concatenated_gate_logits.shape[0] // (batch_size * sequence_length)\n",
    "\n",
    "        # Compute the mask that masks all padding tokens as 0 with the same shape of expert_mask\n",
    "        expert_attention_mask = (\n",
    "            attention_mask[None, :, :, None, None]\n",
    "            .expand((num_hidden_layers, batch_size, sequence_length, top_k, num_experts))\n",
    "            .reshape(-1, top_k, num_experts)\n",
    "            .to(compute_device)\n",
    "        )\n",
    "\n",
    "        # Compute the percentage of tokens routed to each experts\n",
    "        tokens_per_expert = torch.sum(expert_mask.float() * expert_attention_mask, dim=0) / torch.sum(\n",
    "            expert_attention_mask, dim=0\n",
    "        )\n",
    "\n",
    "        # Compute the mask that masks all padding tokens as 0 with the same shape of tokens_per_expert\n",
    "        router_per_expert_attention_mask = (\n",
    "            attention_mask[None, :, :, None]\n",
    "            .expand((num_hidden_layers, batch_size, sequence_length, num_experts))\n",
    "            .reshape(-1, num_experts)\n",
    "            .to(compute_device)\n",
    "        )\n",
    "\n",
    "        # Compute the average probability of routing to these experts\n",
    "        router_prob_per_expert = torch.sum(routing_weights * router_per_expert_attention_mask, dim=0) / torch.sum(\n",
    "            router_per_expert_attention_mask, dim=0\n",
    "        )\n",
    "\n",
    "    overall_loss = torch.sum(tokens_per_expert * router_prob_per_expert.unsqueeze(0))\n",
    "    return overall_loss * num_experts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bf01d5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GptOssForCausalLM(GptOssPreTrainedModel, GenerationMixin):\n",
    "    _tied_weights_keys = [\"lm_head.weight\"]\n",
    "    _tp_plan = {\"lm_head\": \"colwise_rep\"}\n",
    "    _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.model = GptOssModel(config)\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "        self.router_aux_loss_coef = config.router_aux_loss_coef\n",
    "        self.num_experts = config.num_local_experts\n",
    "        self.num_experts_per_tok = config.num_experts_per_tok\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    @can_return_tuple\n",
    "    @auto_docstring\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.LongTensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[Cache] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_router_logits: Optional[bool] = None,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "        logits_to_keep: Union[int, torch.Tensor] = 0,\n",
    "        **kwargs: Unpack[TransformersKwargs],\n",
    "    ) -> MoeCausalLMOutputWithPast:\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n",
    "            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n",
    "            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n",
    "\n",
    "        Example:\n",
    "\n",
    "        ```python\n",
    "        >>> from transformers import AutoTokenizer, GptOssForCausalLM\n",
    "\n",
    "        >>> model = GptOssForCausalLM.from_pretrained(\"mistralai/GptOss-8x7B-v0.1\")\n",
    "        >>> tokenizer = AutoTokenizer.from_pretrained(\"mistralai/GptOss-8x7B-v0.1\")\n",
    "\n",
    "        >>> prompt = \"Hey, are you conscious? Can you talk to me?\"\n",
    "        >>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "        >>> # Generate\n",
    "        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n",
    "        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "        \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\n",
    "        ```\"\"\"\n",
    "\n",
    "        output_router_logits = (\n",
    "            output_router_logits if output_router_logits is not None else self.config.output_router_logits\n",
    "        )\n",
    "\n",
    "        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n",
    "        outputs: MoeModelOutputWithPast = self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            use_cache=use_cache,\n",
    "            output_router_logits=output_router_logits,\n",
    "            cache_position=cache_position,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n",
    "        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n",
    "        logits = self.lm_head(hidden_states[:, slice_indices, :])\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = self.loss_function(logits, labels, self.vocab_size, **kwargs)\n",
    "\n",
    "        aux_loss = None\n",
    "        if output_router_logits:\n",
    "            aux_loss = load_balancing_loss_func(\n",
    "                outputs.router_logits,\n",
    "                self.num_experts,\n",
    "                self.num_experts_per_tok,\n",
    "                attention_mask,\n",
    "            )\n",
    "            if labels is not None:\n",
    "                loss += self.router_aux_loss_coef * aux_loss.to(loss.device)  # make sure to reside in the same device\n",
    "\n",
    "        return MoeCausalLMOutputWithPast(\n",
    "            loss=loss,\n",
    "            aux_loss=aux_loss,\n",
    "            logits=logits,\n",
    "            past_key_values=outputs.past_key_values,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "            router_logits=outputs.router_logits,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d03e5c25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸŸ© loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--openai--gpt-oss-20b/snapshots/6cee5e81ee83917806bbde320786a8fb61efebee/tokenizer.json\n",
      "ðŸŸ© loading file tokenizer.model from cache at None\n",
      "ðŸŸ© loading file added_tokens.json from cache at None\n",
      "ðŸŸ© loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--openai--gpt-oss-20b/snapshots/6cee5e81ee83917806bbde320786a8fb61efebee/special_tokens_map.json\n",
      "ðŸŸ© loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--openai--gpt-oss-20b/snapshots/6cee5e81ee83917806bbde320786a8fb61efebee/tokenizer_config.json\n",
      "ðŸŸ© loading file chat_template.jinja from cache at /root/.cache/huggingface/hub/models--openai--gpt-oss-20b/snapshots/6cee5e81ee83917806bbde320786a8fb61efebee/chat_template.jinja\n",
      "ðŸŸ© Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='openai/gpt-oss-20b', vocab_size=199998, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|startoftext|>', 'eos_token': '<|return|>', 'pad_token': '<|endoftext|>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t199998: AddedToken(\"<|startoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t199999: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t200000: AddedToken(\"<|reserved_200000|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t200001: AddedToken(\"<|reserved_200001|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t200002: AddedToken(\"<|return|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t200003: AddedToken(\"<|constrain|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t200004: AddedToken(\"<|reserved_200004|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t200005: AddedToken(\"<|channel|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t200006: AddedToken(\"<|start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t200007: AddedToken(\"<|end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t200008: AddedToken(\"<|message|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t200009: AddedToken(\"<|reserved_200009|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t200010: AddedToken(\"<|reserved_200010|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t200011: AddedToken(\"<|reserved_200011|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t200012: AddedToken(\"<|call|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t200013: AddedToken(\"<|reserved_200013|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t200014: AddedToken(\"<|reserved_200014|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t200015: AddedToken(\"<|reserved_200015|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t200016: AddedToken(\"<|reserved_200016|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t200017: AddedToken(\"<|reserved_200017|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t200018: AddedToken(\"<|endofprompt|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model_id = \"openai/gpt-oss-20b\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876725b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸŸ© loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--openai--gpt-oss-20b/snapshots/6cee5e81ee83917806bbde320786a8fb61efebee/config.json\n",
      "ðŸŸ© Model config GptOssConfig {\n",
      "  \"architectures\": [\n",
      "    \"GptOssForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": true,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"eos_token_id\": 200002,\n",
      "  \"experts_per_token\": 4,\n",
      "  \"head_dim\": 64,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2880,\n",
      "  \"initial_context_length\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 2880,\n",
      "  \"layer_types\": [\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\",\n",
      "    \"sliding_attention\",\n",
      "    \"full_attention\"\n",
      "  ],\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"model_type\": \"gpt_oss\",\n",
      "  \"num_attention_heads\": 64,\n",
      "  \"num_experts_per_tok\": 4,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"num_local_experts\": 32,\n",
      "  \"output_router_logits\": false,\n",
      "  \"pad_token_id\": 199999,\n",
      "  \"quantization_config\": {\n",
      "    \"modules_to_not_convert\": [\n",
      "      \"model.layers.*.self_attn\",\n",
      "      \"model.layers.*.mlp.router\",\n",
      "      \"model.embed_tokens\",\n",
      "      \"lm_head\"\n",
      "    ],\n",
      "    \"quant_method\": \"mxfp4\"\n",
      "  },\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"beta_fast\": 32.0,\n",
      "    \"beta_slow\": 1.0,\n",
      "    \"factor\": 32.0,\n",
      "    \"original_max_position_embeddings\": 4096,\n",
      "    \"rope_type\": \"yarn\",\n",
      "    \"truncate\": false\n",
      "  },\n",
      "  \"rope_theta\": 150000,\n",
      "  \"router_aux_loss_coef\": 0.9,\n",
      "  \"sliding_window\": 128,\n",
      "  \"swiglu_limit\": 7.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.57.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 201088\n",
      "}\n",
      "\n",
      "ðŸŸ© \n",
      "ðŸŸ© Overriding dtype=torch.bfloat16 with `dtype=torch.bfloat16` due to requirements of `fbgemm-gpu` to enable model loading in fp4. Pass your own dtype to specify the dtype of the remaining non-linear layers or pass dtype=torch.bfloat16 to remove this warning.\n",
      "ðŸŸ© loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--openai--gpt-oss-20b/snapshots/6cee5e81ee83917806bbde320786a8fb61efebee/model.safetensors.index.json\n",
      "ðŸŸ© Instantiating GptOssForCausalLM model under default dtype torch.bfloat16.\n",
      "ðŸŸ© Generate config GenerationConfig {\n",
      "  \"eos_token_id\": 200002,\n",
      "  \"pad_token_id\": 199999\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "543eea659b034ae0913b0e608f54f6ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 41 files:   0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb37b9a26b464220ac3004cfb4f00245",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52c3adbc97d34f268ff8c5183fa7a9d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 41 files:   0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸŸ© loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--openai--gpt-oss-20b/snapshots/6cee5e81ee83917806bbde320786a8fb61efebee/generation_config.json\n",
      "ðŸŸ© Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 199998,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    200002,\n",
      "    199999,\n",
      "    200012\n",
      "  ],\n",
      "  \"pad_token_id\": 199999\n",
      "}\n",
      "\n",
      "ðŸŸ© Could not locate the custom_generate/generate.py inside openai/gpt-oss-20b.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GptOssForCausalLM(\n",
       "  (model): GptOssModel(\n",
       "    (embed_tokens): Embedding(201088, 2880, padding_idx=199999)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x GptOssDecoderLayer(\n",
       "        (self_attn): GptOssAttention(\n",
       "          (q_proj): Linear(in_features=2880, out_features=4096, bias=True)\n",
       "          (k_proj): Linear(in_features=2880, out_features=512, bias=True)\n",
       "          (v_proj): Linear(in_features=2880, out_features=512, bias=True)\n",
       "          (o_proj): Linear(in_features=4096, out_features=2880, bias=True)\n",
       "        )\n",
       "        (mlp): GptOssMLP(\n",
       "          (router): GptOssTopKRouter()\n",
       "          (experts): Mxfp4GptOssExperts()\n",
       "        )\n",
       "        (input_layernorm): GptOssRMSNorm((2880,), eps=1e-05)\n",
       "        (post_attention_layernorm): GptOssRMSNorm((2880,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): GptOssRMSNorm((2880,), eps=1e-05)\n",
       "    (rotary_emb): GptOssRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2880, out_features=201088, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GptOssForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"cuda\",\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6c0c964e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸŸ¨ Device set to use cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'assistant', 'content': 'analysisWe need to explain quantum mechanics clearly and concisely. Provide overview, key concepts, principles, and some examples. Avoid excessive jargon. Should be accessible. Likely need to mention wave-particle duality, superposition, uncertainty principle, quantization, measurement, wavefunction, SchrÃ¶dinger equation, entanglement, probabilistic nature. Need to keep concise. So maybe a structured explanation with bullet points or brief paragraphs. Provide analogies like double-slit, electron in potential well. Ensure clarity. Provide key equations like SchrÃ¶dinger. Also mention historical development. Keep it concise.assistantfinal**Quantum mechanics in a nutshell**\\n\\n| What | Why it matters | Key idea |\\n|------|----------------|----------|\\n| **Waveâ€‘particle duality** | Light and matter behave like waves *and* particles. | Light shows interference (waves); photons are detected as individual packets (particles). |\\n| **Wavefunction (Î¨)** | Encodes *all* information about a system. | A mathematical function that gives the probability of finding a particle in each position. |\\n| **Superposition** | A particle can be in several states at once until measured. | A cat is both alive *and* dead in the famous'}\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Explain quantum mechanics clearly and concisely.\"},\n",
    "]\n",
    "\n",
    "outputs = pipe(\n",
    "    messages,\n",
    "    max_new_tokens=256,\n",
    ")\n",
    "print(outputs[0][\"generated_text\"][-1])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
