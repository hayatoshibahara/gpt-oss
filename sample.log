ğŸŸ© Transformers version: 4.57.1
ğŸŸ© Numpy version: 2.1.2
ğŸŸ© loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--openai--gpt-oss-20b/snapshots/6cee5e81ee83917806bbde320786a8fb61efebee/tokenizer.json
ğŸŸ© loading file tokenizer.model from cache at None
ğŸŸ© loading file added_tokens.json from cache at None
ğŸŸ© loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--openai--gpt-oss-20b/snapshots/6cee5e81ee83917806bbde320786a8fb61efebee/special_tokens_map.json
ğŸŸ© loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--openai--gpt-oss-20b/snapshots/6cee5e81ee83917806bbde320786a8fb61efebee/tokenizer_config.json
ğŸŸ© loading file chat_template.jinja from cache at /root/.cache/huggingface/hub/models--openai--gpt-oss-20b/snapshots/6cee5e81ee83917806bbde320786a8fb61efebee/chat_template.jinja
ğŸŸ© Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
ğŸŸ© loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--openai--gpt-oss-20b/snapshots/6cee5e81ee83917806bbde320786a8fb61efebee/config.json
ğŸŸ© Model config GptOssConfig {
  "architectures": [
    "GptOssForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "eos_token_id": 200002,
  "experts_per_token": 4,
  "head_dim": 64,
  "hidden_act": "silu",
  "hidden_size": 2880,
  "initial_context_length": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 2880,
  "layer_types": [
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention",
    "sliding_attention",
    "full_attention"
  ],
  "max_position_embeddings": 131072,
  "model_type": "gpt_oss",
  "num_attention_heads": 64,
  "num_experts_per_tok": 4,
  "num_hidden_layers": 24,
  "num_key_value_heads": 8,
  "num_local_experts": 32,
  "output_router_logits": false,
  "pad_token_id": 199999,
  "quantization_config": {
    "modules_to_not_convert": [
      "model.layers.*.self_attn",
      "model.layers.*.mlp.router",
      "model.embed_tokens",
      "lm_head"
    ],
    "quant_method": "mxfp4"
  },
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "beta_fast": 32.0,
    "beta_slow": 1.0,
    "factor": 32.0,
    "original_max_position_embeddings": 4096,
    "rope_type": "yarn",
    "truncate": false
  },
  "rope_theta": 150000,
  "router_aux_loss_coef": 0.9,
  "sliding_window": 128,
  "swiglu_limit": 7.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.1",
  "use_cache": true,
  "vocab_size": 201088
}

ğŸŸ© 
ğŸŸ© Overriding dtype=torch.bfloat16 with `dtype=torch.bfloat16` due to requirements of `fbgemm-gpu` to enable model loading in fp4. Pass your own dtype to specify the dtype of the remaining non-linear layers or pass dtype=torch.bfloat16 to remove this warning.
ğŸŸ© loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--openai--gpt-oss-20b/snapshots/6cee5e81ee83917806bbde320786a8fb61efebee/model.safetensors.index.json
ğŸŸ© Instantiating GptOssForCausalLM model under default dtype torch.bfloat16.
ğŸŸ© GptOssForCausalLMã‚’åˆæœŸåŒ–é–‹å§‹ config.vocab_size=201088, config.hidden_size=2880, config.num_hidden_layers=24, config.pad_token_id=199999, config.router_aux_loss_coef=0.9, config.num_local_experts=32, config.num_experts_per_tok=4
ğŸŸ© Generate config GenerationConfig {
  "eos_token_id": 200002,
  "pad_token_id": 199999
}

ğŸŸ© GptOssModelã‚’åˆæœŸåŒ–é–‹å§‹ config.vocab_size=201088, config.hidden_size=2880, config.num_hidden_layers=24, config.pad_token_id=199999
ğŸŸ© GptOssDecoderLayerã‚’åˆæœŸåŒ–é–‹å§‹ config.hidden_size=2880, config.intermediate_size=2880, config.num_attention_heads=64, config.num_key_value_heads=8, config.attention_bias=True, config.attention_dropout=0.0, layer_idx=0
ğŸŸ© GptOssAttentionã‚’åˆæœŸåŒ–é–‹å§‹ config.hidden_size=2880, config.num_attention_heads=64, config.num_key_value_heads=8, config.attention_bias=True, config.attention_dropout=0.0, layer_idx=0
ğŸŸ¦ self.head_dim=64
ğŸŸ¦ self.num_key_value_groups=8
ğŸŸ¦ self.scaling=0.125
ğŸŸ¦ self.attention_dropout=0.0
ğŸŸ¦ self.sliding_window=128
ğŸŸ© GptOssAttentionã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© GptOssMLPã‚’åˆæœŸåŒ–é–‹å§‹
ğŸŸ© GptOssTopKRouterã‚’åˆæœŸåŒ–é–‹å§‹ config.num_experts_per_tok=4, config.num_local_experts=32, config.hidden_size=2880
ğŸŸ© GptOssTopKRouterã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© GptOssExpertsã‚’åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=2880, config.num_local_experts=32, config.hidden_size=2880
ğŸŸ© GptOssExpertsã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© GptOssMLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size=2880, eps=1e-05
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size=2880, eps=1e-05
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ¦ self.attention_type='sliding_attention'
ğŸŸ© GptOssDecoderLayerã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© GptOssDecoderLayerã‚’åˆæœŸåŒ–é–‹å§‹ config.hidden_size=2880, config.intermediate_size=2880, config.num_attention_heads=64, config.num_key_value_heads=8, config.attention_bias=True, config.attention_dropout=0.0, layer_idx=1
ğŸŸ© GptOssAttentionã‚’åˆæœŸåŒ–é–‹å§‹ config.hidden_size=2880, config.num_attention_heads=64, config.num_key_value_heads=8, config.attention_bias=True, config.attention_dropout=0.0, layer_idx=1
ğŸŸ¦ self.head_dim=64
ğŸŸ¦ self.num_key_value_groups=8
ğŸŸ¦ self.scaling=0.125
ğŸŸ¦ self.attention_dropout=0.0
ğŸŸ¦ self.sliding_window=None
ğŸŸ© GptOssAttentionã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© GptOssMLPã‚’åˆæœŸåŒ–é–‹å§‹
ğŸŸ© GptOssTopKRouterã‚’åˆæœŸåŒ–é–‹å§‹ config.num_experts_per_tok=4, config.num_local_experts=32, config.hidden_size=2880
ğŸŸ© GptOssTopKRouterã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© GptOssExpertsã‚’åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=2880, config.num_local_experts=32, config.hidden_size=2880
ğŸŸ© GptOssExpertsã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© GptOssMLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size=2880, eps=1e-05
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size=2880, eps=1e-05
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ¦ self.attention_type='full_attention'
ğŸŸ© GptOssDecoderLayerã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© GptOssDecoderLayerã‚’åˆæœŸåŒ–é–‹å§‹ config.hidden_size=2880, config.intermediate_size=2880, config.num_attention_heads=64, config.num_key_value_heads=8, config.attention_bias=True, config.attention_dropout=0.0, layer_idx=2
ğŸŸ© GptOssAttentionã‚’åˆæœŸåŒ–é–‹å§‹ config.hidden_size=2880, config.num_attention_heads=64, config.num_key_value_heads=8, config.attention_bias=True, config.attention_dropout=0.0, layer_idx=2
ğŸŸ¦ self.head_dim=64
ğŸŸ¦ self.num_key_value_groups=8
ğŸŸ¦ self.scaling=0.125
ğŸŸ¦ self.attention_dropout=0.0
ğŸŸ¦ self.sliding_window=128
ğŸŸ© GptOssAttentionã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© GptOssMLPã‚’åˆæœŸåŒ–é–‹å§‹
ğŸŸ© GptOssTopKRouterã‚’åˆæœŸåŒ–é–‹å§‹ config.num_experts_per_tok=4, config.num_local_experts=32, config.hidden_size=2880
ğŸŸ© GptOssTopKRouterã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© GptOssExpertsã‚’åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=2880, config.num_local_experts=32, config.hidden_size=2880
ğŸŸ© GptOssExpertsã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© GptOssMLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size=2880, eps=1e-05
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size=2880, eps=1e-05
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ¦ self.attention_type='sliding_attention'
ğŸŸ© GptOssDecoderLayerã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© GptOssDecoderLayerã‚’åˆæœŸåŒ–é–‹å§‹ config.hidden_size=2880, config.intermediate_size=2880, config.num_attention_heads=64, config.num_key_value_heads=8, config.attention_bias=True, config.attention_dropout=0.0, layer_idx=3
ğŸŸ© GptOssAttentionã‚’åˆæœŸåŒ–é–‹å§‹ config.hidden_size=2880, config.num_attention_heads=64, config.num_key_value_heads=8, config.attention_bias=True, config.attention_dropout=0.0, layer_idx=3
ğŸŸ¦ self.head_dim=64
ğŸŸ¦ self.num_key_value_groups=8
ğŸŸ¦ self.scaling=0.125
ğŸŸ¦ self.attention_dropout=0.0
ğŸŸ¦ self.sliding_window=None
ğŸŸ© GptOssAttentionã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© GptOssMLPã‚’åˆæœŸåŒ–é–‹å§‹
ğŸŸ© GptOssTopKRouterã‚’åˆæœŸåŒ–é–‹å§‹ config.num_experts_per_tok=4, config.num_local_experts=32, config.hidden_size=2880
ğŸŸ© GptOssTopKRouterã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© GptOssExpertsã‚’åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=2880, config.num_local_experts=32, config.hidden_size=2880
ğŸŸ© GptOssExpertsã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© GptOssMLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size=2880, eps=1e-05
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size=2880, eps=1e-05
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ¦ self.attention_type='full_attention'
ğŸŸ© GptOssDecoderLayerã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© GptOssDecoderLayerã‚’åˆæœŸåŒ–é–‹å§‹ config.hidden_size=2880, config.intermediate_size=2880, config.num_attention_heads=64, config.num_key_value_heads=8, config.attention_bias=True, config.attention_dropout=0.0, layer_idx=4
ğŸŸ© GptOssAttentionã‚’åˆæœŸåŒ–é–‹å§‹ config.hidden_size=2880, config.num_attention_heads=64, config.num_key_value_heads=8, config.attention_bias=True, config.attention_dropout=0.0, layer_idx=4
ğŸŸ¦ self.head_dim=64
ğŸŸ¦ self.num_key_value_groups=8
ğŸŸ¦ self.scaling=0.125
ğŸŸ¦ self.attention_dropout=0.0
ğŸŸ¦ self.sliding_window=128
ğŸŸ© GptOssAttentionã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© GptOssMLPã‚’åˆæœŸåŒ–é–‹å§‹
ğŸŸ© GptOssTopKRouterã‚’åˆæœŸåŒ–é–‹å§‹ config.num_experts_per_tok=4, config.num_local_experts=32, config.hidden_size=2880
ğŸŸ© GptOssTopKRouterã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© GptOssExpertsã‚’åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=2880, config.num_local_experts=32, config.hidden_size=2880
ğŸŸ© GptOssExpertsã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© GptOssMLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size=2880, eps=1e-05
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size=2880, eps=1e-05
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ¦ self.attention_type='sliding_attention'
ğŸŸ© GptOssDecoderLayerã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© GptOssDecoderLayerã‚’åˆæœŸåŒ–é–‹å§‹ config.hidden_size=2880, config.intermediate_size=2880, config.num_attention_heads=64, config.num_key_value_heads=8, config.attention_bias=True, config.attention_dropout=0.0, layer_idx=5
ğŸŸ© GptOssAttentionã‚’åˆæœŸåŒ–é–‹å§‹ config.hidden_size=2880, config.num_attention_heads=64, config.num_key_value_heads=8, config.attention_bias=True, config.attention_dropout=0.0, layer_idx=5
ğŸŸ¦ self.head_dim=64
ğŸŸ¦ self.num_key_value_groups=8
ğŸŸ¦ self.scaling=0.125
ğŸŸ¦ self.attention_dropout=0.0
ğŸŸ¦ self.sliding_window=None
ğŸŸ© GptOssAttentionã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© GptOssMLPã‚’åˆæœŸåŒ–é–‹å§‹
ğŸŸ© GptOssTopKRouterã‚’åˆæœŸåŒ–é–‹å§‹ config.num_experts_per_tok=4, config.num_local_experts=32, config.hidden_size=2880
ğŸŸ© GptOssTopKRouterã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© GptOssExpertsã‚’åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=2880, config.num_local_experts=32, config.hidden_size=2880
ğŸŸ© GptOssExpertsã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© GptOssMLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size=2880, eps=1e-05
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size=2880, eps=1e-05
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ¦ self.attention_type='full_attention'
ğŸŸ© GptOssDecoderLayerã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© GptOssDecoderLayerã‚’åˆæœŸåŒ–é–‹å§‹ config.hidden_size=2880, config.intermediate_size=2880, config.num_attention_heads=64, config.num_key_value_heads=8, config.attention_bias=True, config.attention_dropout=0.0, layer_idx=6
ğŸŸ© GptOssAttentionã‚’åˆæœŸåŒ–é–‹å§‹ config.hidden_size=2880, config.num_attention_heads=64, config.num_key_value_heads=8, config.attention_bias=True, config.attention_dropout=0.0, layer_idx=6
ğŸŸ¦ self.head_dim=64
ğŸŸ¦ self.num_key_value_groups=8
ğŸŸ¦ self.scaling=0.125
ğŸŸ¦ self.attention_dropout=0.0
ğŸŸ¦ self.sliding_window=128
ğŸŸ© GptOssAttentionã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© GptOssMLPã‚’åˆæœŸåŒ–é–‹å§‹
ğŸŸ© GptOssTopKRouterã‚’åˆæœŸåŒ–é–‹å§‹ config.num_experts_per_tok=4, config.num_local_experts=32, config.hidden_size=2880
ğŸŸ© GptOssTopKRouterã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© GptOssExpertsã‚’åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=2880, config.num_local_experts=32, config.hidden_size=2880
ğŸŸ© GptOssExpertsã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© GptOssMLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size=2880, eps=1e-05
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size=2880, eps=1e-05
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ¦ self.attention_type='sliding_attention'
ğŸŸ© GptOssDecoderLayerã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© GptOssDecoderLayerã‚’åˆæœŸåŒ–é–‹å§‹ config.hidden_size=2880, config.intermediate_size=2880, config.num_attention_heads=64, config.num_key_value_heads=8, config.attention_bias=True, config.attention_dropout=0.0, layer_idx=7
ğŸŸ© GptOssAttentionã‚’åˆæœŸåŒ–é–‹å§‹ config.hidden_size=2880, config.num_attention_heads=64, config.num_key_value_heads=8, config.attention_bias=True, config.attention_dropout=0.0, layer_idx=7
ğŸŸ¦ self.head_dim=64
ğŸŸ¦ self.num_key_value_groups=8
ğŸŸ¦ self.scaling=0.125
ğŸŸ¦ self.attention_dropout=0.0
ğŸŸ¦ self.sliding_window=None
ğŸŸ© GptOssAttentionã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© GptOssMLPã‚’åˆæœŸåŒ–é–‹å§‹
ğŸŸ© GptOssTopKRouterã‚’åˆæœŸåŒ–é–‹å§‹ config.num_experts_per_tok=4, config.num_local_experts=32, config.hidden_size=2880
ğŸŸ© GptOssTopKRouterã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© GptOssExpertsã‚’åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=2880, config.num_local_experts=32, config.hidden_size=2880
ğŸŸ© GptOssExpertsã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© GptOssMLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size=2880, eps=1e-05
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size=2880, eps=1e-05
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ¦ self.attention_type='full_attention'
ğŸŸ© GptOssDecoderLayerã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© GptOssDecoderLayerã‚’åˆæœŸåŒ–é–‹å§‹ config.hidden_size=2880, config.intermediate_size=2880, config.num_attention_heads=64, config.num_key_value_heads=8, config.attention_bias=True, config.attention_dropout=0.0, layer_idx=8
ğŸŸ© GptOssAttentionã‚’åˆæœŸåŒ–é–‹å§‹ config.hidden_size=2880, config.num_attention_heads=64, config.num_key_value_heads=8, config.attention_bias=True, config.attention_dropout=0.0, layer_idx=8
ğŸŸ¦ self.head_dim=64
ğŸŸ¦ self.num_key_value_groups=8
ğŸŸ¦ self.scaling=0.125
ğŸŸ¦ self.attention_dropout=0.0
ğŸŸ¦ self.sliding_window=128
ğŸŸ© GptOssAttentionã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© GptOssMLPã‚’åˆæœŸåŒ–é–‹å§‹
ğŸŸ© GptOssTopKRouterã‚’åˆæœŸåŒ–é–‹å§‹ config.num_experts_per_tok=4, config.num_local_experts=32, config.hidden_size=2880
ğŸŸ© GptOssTopKRouterã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© GptOssExpertsã‚’åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=2880, config.num_local_experts=32, config.hidden_size=2880
ğŸŸ© GptOssExpertsã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© GptOssMLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size=2880, eps=1e-05
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size=2880, eps=1e-05
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ¦ self.attention_type='sliding_attention'
ğŸŸ© GptOssDecoderLayerã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© GptOssDecoderLayerã‚’åˆæœŸåŒ–é–‹å§‹ config.hidden_size=2880, config.intermediate_size=2880, config.num_attention_heads=64, config.num_key_value_heads=8, config.attention_bias=True, config.attention_dropout=0.0, layer_idx=9
ğŸŸ© GptOssAttentionã‚’åˆæœŸåŒ–é–‹å§‹ config.hidden_size=2880, config.num_attention_heads=64, config.num_key_value_heads=8, config.attention_bias=True, config.attention_dropout=0.0, layer_idx=9
ğŸŸ¦ self.head_dim=64
ğŸŸ¦ self.num_key_value_groups=8
ğŸŸ¦ self.scaling=0.125
ğŸŸ¦ self.attention_dropout=0.0
ğŸŸ¦ self.sliding_window=None
ğŸŸ© GptOssAttentionã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© GptOssMLPã‚’åˆæœŸåŒ–é–‹å§‹
ğŸŸ© GptOssTopKRouterã‚’åˆæœŸåŒ–é–‹å§‹ config.num_experts_per_tok=4, config.num_local_experts=32, config.hidden_size=2880
ğŸŸ© GptOssTopKRouterã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© GptOssExpertsã‚’åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=2880, config.num_local_experts=32, config.hidden_size=2880
ğŸŸ© GptOssExpertsã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© GptOssMLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size=2880, eps=1e-05
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size=2880, eps=1e-05
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ¦ self.attention_type='full_attention'
ğŸŸ© GptOssDecoderLayerã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© GptOssDecoderLayerã‚’åˆæœŸåŒ–é–‹å§‹ config.hidden_size=2880, config.intermediate_size=2880, config.num_attention_heads=64, config.num_key_value_heads=8, config.attention_bias=True, config.attention_dropout=0.0, layer_idx=10
ğŸŸ© GptOssAttentionã‚’åˆæœŸåŒ–é–‹å§‹ config.hidden_size=2880, config.num_attention_heads=64, config.num_key_value_heads=8, config.attention_bias=True, config.attention_dropout=0.0, layer_idx=10
ğŸŸ¦ self.head_dim=64
ğŸŸ¦ self.num_key_value_groups=8
ğŸŸ¦ self.scaling=0.125
ğŸŸ¦ self.attention_dropout=0.0
ğŸŸ¦ self.sliding_window=128
ğŸŸ© GptOssAttentionã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© GptOssMLPã‚’åˆæœŸåŒ–é–‹å§‹
ğŸŸ© GptOssTopKRouterã‚’åˆæœŸåŒ–é–‹å§‹ config.num_experts_per_tok=4, config.num_local_experts=32, config.hidden_size=2880
ğŸŸ© GptOssTopKRouterã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© GptOssExpertsã‚’åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=2880, config.num_local_experts=32, config.hidden_size=2880
ğŸŸ© GptOssExpertsã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© GptOssMLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size=2880, eps=1e-05
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size=2880, eps=1e-05
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ¦ self.attention_type='sliding_attention'
ğŸŸ© GptOssDecoderLayerã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© GptOssDecoderLayerã‚’åˆæœŸåŒ–é–‹å§‹ config.hidden_size=2880, config.intermediate_size=2880, config.num_attention_heads=64, config.num_key_value_heads=8, config.attention_bias=True, config.attention_dropout=0.0, layer_idx=11
ğŸŸ© GptOssAttentionã‚’åˆæœŸåŒ–é–‹å§‹ config.hidden_size=2880, config.num_attention_heads=64, config.num_key_value_heads=8, config.attention_bias=True, config.attention_dropout=0.0, layer_idx=11
ğŸŸ¦ self.head_dim=64
ğŸŸ¦ self.num_key_value_groups=8
ğŸŸ¦ self.scaling=0.125
ğŸŸ¦ self.attention_dropout=0.0
ğŸŸ¦ self.sliding_window=None
ğŸŸ© GptOssAttentionã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© GptOssMLPã‚’åˆæœŸåŒ–é–‹å§‹
ğŸŸ© GptOssTopKRouterã‚’åˆæœŸåŒ–é–‹å§‹ config.num_experts_per_tok=4, config.num_local_experts=32, config.hidden_size=2880
ğŸŸ© GptOssTopKRouterã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© GptOssExpertsã‚’åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=2880, config.num_local_experts=32, config.hidden_size=2880
ğŸŸ© GptOssExpertsã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© GptOssMLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size=2880, eps=1e-05
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size=2880, eps=1e-05
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ¦ self.attention_type='full_attention'
ğŸŸ© GptOssDecoderLayerã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© GptOssDecoderLayerã‚’åˆæœŸåŒ–é–‹å§‹ config.hidden_size=2880, config.intermediate_size=2880, config.num_attention_heads=64, config.num_key_value_heads=8, config.attention_bias=True, config.attention_dropout=0.0, layer_idx=12
ğŸŸ© GptOssAttentionã‚’åˆæœŸåŒ–é–‹å§‹ config.hidden_size=2880, config.num_attention_heads=64, config.num_key_value_heads=8, config.attention_bias=True, config.attention_dropout=0.0, layer_idx=12
ğŸŸ¦ self.head_dim=64
ğŸŸ¦ self.num_key_value_groups=8
ğŸŸ¦ self.scaling=0.125
ğŸŸ¦ self.attention_dropout=0.0
ğŸŸ¦ self.sliding_window=128
ğŸŸ© GptOssAttentionã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© GptOssMLPã‚’åˆæœŸåŒ–é–‹å§‹
ğŸŸ© GptOssTopKRouterã‚’åˆæœŸåŒ–é–‹å§‹ config.num_experts_per_tok=4, config.num_local_experts=32, config.hidden_size=2880
ğŸŸ© GptOssTopKRouterã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© GptOssExpertsã‚’åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=2880, config.num_local_experts=32, config.hidden_size=2880
ğŸŸ© GptOssExpertsã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© GptOssMLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size=2880, eps=1e-05
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size=2880, eps=1e-05
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ¦ self.attention_type='sliding_attention'
ğŸŸ© GptOssDecoderLayerã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© GptOssDecoderLayerã‚’åˆæœŸåŒ–é–‹å§‹ config.hidden_size=2880, config.intermediate_size=2880, config.num_attention_heads=64, config.num_key_value_heads=8, config.attention_bias=True, config.attention_dropout=0.0, layer_idx=13
ğŸŸ© GptOssAttentionã‚’åˆæœŸåŒ–é–‹å§‹ config.hidden_size=2880, config.num_attention_heads=64, config.num_key_value_heads=8, config.attention_bias=True, config.attention_dropout=0.0, layer_idx=13
ğŸŸ¦ self.head_dim=64
ğŸŸ¦ self.num_key_value_groups=8
ğŸŸ¦ self.scaling=0.125
ğŸŸ¦ self.attention_dropout=0.0
ğŸŸ¦ self.sliding_window=None
ğŸŸ© GptOssAttentionã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© GptOssMLPã‚’åˆæœŸåŒ–é–‹å§‹
ğŸŸ© GptOssTopKRouterã‚’åˆæœŸåŒ–é–‹å§‹ config.num_experts_per_tok=4, config.num_local_experts=32, config.hidden_size=2880
ğŸŸ© GptOssTopKRouterã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© GptOssExpertsã‚’åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=2880, config.num_local_experts=32, config.hidden_size=2880
ğŸŸ© GptOssExpertsã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© GptOssMLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size=2880, eps=1e-05
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size=2880, eps=1e-05
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ¦ self.attention_type='full_attention'
ğŸŸ© GptOssDecoderLayerã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© GptOssDecoderLayerã‚’åˆæœŸåŒ–é–‹å§‹ config.hidden_size=2880, config.intermediate_size=2880, config.num_attention_heads=64, config.num_key_value_heads=8, config.attention_bias=True, config.attention_dropout=0.0, layer_idx=14
ğŸŸ© GptOssAttentionã‚’åˆæœŸåŒ–é–‹å§‹ config.hidden_size=2880, config.num_attention_heads=64, config.num_key_value_heads=8, config.attention_bias=True, config.attention_dropout=0.0, layer_idx=14
ğŸŸ¦ self.head_dim=64
ğŸŸ¦ self.num_key_value_groups=8
ğŸŸ¦ self.scaling=0.125
ğŸŸ¦ self.attention_dropout=0.0
ğŸŸ¦ self.sliding_window=128
ğŸŸ© GptOssAttentionã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© GptOssMLPã‚’åˆæœŸåŒ–é–‹å§‹
ğŸŸ© GptOssTopKRouterã‚’åˆæœŸåŒ–é–‹å§‹ config.num_experts_per_tok=4, config.num_local_experts=32, config.hidden_size=2880
ğŸŸ© GptOssTopKRouterã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© GptOssExpertsã‚’åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=2880, config.num_local_experts=32, config.hidden_size=2880
ğŸŸ© GptOssExpertsã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© GptOssMLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size=2880, eps=1e-05
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size=2880, eps=1e-05
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ¦ self.attention_type='sliding_attention'
ğŸŸ© GptOssDecoderLayerã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© GptOssDecoderLayerã‚’åˆæœŸåŒ–é–‹å§‹ config.hidden_size=2880, config.intermediate_size=2880, config.num_attention_heads=64, config.num_key_value_heads=8, config.attention_bias=True, config.attention_dropout=0.0, layer_idx=15
ğŸŸ© GptOssAttentionã‚’åˆæœŸåŒ–é–‹å§‹ config.hidden_size=2880, config.num_attention_heads=64, config.num_key_value_heads=8, config.attention_bias=True, config.attention_dropout=0.0, layer_idx=15
ğŸŸ¦ self.head_dim=64
ğŸŸ¦ self.num_key_value_groups=8
ğŸŸ¦ self.scaling=0.125
ğŸŸ¦ self.attention_dropout=0.0
ğŸŸ¦ self.sliding_window=None
ğŸŸ© GptOssAttentionã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© GptOssMLPã‚’åˆæœŸåŒ–é–‹å§‹
ğŸŸ© GptOssTopKRouterã‚’åˆæœŸåŒ–é–‹å§‹ config.num_experts_per_tok=4, config.num_local_experts=32, config.hidden_size=2880
ğŸŸ© GptOssTopKRouterã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© GptOssExpertsã‚’åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=2880, config.num_local_experts=32, config.hidden_size=2880
ğŸŸ© GptOssExpertsã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© GptOssMLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size=2880, eps=1e-05
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size=2880, eps=1e-05
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ¦ self.attention_type='full_attention'
ğŸŸ© GptOssDecoderLayerã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© GptOssDecoderLayerã‚’åˆæœŸåŒ–é–‹å§‹ config.hidden_size=2880, config.intermediate_size=2880, config.num_attention_heads=64, config.num_key_value_heads=8, config.attention_bias=True, config.attention_dropout=0.0, layer_idx=16
ğŸŸ© GptOssAttentionã‚’åˆæœŸåŒ–é–‹å§‹ config.hidden_size=2880, config.num_attention_heads=64, config.num_key_value_heads=8, config.attention_bias=True, config.attention_dropout=0.0, layer_idx=16
ğŸŸ¦ self.head_dim=64
ğŸŸ¦ self.num_key_value_groups=8
ğŸŸ¦ self.scaling=0.125
ğŸŸ¦ self.attention_dropout=0.0
ğŸŸ¦ self.sliding_window=128
ğŸŸ© GptOssAttentionã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© GptOssMLPã‚’åˆæœŸåŒ–é–‹å§‹
ğŸŸ© GptOssTopKRouterã‚’åˆæœŸåŒ–é–‹å§‹ config.num_experts_per_tok=4, config.num_local_experts=32, config.hidden_size=2880
ğŸŸ© GptOssTopKRouterã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© GptOssExpertsã‚’åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=2880, config.num_local_experts=32, config.hidden_size=2880
ğŸŸ© GptOssExpertsã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© GptOssMLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size=2880, eps=1e-05
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size=2880, eps=1e-05
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ¦ self.attention_type='sliding_attention'
ğŸŸ© GptOssDecoderLayerã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© GptOssDecoderLayerã‚’åˆæœŸåŒ–é–‹å§‹ config.hidden_size=2880, config.intermediate_size=2880, config.num_attention_heads=64, config.num_key_value_heads=8, config.attention_bias=True, config.attention_dropout=0.0, layer_idx=17
ğŸŸ© GptOssAttentionã‚’åˆæœŸåŒ–é–‹å§‹ config.hidden_size=2880, config.num_attention_heads=64, config.num_key_value_heads=8, config.attention_bias=True, config.attention_dropout=0.0, layer_idx=17
ğŸŸ¦ self.head_dim=64
ğŸŸ¦ self.num_key_value_groups=8
ğŸŸ¦ self.scaling=0.125
ğŸŸ¦ self.attention_dropout=0.0
ğŸŸ¦ self.sliding_window=None
ğŸŸ© GptOssAttentionã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© GptOssMLPã‚’åˆæœŸåŒ–é–‹å§‹
ğŸŸ© GptOssTopKRouterã‚’åˆæœŸåŒ–é–‹å§‹ config.num_experts_per_tok=4, config.num_local_experts=32, config.hidden_size=2880
ğŸŸ© GptOssTopKRouterã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© GptOssExpertsã‚’åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=2880, config.num_local_experts=32, config.hidden_size=2880
ğŸŸ© GptOssExpertsã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© GptOssMLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size=2880, eps=1e-05
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size=2880, eps=1e-05
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ¦ self.attention_type='full_attention'
ğŸŸ© GptOssDecoderLayerã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© GptOssDecoderLayerã‚’åˆæœŸåŒ–é–‹å§‹ config.hidden_size=2880, config.intermediate_size=2880, config.num_attention_heads=64, config.num_key_value_heads=8, config.attention_bias=True, config.attention_dropout=0.0, layer_idx=18
ğŸŸ© GptOssAttentionã‚’åˆæœŸåŒ–é–‹å§‹ config.hidden_size=2880, config.num_attention_heads=64, config.num_key_value_heads=8, config.attention_bias=True, config.attention_dropout=0.0, layer_idx=18
ğŸŸ¦ self.head_dim=64
ğŸŸ¦ self.num_key_value_groups=8
ğŸŸ¦ self.scaling=0.125
ğŸŸ¦ self.attention_dropout=0.0
ğŸŸ¦ self.sliding_window=128
ğŸŸ© GptOssAttentionã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© GptOssMLPã‚’åˆæœŸåŒ–é–‹å§‹
ğŸŸ© GptOssTopKRouterã‚’åˆæœŸåŒ–é–‹å§‹ config.num_experts_per_tok=4, config.num_local_experts=32, config.hidden_size=2880
ğŸŸ© GptOssTopKRouterã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© GptOssExpertsã‚’åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=2880, config.num_local_experts=32, config.hidden_size=2880
ğŸŸ© GptOssExpertsã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© GptOssMLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size=2880, eps=1e-05
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size=2880, eps=1e-05
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ¦ self.attention_type='sliding_attention'
ğŸŸ© GptOssDecoderLayerã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© GptOssDecoderLayerã‚’åˆæœŸåŒ–é–‹å§‹ config.hidden_size=2880, config.intermediate_size=2880, config.num_attention_heads=64, config.num_key_value_heads=8, config.attention_bias=True, config.attention_dropout=0.0, layer_idx=19
ğŸŸ© GptOssAttentionã‚’åˆæœŸåŒ–é–‹å§‹ config.hidden_size=2880, config.num_attention_heads=64, config.num_key_value_heads=8, config.attention_bias=True, config.attention_dropout=0.0, layer_idx=19
ğŸŸ¦ self.head_dim=64
ğŸŸ¦ self.num_key_value_groups=8
ğŸŸ¦ self.scaling=0.125
ğŸŸ¦ self.attention_dropout=0.0
ğŸŸ¦ self.sliding_window=None
ğŸŸ© GptOssAttentionã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© GptOssMLPã‚’åˆæœŸåŒ–é–‹å§‹
ğŸŸ© GptOssTopKRouterã‚’åˆæœŸåŒ–é–‹å§‹ config.num_experts_per_tok=4, config.num_local_experts=32, config.hidden_size=2880
ğŸŸ© GptOssTopKRouterã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© GptOssExpertsã‚’åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=2880, config.num_local_experts=32, config.hidden_size=2880
ğŸŸ© GptOssExpertsã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© GptOssMLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size=2880, eps=1e-05
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size=2880, eps=1e-05
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ¦ self.attention_type='full_attention'
ğŸŸ© GptOssDecoderLayerã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© GptOssDecoderLayerã‚’åˆæœŸåŒ–é–‹å§‹ config.hidden_size=2880, config.intermediate_size=2880, config.num_attention_heads=64, config.num_key_value_heads=8, config.attention_bias=True, config.attention_dropout=0.0, layer_idx=20
ğŸŸ© GptOssAttentionã‚’åˆæœŸåŒ–é–‹å§‹ config.hidden_size=2880, config.num_attention_heads=64, config.num_key_value_heads=8, config.attention_bias=True, config.attention_dropout=0.0, layer_idx=20
ğŸŸ¦ self.head_dim=64
ğŸŸ¦ self.num_key_value_groups=8
ğŸŸ¦ self.scaling=0.125
ğŸŸ¦ self.attention_dropout=0.0
ğŸŸ¦ self.sliding_window=128
ğŸŸ© GptOssAttentionã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© GptOssMLPã‚’åˆæœŸåŒ–é–‹å§‹
ğŸŸ© GptOssTopKRouterã‚’åˆæœŸåŒ–é–‹å§‹ config.num_experts_per_tok=4, config.num_local_experts=32, config.hidden_size=2880
ğŸŸ© GptOssTopKRouterã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© GptOssExpertsã‚’åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=2880, config.num_local_experts=32, config.hidden_size=2880
ğŸŸ© GptOssExpertsã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© GptOssMLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size=2880, eps=1e-05
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size=2880, eps=1e-05
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ¦ self.attention_type='sliding_attention'
ğŸŸ© GptOssDecoderLayerã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© GptOssDecoderLayerã‚’åˆæœŸåŒ–é–‹å§‹ config.hidden_size=2880, config.intermediate_size=2880, config.num_attention_heads=64, config.num_key_value_heads=8, config.attention_bias=True, config.attention_dropout=0.0, layer_idx=21
ğŸŸ© GptOssAttentionã‚’åˆæœŸåŒ–é–‹å§‹ config.hidden_size=2880, config.num_attention_heads=64, config.num_key_value_heads=8, config.attention_bias=True, config.attention_dropout=0.0, layer_idx=21
ğŸŸ¦ self.head_dim=64
ğŸŸ¦ self.num_key_value_groups=8
ğŸŸ¦ self.scaling=0.125
ğŸŸ¦ self.attention_dropout=0.0
ğŸŸ¦ self.sliding_window=None
ğŸŸ© GptOssAttentionã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© GptOssMLPã‚’åˆæœŸåŒ–é–‹å§‹
ğŸŸ© GptOssTopKRouterã‚’åˆæœŸåŒ–é–‹å§‹ config.num_experts_per_tok=4, config.num_local_experts=32, config.hidden_size=2880
ğŸŸ© GptOssTopKRouterã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© GptOssExpertsã‚’åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=2880, config.num_local_experts=32, config.hidden_size=2880
ğŸŸ© GptOssExpertsã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© GptOssMLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size=2880, eps=1e-05
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size=2880, eps=1e-05
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ¦ self.attention_type='full_attention'
ğŸŸ© GptOssDecoderLayerã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© GptOssDecoderLayerã‚’åˆæœŸåŒ–é–‹å§‹ config.hidden_size=2880, config.intermediate_size=2880, config.num_attention_heads=64, config.num_key_value_heads=8, config.attention_bias=True, config.attention_dropout=0.0, layer_idx=22
ğŸŸ© GptOssAttentionã‚’åˆæœŸåŒ–é–‹å§‹ config.hidden_size=2880, config.num_attention_heads=64, config.num_key_value_heads=8, config.attention_bias=True, config.attention_dropout=0.0, layer_idx=22
ğŸŸ¦ self.head_dim=64
ğŸŸ¦ self.num_key_value_groups=8
ğŸŸ¦ self.scaling=0.125
ğŸŸ¦ self.attention_dropout=0.0
ğŸŸ¦ self.sliding_window=128
ğŸŸ© GptOssAttentionã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© GptOssMLPã‚’åˆæœŸåŒ–é–‹å§‹
ğŸŸ© GptOssTopKRouterã‚’åˆæœŸåŒ–é–‹å§‹ config.num_experts_per_tok=4, config.num_local_experts=32, config.hidden_size=2880
ğŸŸ© GptOssTopKRouterã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© GptOssExpertsã‚’åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=2880, config.num_local_experts=32, config.hidden_size=2880
ğŸŸ© GptOssExpertsã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© GptOssMLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size=2880, eps=1e-05
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size=2880, eps=1e-05
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ¦ self.attention_type='sliding_attention'
ğŸŸ© GptOssDecoderLayerã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© GptOssDecoderLayerã‚’åˆæœŸåŒ–é–‹å§‹ config.hidden_size=2880, config.intermediate_size=2880, config.num_attention_heads=64, config.num_key_value_heads=8, config.attention_bias=True, config.attention_dropout=0.0, layer_idx=23
ğŸŸ© GptOssAttentionã‚’åˆæœŸåŒ–é–‹å§‹ config.hidden_size=2880, config.num_attention_heads=64, config.num_key_value_heads=8, config.attention_bias=True, config.attention_dropout=0.0, layer_idx=23
ğŸŸ¦ self.head_dim=64
ğŸŸ¦ self.num_key_value_groups=8
ğŸŸ¦ self.scaling=0.125
ğŸŸ¦ self.attention_dropout=0.0
ğŸŸ¦ self.sliding_window=None
ğŸŸ© GptOssAttentionã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© GptOssMLPã‚’åˆæœŸåŒ–é–‹å§‹
ğŸŸ© GptOssTopKRouterã‚’åˆæœŸåŒ–é–‹å§‹ config.num_experts_per_tok=4, config.num_local_experts=32, config.hidden_size=2880
ğŸŸ© GptOssTopKRouterã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© GptOssExpertsã‚’åˆæœŸåŒ–é–‹å§‹ config.intermediate_size=2880, config.num_local_experts=32, config.hidden_size=2880
ğŸŸ© GptOssExpertsã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© GptOssMLPã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size=2880, eps=1e-05
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size=2880, eps=1e-05
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ¦ self.attention_type='full_attention'
ğŸŸ© GptOssDecoderLayerã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© RMSNormã‚’åˆæœŸåŒ–é–‹å§‹ hidden_size=2880, eps=1e-05
ğŸŸ© RMSNormã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© GptOssRotaryEmbeddingã‚’åˆæœŸåŒ–é–‹å§‹ config.max_position_embeddings=131072 config.rope_scaling={'beta_fast': 32.0, 'beta_slow': 1.0, 'factor': 32.0, 'original_max_position_embeddings': 4096, 'rope_type': 'yarn', 'truncate': False}
ğŸŸ© RoPEã®åˆæœŸåŒ–é–¢æ•°: _compute_yarn_parameters
ğŸŸ© é€†å‘¨æ³¢æ•°ã®è¨ˆç®— inv_freq.shape=torch.Size([32]), inv_freq.dtype=torch.float32, self.attention_scaling=1.3465735902799727
ğŸŸ© GptOssRotaryEmbeddingã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© GptOssModelã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© GptOssForCausalLMã®åˆæœŸåŒ–å®Œäº†
ğŸŸ© loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--openai--gpt-oss-20b/snapshots/6cee5e81ee83917806bbde320786a8fb61efebee/generation_config.json
ğŸŸ© Generate config GenerationConfig {
  "bos_token_id": 199998,
  "do_sample": true,
  "eos_token_id": [
    200002,
    199999,
    200012
  ],
  "pad_token_id": 199999
}

ğŸŸ© Could not locate the custom_generate/generate.py inside openai/gpt-oss-20b.
ğŸŸ¨ Device set to use cuda
ğŸŸ© GptOssForCausalLMã®é †ä¼æ’­é–‹å§‹ input_ids.shape=torch.Size([1, 69]), position_ids.shape=torch.Size([1, 69]), past_key_values=DynamicCache(layers=[DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer]), use_cache=True, cache_position.shape=torch.Size([69]), logits_to_keep=1, attention_mask.shape=torch.Size([1, 69])
ğŸŸ¦ output_router_logits=False
ğŸŸ© GptOssModelã®é †ä¼æ’­é–‹å§‹ input_ids.shape=torch.Size([1, 69]), position_ids.shape=torch.Size([1, 69]), past_key_values=DynamicCache(layers=[DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer]), inputs_embeds=None, use_cache=True, cache_position.shape=torch.Size([69]), attention_mask.shape=torch.Size([1, 69])
ğŸŸ¦ å…¥åŠ›ã®åŸ‹ã‚è¾¼ã¿ã‚’è¨ˆç®— inputs_embeds.shape=torch.Size([1, 69, 2880]), inputs_embeds.dtype=torch.bfloat16
ğŸŸ¦ å› æœãƒã‚¹ã‚¯ã‚’æº–å‚™ causal_mask_mapping['full_attention'].shape=torch.Size([1, 1, 69, 69]), causal_mask_mapping['sliding_attention'].shape=torch.Size([1, 1, 69, 69])
ğŸŸ© GptOssRotaryEmbeddingã®é †ä¼æ’­é–‹å§‹ x.shape=torch.Size([1, 69, 2880]), x.dtype=torch.bfloat16, position_ids.shape=torch.Size([1, 69]), position_ids.dtype=torch.int64
ğŸŸ¦ inv_freq_expanded.shape=torch.Size([1, 32, 1]), inv_freq_expanded.dtype=torch.float32
ğŸŸ¦ position_ids_expanded.shape=torch.Size([1, 1, 69]), position_ids_expanded.dtype=torch.float32
ğŸŸ© GptOssRotaryEmbeddingã®é †ä¼æ’­å®Œäº† cos.shape=torch.Size([1, 69, 32]), cos.dtype=torch.float32, sin.shape=torch.Size([1, 69, 32]), sin.dtype=torch.float32
ğŸŸ¦ position_embeddings[0].shape=torch.Size([1, 69, 32]), position_embeddings[1].shape=torch.Size([1, 69, 32])
ğŸŸ© GptOssDecoderLayerã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16, position_ids.shape=torch.Size([1, 69]), past_key_values=DynamicCache(layers=[DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer]), use_cache=True, cache_position.shape=torch.Size([69]), position_embeddings[0].shape=torch.Size([1, 69, 32]), position_embeddings[1].shape=torch.Size([1, 69, 32]), attention_mask.shape=torch.Size([1, 1, 69, 69])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒƒãƒ—ã‚­ãƒ£ã‚¹ãƒˆ input_dtype=torch.bfloat16 hidden_states.dtype=torch.float32
ğŸŸ© RMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 69, 2880]), res.dtype=torch.bfloat16
ğŸŸ© GptOssAttentionã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 69, 2880]), position_embeddings[0].shape=torch.Size([1, 69, 32]), position_embeddings[1].shape=torch.Size([1, 69, 32]), past_key_values=DynamicCache(layers=[DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer]), cache_position.shape=torch.Size([69]), attention_mask.shape=torch.Size([1, 1, 69, 69])
ğŸŸ¦ input_shape=torch.Size([1, 69])
ğŸŸ¦ hidden_shape=(1, 69, -1, 64)
ğŸŸ¦ ã‚¯ã‚¨ãƒªã‚’è¨ˆç®— query_states.shape=torch.Size([1, 64, 69, 64])
ğŸŸ¦ ã‚­ãƒ¼ã‚’è¨ˆç®— key_states.shape=torch.Size([1, 8, 69, 64])
ğŸŸ¦ ãƒãƒªãƒ¥ãƒ¼ã‚’è¨ˆç®— value_states.shape=torch.Size([1, 8, 69, 64])
ğŸŸ¦ RoPEã®cosã¨sinã‚’å–å¾— cos.shape=torch.Size([1, 69, 32]), sin.shape=torch.Size([1, 69, 32])
ğŸŸ¦ RoPEã‚’é©ç”¨ query_states.shape=torch.Size([1, 64, 69, 64]), key_states.shape=torch.Size([1, 8, 69, 64])
ğŸŸ¦ ã‚­ãƒ¼ãƒãƒªãƒ¥ãƒ¼ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æ›´æ–° key_states.shape=torch.Size([1, 8, 69, 64]), value_states.shape=torch.Size([1, 8, 69, 64])
ğŸŸ¦ ä½¿ç”¨ã™ã‚‹ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å®Ÿè£…: eager_attention_forward
ğŸŸ© eager_attention_forwardã®é–‹å§‹ query.shape=torch.Size([1, 64, 69, 64]), key.shape=torch.Size([1, 8, 69, 64]), value.shape=torch.Size([1, 8, 69, 64]), scaling=0.125, dropout=0.0 attention_mask.shape=torch.Size([1, 1, 69, 69])
ğŸŸ© repeat_kvã®é–‹å§‹ hidden_states.shape=torch.Size([1, 8, 69, 64]), hidden_states.dtype=torch.bfloat16, n_rep=8
ğŸŸ¦ hidden_states.shape=torch.Size([1, 8, 8, 69, 64]), hidden_states.dtype=torch.bfloat16
ğŸŸ© repeat_kvã®å®Œäº† res.shape=torch.Size([1, 64, 69, 64]), res.dtype=torch.bfloat16
ğŸŸ¦ ã‚­ãƒ¼ã‚’è¤‡è£½ key_states.shape=torch.Size([1, 64, 69, 64]), key_states.dtype=torch.bfloat16
ğŸŸ© repeat_kvã®é–‹å§‹ hidden_states.shape=torch.Size([1, 8, 69, 64]), hidden_states.dtype=torch.bfloat16, n_rep=8
ğŸŸ¦ hidden_states.shape=torch.Size([1, 8, 8, 69, 64]), hidden_states.dtype=torch.bfloat16
ğŸŸ© repeat_kvã®å®Œäº† res.shape=torch.Size([1, 64, 69, 64]), res.dtype=torch.bfloat16
ğŸŸ¦ ãƒãƒªãƒ¥ãƒ¼ã‚’è¤‡è£½ value_states.shape=torch.Size([1, 64, 69, 64]), value_states.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ­ã‚¸ãƒƒãƒˆã‚’è¨ˆç®— attn_weights.shape=torch.Size([1, 64, 69, 69]), attn_weights.dtype=torch.bfloat16
ğŸŸ¦ causal_mask.shape=torch.Size([1, 1, 69, 69]), causal_mask.dtype=torch.bfloat16
ğŸŸ¦ å› æœãƒã‚¹ã‚¯é©ç”¨ attn_weights.shape=torch.Size([1, 64, 69, 69]), attn_weights.dtype=torch.bfloat16
ğŸŸ¦ sinks.shape=torch.Size([1, 64, 69, 1]), sinks.dtype=torch.bfloat16
ğŸŸ¦ ã‚·ãƒ³ã‚¯ã‚’è¿½åŠ  combined_logits.shape=torch.Size([1, 64, 69, 70]), combined_logits.dtype=torch.bfloat16
ğŸŸ¦ æœ€å¤§å€¤ã‚’æ¸›ç®— combined_logits.shape=torch.Size([1, 64, 69, 70]), combined_logits.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®— probs.shape=torch.Size([1, 64, 69, 70]), probs.dtype=torch.bfloat16
ğŸŸ¦ ã‚·ãƒ³ã‚¯ã®ç¢ºç‡ã‚’é™¤å» scores.shape=torch.Size([1, 64, 69, 69]), scores.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ attn_output.shape=torch.Size([1, 64, 69, 64]), attn_output.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’è»¢ç½® attn_output.shape=torch.Size([1, 69, 64, 64]), attn_output.dtype=torch.bfloat16
ğŸŸ© eager_attention_forwardã®å®Œäº† attn_output.shape=torch.Size([1, 69, 64, 64]), attn_output.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ attn_output.shape=torch.Size([1, 69, 64, 64])
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’å…ƒã®å½¢çŠ¶ã«æˆ»ã™ attn_output.shape=torch.Size([1, 69, 4096])
ğŸŸ© GptOssAttentionã®é †ä¼æ’­å®Œäº† attn_output.shape=torch.Size([1, 69, 2880]), attn_output.dtype=torch.bfloat16, attn_weights.shape=torch.Size([1, 64, 69, 69]), attn_weights.dtype=torch.bfloat16
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒƒãƒ—ã‚­ãƒ£ã‚¹ãƒˆ input_dtype=torch.bfloat16 hidden_states.dtype=torch.float32
ğŸŸ© RMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 69, 2880]), res.dtype=torch.bfloat16
ğŸŸ¦ MoEå±¤ã®å…¥åŠ› hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16
ğŸŸ¦ MoEå±¤ã®å‡ºåŠ› hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16
ğŸŸ© GptOssDecoderLayerã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16, position_ids.shape=torch.Size([1, 69]), past_key_values=DynamicCache(layers=[DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer]), use_cache=True, cache_position.shape=torch.Size([69]), position_embeddings[0].shape=torch.Size([1, 69, 32]), position_embeddings[1].shape=torch.Size([1, 69, 32]), attention_mask.shape=torch.Size([1, 1, 69, 69])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒƒãƒ—ã‚­ãƒ£ã‚¹ãƒˆ input_dtype=torch.bfloat16 hidden_states.dtype=torch.float32
ğŸŸ© RMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 69, 2880]), res.dtype=torch.bfloat16
ğŸŸ© GptOssAttentionã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 69, 2880]), position_embeddings[0].shape=torch.Size([1, 69, 32]), position_embeddings[1].shape=torch.Size([1, 69, 32]), past_key_values=DynamicCache(layers=[DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer]), cache_position.shape=torch.Size([69]), attention_mask.shape=torch.Size([1, 1, 69, 69])
ğŸŸ¦ input_shape=torch.Size([1, 69])
ğŸŸ¦ hidden_shape=(1, 69, -1, 64)
ğŸŸ¦ ã‚¯ã‚¨ãƒªã‚’è¨ˆç®— query_states.shape=torch.Size([1, 64, 69, 64])
ğŸŸ¦ ã‚­ãƒ¼ã‚’è¨ˆç®— key_states.shape=torch.Size([1, 8, 69, 64])
ğŸŸ¦ ãƒãƒªãƒ¥ãƒ¼ã‚’è¨ˆç®— value_states.shape=torch.Size([1, 8, 69, 64])
ğŸŸ¦ RoPEã®cosã¨sinã‚’å–å¾— cos.shape=torch.Size([1, 69, 32]), sin.shape=torch.Size([1, 69, 32])
ğŸŸ¦ RoPEã‚’é©ç”¨ query_states.shape=torch.Size([1, 64, 69, 64]), key_states.shape=torch.Size([1, 8, 69, 64])
ğŸŸ¦ ã‚­ãƒ¼ãƒãƒªãƒ¥ãƒ¼ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æ›´æ–° key_states.shape=torch.Size([1, 8, 69, 64]), value_states.shape=torch.Size([1, 8, 69, 64])
ğŸŸ¦ ä½¿ç”¨ã™ã‚‹ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å®Ÿè£…: eager_attention_forward
ğŸŸ© eager_attention_forwardã®é–‹å§‹ query.shape=torch.Size([1, 64, 69, 64]), key.shape=torch.Size([1, 8, 69, 64]), value.shape=torch.Size([1, 8, 69, 64]), scaling=0.125, dropout=0.0 attention_mask.shape=torch.Size([1, 1, 69, 69])
ğŸŸ© repeat_kvã®é–‹å§‹ hidden_states.shape=torch.Size([1, 8, 69, 64]), hidden_states.dtype=torch.bfloat16, n_rep=8
ğŸŸ¦ hidden_states.shape=torch.Size([1, 8, 8, 69, 64]), hidden_states.dtype=torch.bfloat16
ğŸŸ© repeat_kvã®å®Œäº† res.shape=torch.Size([1, 64, 69, 64]), res.dtype=torch.bfloat16
ğŸŸ¦ ã‚­ãƒ¼ã‚’è¤‡è£½ key_states.shape=torch.Size([1, 64, 69, 64]), key_states.dtype=torch.bfloat16
ğŸŸ© repeat_kvã®é–‹å§‹ hidden_states.shape=torch.Size([1, 8, 69, 64]), hidden_states.dtype=torch.bfloat16, n_rep=8
ğŸŸ¦ hidden_states.shape=torch.Size([1, 8, 8, 69, 64]), hidden_states.dtype=torch.bfloat16
ğŸŸ© repeat_kvã®å®Œäº† res.shape=torch.Size([1, 64, 69, 64]), res.dtype=torch.bfloat16
ğŸŸ¦ ãƒãƒªãƒ¥ãƒ¼ã‚’è¤‡è£½ value_states.shape=torch.Size([1, 64, 69, 64]), value_states.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ­ã‚¸ãƒƒãƒˆã‚’è¨ˆç®— attn_weights.shape=torch.Size([1, 64, 69, 69]), attn_weights.dtype=torch.bfloat16
ğŸŸ¦ causal_mask.shape=torch.Size([1, 1, 69, 69]), causal_mask.dtype=torch.bfloat16
ğŸŸ¦ å› æœãƒã‚¹ã‚¯é©ç”¨ attn_weights.shape=torch.Size([1, 64, 69, 69]), attn_weights.dtype=torch.bfloat16
ğŸŸ¦ sinks.shape=torch.Size([1, 64, 69, 1]), sinks.dtype=torch.bfloat16
ğŸŸ¦ ã‚·ãƒ³ã‚¯ã‚’è¿½åŠ  combined_logits.shape=torch.Size([1, 64, 69, 70]), combined_logits.dtype=torch.bfloat16
ğŸŸ¦ æœ€å¤§å€¤ã‚’æ¸›ç®— combined_logits.shape=torch.Size([1, 64, 69, 70]), combined_logits.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®— probs.shape=torch.Size([1, 64, 69, 70]), probs.dtype=torch.bfloat16
ğŸŸ¦ ã‚·ãƒ³ã‚¯ã®ç¢ºç‡ã‚’é™¤å» scores.shape=torch.Size([1, 64, 69, 69]), scores.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ attn_output.shape=torch.Size([1, 64, 69, 64]), attn_output.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’è»¢ç½® attn_output.shape=torch.Size([1, 69, 64, 64]), attn_output.dtype=torch.bfloat16
ğŸŸ© eager_attention_forwardã®å®Œäº† attn_output.shape=torch.Size([1, 69, 64, 64]), attn_output.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ attn_output.shape=torch.Size([1, 69, 64, 64])
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’å…ƒã®å½¢çŠ¶ã«æˆ»ã™ attn_output.shape=torch.Size([1, 69, 4096])
ğŸŸ© GptOssAttentionã®é †ä¼æ’­å®Œäº† attn_output.shape=torch.Size([1, 69, 2880]), attn_output.dtype=torch.bfloat16, attn_weights.shape=torch.Size([1, 64, 69, 69]), attn_weights.dtype=torch.bfloat16
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒƒãƒ—ã‚­ãƒ£ã‚¹ãƒˆ input_dtype=torch.bfloat16 hidden_states.dtype=torch.float32
ğŸŸ© RMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 69, 2880]), res.dtype=torch.bfloat16
ğŸŸ¦ MoEå±¤ã®å…¥åŠ› hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16
ğŸŸ¦ MoEå±¤ã®å‡ºåŠ› hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16
ğŸŸ© GptOssDecoderLayerã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16, position_ids.shape=torch.Size([1, 69]), past_key_values=DynamicCache(layers=[DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer]), use_cache=True, cache_position.shape=torch.Size([69]), position_embeddings[0].shape=torch.Size([1, 69, 32]), position_embeddings[1].shape=torch.Size([1, 69, 32]), attention_mask.shape=torch.Size([1, 1, 69, 69])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒƒãƒ—ã‚­ãƒ£ã‚¹ãƒˆ input_dtype=torch.bfloat16 hidden_states.dtype=torch.float32
ğŸŸ© RMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 69, 2880]), res.dtype=torch.bfloat16
ğŸŸ© GptOssAttentionã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 69, 2880]), position_embeddings[0].shape=torch.Size([1, 69, 32]), position_embeddings[1].shape=torch.Size([1, 69, 32]), past_key_values=DynamicCache(layers=[DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer]), cache_position.shape=torch.Size([69]), attention_mask.shape=torch.Size([1, 1, 69, 69])
ğŸŸ¦ input_shape=torch.Size([1, 69])
ğŸŸ¦ hidden_shape=(1, 69, -1, 64)
ğŸŸ¦ ã‚¯ã‚¨ãƒªã‚’è¨ˆç®— query_states.shape=torch.Size([1, 64, 69, 64])
ğŸŸ¦ ã‚­ãƒ¼ã‚’è¨ˆç®— key_states.shape=torch.Size([1, 8, 69, 64])
ğŸŸ¦ ãƒãƒªãƒ¥ãƒ¼ã‚’è¨ˆç®— value_states.shape=torch.Size([1, 8, 69, 64])
ğŸŸ¦ RoPEã®cosã¨sinã‚’å–å¾— cos.shape=torch.Size([1, 69, 32]), sin.shape=torch.Size([1, 69, 32])
ğŸŸ¦ RoPEã‚’é©ç”¨ query_states.shape=torch.Size([1, 64, 69, 64]), key_states.shape=torch.Size([1, 8, 69, 64])
ğŸŸ¦ ã‚­ãƒ¼ãƒãƒªãƒ¥ãƒ¼ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æ›´æ–° key_states.shape=torch.Size([1, 8, 69, 64]), value_states.shape=torch.Size([1, 8, 69, 64])
ğŸŸ¦ ä½¿ç”¨ã™ã‚‹ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å®Ÿè£…: eager_attention_forward
ğŸŸ© eager_attention_forwardã®é–‹å§‹ query.shape=torch.Size([1, 64, 69, 64]), key.shape=torch.Size([1, 8, 69, 64]), value.shape=torch.Size([1, 8, 69, 64]), scaling=0.125, dropout=0.0 attention_mask.shape=torch.Size([1, 1, 69, 69])
ğŸŸ© repeat_kvã®é–‹å§‹ hidden_states.shape=torch.Size([1, 8, 69, 64]), hidden_states.dtype=torch.bfloat16, n_rep=8
ğŸŸ¦ hidden_states.shape=torch.Size([1, 8, 8, 69, 64]), hidden_states.dtype=torch.bfloat16
ğŸŸ© repeat_kvã®å®Œäº† res.shape=torch.Size([1, 64, 69, 64]), res.dtype=torch.bfloat16
ğŸŸ¦ ã‚­ãƒ¼ã‚’è¤‡è£½ key_states.shape=torch.Size([1, 64, 69, 64]), key_states.dtype=torch.bfloat16
ğŸŸ© repeat_kvã®é–‹å§‹ hidden_states.shape=torch.Size([1, 8, 69, 64]), hidden_states.dtype=torch.bfloat16, n_rep=8
ğŸŸ¦ hidden_states.shape=torch.Size([1, 8, 8, 69, 64]), hidden_states.dtype=torch.bfloat16
ğŸŸ© repeat_kvã®å®Œäº† res.shape=torch.Size([1, 64, 69, 64]), res.dtype=torch.bfloat16
ğŸŸ¦ ãƒãƒªãƒ¥ãƒ¼ã‚’è¤‡è£½ value_states.shape=torch.Size([1, 64, 69, 64]), value_states.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ­ã‚¸ãƒƒãƒˆã‚’è¨ˆç®— attn_weights.shape=torch.Size([1, 64, 69, 69]), attn_weights.dtype=torch.bfloat16
ğŸŸ¦ causal_mask.shape=torch.Size([1, 1, 69, 69]), causal_mask.dtype=torch.bfloat16
ğŸŸ¦ å› æœãƒã‚¹ã‚¯é©ç”¨ attn_weights.shape=torch.Size([1, 64, 69, 69]), attn_weights.dtype=torch.bfloat16
ğŸŸ¦ sinks.shape=torch.Size([1, 64, 69, 1]), sinks.dtype=torch.bfloat16
ğŸŸ¦ ã‚·ãƒ³ã‚¯ã‚’è¿½åŠ  combined_logits.shape=torch.Size([1, 64, 69, 70]), combined_logits.dtype=torch.bfloat16
ğŸŸ¦ æœ€å¤§å€¤ã‚’æ¸›ç®— combined_logits.shape=torch.Size([1, 64, 69, 70]), combined_logits.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®— probs.shape=torch.Size([1, 64, 69, 70]), probs.dtype=torch.bfloat16
ğŸŸ¦ ã‚·ãƒ³ã‚¯ã®ç¢ºç‡ã‚’é™¤å» scores.shape=torch.Size([1, 64, 69, 69]), scores.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ attn_output.shape=torch.Size([1, 64, 69, 64]), attn_output.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’è»¢ç½® attn_output.shape=torch.Size([1, 69, 64, 64]), attn_output.dtype=torch.bfloat16
ğŸŸ© eager_attention_forwardã®å®Œäº† attn_output.shape=torch.Size([1, 69, 64, 64]), attn_output.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ attn_output.shape=torch.Size([1, 69, 64, 64])
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’å…ƒã®å½¢çŠ¶ã«æˆ»ã™ attn_output.shape=torch.Size([1, 69, 4096])
ğŸŸ© GptOssAttentionã®é †ä¼æ’­å®Œäº† attn_output.shape=torch.Size([1, 69, 2880]), attn_output.dtype=torch.bfloat16, attn_weights.shape=torch.Size([1, 64, 69, 69]), attn_weights.dtype=torch.bfloat16
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒƒãƒ—ã‚­ãƒ£ã‚¹ãƒˆ input_dtype=torch.bfloat16 hidden_states.dtype=torch.float32
ğŸŸ© RMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 69, 2880]), res.dtype=torch.bfloat16
ğŸŸ¦ MoEå±¤ã®å…¥åŠ› hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16
ğŸŸ¦ MoEå±¤ã®å‡ºåŠ› hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16
ğŸŸ© GptOssDecoderLayerã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16, position_ids.shape=torch.Size([1, 69]), past_key_values=DynamicCache(layers=[DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer]), use_cache=True, cache_position.shape=torch.Size([69]), position_embeddings[0].shape=torch.Size([1, 69, 32]), position_embeddings[1].shape=torch.Size([1, 69, 32]), attention_mask.shape=torch.Size([1, 1, 69, 69])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒƒãƒ—ã‚­ãƒ£ã‚¹ãƒˆ input_dtype=torch.bfloat16 hidden_states.dtype=torch.float32
ğŸŸ© RMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 69, 2880]), res.dtype=torch.bfloat16
ğŸŸ© GptOssAttentionã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 69, 2880]), position_embeddings[0].shape=torch.Size([1, 69, 32]), position_embeddings[1].shape=torch.Size([1, 69, 32]), past_key_values=DynamicCache(layers=[DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer]), cache_position.shape=torch.Size([69]), attention_mask.shape=torch.Size([1, 1, 69, 69])
ğŸŸ¦ input_shape=torch.Size([1, 69])
ğŸŸ¦ hidden_shape=(1, 69, -1, 64)
ğŸŸ¦ ã‚¯ã‚¨ãƒªã‚’è¨ˆç®— query_states.shape=torch.Size([1, 64, 69, 64])
ğŸŸ¦ ã‚­ãƒ¼ã‚’è¨ˆç®— key_states.shape=torch.Size([1, 8, 69, 64])
ğŸŸ¦ ãƒãƒªãƒ¥ãƒ¼ã‚’è¨ˆç®— value_states.shape=torch.Size([1, 8, 69, 64])
ğŸŸ¦ RoPEã®cosã¨sinã‚’å–å¾— cos.shape=torch.Size([1, 69, 32]), sin.shape=torch.Size([1, 69, 32])
ğŸŸ¦ RoPEã‚’é©ç”¨ query_states.shape=torch.Size([1, 64, 69, 64]), key_states.shape=torch.Size([1, 8, 69, 64])
ğŸŸ¦ ã‚­ãƒ¼ãƒãƒªãƒ¥ãƒ¼ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æ›´æ–° key_states.shape=torch.Size([1, 8, 69, 64]), value_states.shape=torch.Size([1, 8, 69, 64])
ğŸŸ¦ ä½¿ç”¨ã™ã‚‹ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å®Ÿè£…: eager_attention_forward
ğŸŸ© eager_attention_forwardã®é–‹å§‹ query.shape=torch.Size([1, 64, 69, 64]), key.shape=torch.Size([1, 8, 69, 64]), value.shape=torch.Size([1, 8, 69, 64]), scaling=0.125, dropout=0.0 attention_mask.shape=torch.Size([1, 1, 69, 69])
ğŸŸ© repeat_kvã®é–‹å§‹ hidden_states.shape=torch.Size([1, 8, 69, 64]), hidden_states.dtype=torch.bfloat16, n_rep=8
ğŸŸ¦ hidden_states.shape=torch.Size([1, 8, 8, 69, 64]), hidden_states.dtype=torch.bfloat16
ğŸŸ© repeat_kvã®å®Œäº† res.shape=torch.Size([1, 64, 69, 64]), res.dtype=torch.bfloat16
ğŸŸ¦ ã‚­ãƒ¼ã‚’è¤‡è£½ key_states.shape=torch.Size([1, 64, 69, 64]), key_states.dtype=torch.bfloat16
ğŸŸ© repeat_kvã®é–‹å§‹ hidden_states.shape=torch.Size([1, 8, 69, 64]), hidden_states.dtype=torch.bfloat16, n_rep=8
ğŸŸ¦ hidden_states.shape=torch.Size([1, 8, 8, 69, 64]), hidden_states.dtype=torch.bfloat16
ğŸŸ© repeat_kvã®å®Œäº† res.shape=torch.Size([1, 64, 69, 64]), res.dtype=torch.bfloat16
ğŸŸ¦ ãƒãƒªãƒ¥ãƒ¼ã‚’è¤‡è£½ value_states.shape=torch.Size([1, 64, 69, 64]), value_states.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ­ã‚¸ãƒƒãƒˆã‚’è¨ˆç®— attn_weights.shape=torch.Size([1, 64, 69, 69]), attn_weights.dtype=torch.bfloat16
ğŸŸ¦ causal_mask.shape=torch.Size([1, 1, 69, 69]), causal_mask.dtype=torch.bfloat16
ğŸŸ¦ å› æœãƒã‚¹ã‚¯é©ç”¨ attn_weights.shape=torch.Size([1, 64, 69, 69]), attn_weights.dtype=torch.bfloat16
ğŸŸ¦ sinks.shape=torch.Size([1, 64, 69, 1]), sinks.dtype=torch.bfloat16
ğŸŸ¦ ã‚·ãƒ³ã‚¯ã‚’è¿½åŠ  combined_logits.shape=torch.Size([1, 64, 69, 70]), combined_logits.dtype=torch.bfloat16
ğŸŸ¦ æœ€å¤§å€¤ã‚’æ¸›ç®— combined_logits.shape=torch.Size([1, 64, 69, 70]), combined_logits.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®— probs.shape=torch.Size([1, 64, 69, 70]), probs.dtype=torch.bfloat16
ğŸŸ¦ ã‚·ãƒ³ã‚¯ã®ç¢ºç‡ã‚’é™¤å» scores.shape=torch.Size([1, 64, 69, 69]), scores.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ attn_output.shape=torch.Size([1, 64, 69, 64]), attn_output.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’è»¢ç½® attn_output.shape=torch.Size([1, 69, 64, 64]), attn_output.dtype=torch.bfloat16
ğŸŸ© eager_attention_forwardã®å®Œäº† attn_output.shape=torch.Size([1, 69, 64, 64]), attn_output.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ attn_output.shape=torch.Size([1, 69, 64, 64])
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’å…ƒã®å½¢çŠ¶ã«æˆ»ã™ attn_output.shape=torch.Size([1, 69, 4096])
ğŸŸ© GptOssAttentionã®é †ä¼æ’­å®Œäº† attn_output.shape=torch.Size([1, 69, 2880]), attn_output.dtype=torch.bfloat16, attn_weights.shape=torch.Size([1, 64, 69, 69]), attn_weights.dtype=torch.bfloat16
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒƒãƒ—ã‚­ãƒ£ã‚¹ãƒˆ input_dtype=torch.bfloat16 hidden_states.dtype=torch.float32
ğŸŸ© RMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 69, 2880]), res.dtype=torch.bfloat16
ğŸŸ¦ MoEå±¤ã®å…¥åŠ› hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16
ğŸŸ¦ MoEå±¤ã®å‡ºåŠ› hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16
ğŸŸ© GptOssDecoderLayerã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16, position_ids.shape=torch.Size([1, 69]), past_key_values=DynamicCache(layers=[DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer]), use_cache=True, cache_position.shape=torch.Size([69]), position_embeddings[0].shape=torch.Size([1, 69, 32]), position_embeddings[1].shape=torch.Size([1, 69, 32]), attention_mask.shape=torch.Size([1, 1, 69, 69])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒƒãƒ—ã‚­ãƒ£ã‚¹ãƒˆ input_dtype=torch.bfloat16 hidden_states.dtype=torch.float32
ğŸŸ© RMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 69, 2880]), res.dtype=torch.bfloat16
ğŸŸ© GptOssAttentionã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 69, 2880]), position_embeddings[0].shape=torch.Size([1, 69, 32]), position_embeddings[1].shape=torch.Size([1, 69, 32]), past_key_values=DynamicCache(layers=[DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer]), cache_position.shape=torch.Size([69]), attention_mask.shape=torch.Size([1, 1, 69, 69])
ğŸŸ¦ input_shape=torch.Size([1, 69])
ğŸŸ¦ hidden_shape=(1, 69, -1, 64)
ğŸŸ¦ ã‚¯ã‚¨ãƒªã‚’è¨ˆç®— query_states.shape=torch.Size([1, 64, 69, 64])
ğŸŸ¦ ã‚­ãƒ¼ã‚’è¨ˆç®— key_states.shape=torch.Size([1, 8, 69, 64])
ğŸŸ¦ ãƒãƒªãƒ¥ãƒ¼ã‚’è¨ˆç®— value_states.shape=torch.Size([1, 8, 69, 64])
ğŸŸ¦ RoPEã®cosã¨sinã‚’å–å¾— cos.shape=torch.Size([1, 69, 32]), sin.shape=torch.Size([1, 69, 32])
ğŸŸ¦ RoPEã‚’é©ç”¨ query_states.shape=torch.Size([1, 64, 69, 64]), key_states.shape=torch.Size([1, 8, 69, 64])
ğŸŸ¦ ã‚­ãƒ¼ãƒãƒªãƒ¥ãƒ¼ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æ›´æ–° key_states.shape=torch.Size([1, 8, 69, 64]), value_states.shape=torch.Size([1, 8, 69, 64])
ğŸŸ¦ ä½¿ç”¨ã™ã‚‹ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å®Ÿè£…: eager_attention_forward
ğŸŸ© eager_attention_forwardã®é–‹å§‹ query.shape=torch.Size([1, 64, 69, 64]), key.shape=torch.Size([1, 8, 69, 64]), value.shape=torch.Size([1, 8, 69, 64]), scaling=0.125, dropout=0.0 attention_mask.shape=torch.Size([1, 1, 69, 69])
ğŸŸ© repeat_kvã®é–‹å§‹ hidden_states.shape=torch.Size([1, 8, 69, 64]), hidden_states.dtype=torch.bfloat16, n_rep=8
ğŸŸ¦ hidden_states.shape=torch.Size([1, 8, 8, 69, 64]), hidden_states.dtype=torch.bfloat16
ğŸŸ© repeat_kvã®å®Œäº† res.shape=torch.Size([1, 64, 69, 64]), res.dtype=torch.bfloat16
ğŸŸ¦ ã‚­ãƒ¼ã‚’è¤‡è£½ key_states.shape=torch.Size([1, 64, 69, 64]), key_states.dtype=torch.bfloat16
ğŸŸ© repeat_kvã®é–‹å§‹ hidden_states.shape=torch.Size([1, 8, 69, 64]), hidden_states.dtype=torch.bfloat16, n_rep=8
ğŸŸ¦ hidden_states.shape=torch.Size([1, 8, 8, 69, 64]), hidden_states.dtype=torch.bfloat16
ğŸŸ© repeat_kvã®å®Œäº† res.shape=torch.Size([1, 64, 69, 64]), res.dtype=torch.bfloat16
ğŸŸ¦ ãƒãƒªãƒ¥ãƒ¼ã‚’è¤‡è£½ value_states.shape=torch.Size([1, 64, 69, 64]), value_states.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ­ã‚¸ãƒƒãƒˆã‚’è¨ˆç®— attn_weights.shape=torch.Size([1, 64, 69, 69]), attn_weights.dtype=torch.bfloat16
ğŸŸ¦ causal_mask.shape=torch.Size([1, 1, 69, 69]), causal_mask.dtype=torch.bfloat16
ğŸŸ¦ å› æœãƒã‚¹ã‚¯é©ç”¨ attn_weights.shape=torch.Size([1, 64, 69, 69]), attn_weights.dtype=torch.bfloat16
ğŸŸ¦ sinks.shape=torch.Size([1, 64, 69, 1]), sinks.dtype=torch.bfloat16
ğŸŸ¦ ã‚·ãƒ³ã‚¯ã‚’è¿½åŠ  combined_logits.shape=torch.Size([1, 64, 69, 70]), combined_logits.dtype=torch.bfloat16
ğŸŸ¦ æœ€å¤§å€¤ã‚’æ¸›ç®— combined_logits.shape=torch.Size([1, 64, 69, 70]), combined_logits.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®— probs.shape=torch.Size([1, 64, 69, 70]), probs.dtype=torch.bfloat16
ğŸŸ¦ ã‚·ãƒ³ã‚¯ã®ç¢ºç‡ã‚’é™¤å» scores.shape=torch.Size([1, 64, 69, 69]), scores.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ attn_output.shape=torch.Size([1, 64, 69, 64]), attn_output.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’è»¢ç½® attn_output.shape=torch.Size([1, 69, 64, 64]), attn_output.dtype=torch.bfloat16
ğŸŸ© eager_attention_forwardã®å®Œäº† attn_output.shape=torch.Size([1, 69, 64, 64]), attn_output.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ attn_output.shape=torch.Size([1, 69, 64, 64])
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’å…ƒã®å½¢çŠ¶ã«æˆ»ã™ attn_output.shape=torch.Size([1, 69, 4096])
ğŸŸ© GptOssAttentionã®é †ä¼æ’­å®Œäº† attn_output.shape=torch.Size([1, 69, 2880]), attn_output.dtype=torch.bfloat16, attn_weights.shape=torch.Size([1, 64, 69, 69]), attn_weights.dtype=torch.bfloat16
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒƒãƒ—ã‚­ãƒ£ã‚¹ãƒˆ input_dtype=torch.bfloat16 hidden_states.dtype=torch.float32
ğŸŸ© RMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 69, 2880]), res.dtype=torch.bfloat16
ğŸŸ¦ MoEå±¤ã®å…¥åŠ› hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16
ğŸŸ¦ MoEå±¤ã®å‡ºåŠ› hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16
ğŸŸ© GptOssDecoderLayerã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16, position_ids.shape=torch.Size([1, 69]), past_key_values=DynamicCache(layers=[DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer]), use_cache=True, cache_position.shape=torch.Size([69]), position_embeddings[0].shape=torch.Size([1, 69, 32]), position_embeddings[1].shape=torch.Size([1, 69, 32]), attention_mask.shape=torch.Size([1, 1, 69, 69])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒƒãƒ—ã‚­ãƒ£ã‚¹ãƒˆ input_dtype=torch.bfloat16 hidden_states.dtype=torch.float32
ğŸŸ© RMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 69, 2880]), res.dtype=torch.bfloat16
ğŸŸ© GptOssAttentionã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 69, 2880]), position_embeddings[0].shape=torch.Size([1, 69, 32]), position_embeddings[1].shape=torch.Size([1, 69, 32]), past_key_values=DynamicCache(layers=[DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer]), cache_position.shape=torch.Size([69]), attention_mask.shape=torch.Size([1, 1, 69, 69])
ğŸŸ¦ input_shape=torch.Size([1, 69])
ğŸŸ¦ hidden_shape=(1, 69, -1, 64)
ğŸŸ¦ ã‚¯ã‚¨ãƒªã‚’è¨ˆç®— query_states.shape=torch.Size([1, 64, 69, 64])
ğŸŸ¦ ã‚­ãƒ¼ã‚’è¨ˆç®— key_states.shape=torch.Size([1, 8, 69, 64])
ğŸŸ¦ ãƒãƒªãƒ¥ãƒ¼ã‚’è¨ˆç®— value_states.shape=torch.Size([1, 8, 69, 64])
ğŸŸ¦ RoPEã®cosã¨sinã‚’å–å¾— cos.shape=torch.Size([1, 69, 32]), sin.shape=torch.Size([1, 69, 32])
ğŸŸ¦ RoPEã‚’é©ç”¨ query_states.shape=torch.Size([1, 64, 69, 64]), key_states.shape=torch.Size([1, 8, 69, 64])
ğŸŸ¦ ã‚­ãƒ¼ãƒãƒªãƒ¥ãƒ¼ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æ›´æ–° key_states.shape=torch.Size([1, 8, 69, 64]), value_states.shape=torch.Size([1, 8, 69, 64])
ğŸŸ¦ ä½¿ç”¨ã™ã‚‹ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å®Ÿè£…: eager_attention_forward
ğŸŸ© eager_attention_forwardã®é–‹å§‹ query.shape=torch.Size([1, 64, 69, 64]), key.shape=torch.Size([1, 8, 69, 64]), value.shape=torch.Size([1, 8, 69, 64]), scaling=0.125, dropout=0.0 attention_mask.shape=torch.Size([1, 1, 69, 69])
ğŸŸ© repeat_kvã®é–‹å§‹ hidden_states.shape=torch.Size([1, 8, 69, 64]), hidden_states.dtype=torch.bfloat16, n_rep=8
ğŸŸ¦ hidden_states.shape=torch.Size([1, 8, 8, 69, 64]), hidden_states.dtype=torch.bfloat16
ğŸŸ© repeat_kvã®å®Œäº† res.shape=torch.Size([1, 64, 69, 64]), res.dtype=torch.bfloat16
ğŸŸ¦ ã‚­ãƒ¼ã‚’è¤‡è£½ key_states.shape=torch.Size([1, 64, 69, 64]), key_states.dtype=torch.bfloat16
ğŸŸ© repeat_kvã®é–‹å§‹ hidden_states.shape=torch.Size([1, 8, 69, 64]), hidden_states.dtype=torch.bfloat16, n_rep=8
ğŸŸ¦ hidden_states.shape=torch.Size([1, 8, 8, 69, 64]), hidden_states.dtype=torch.bfloat16
ğŸŸ© repeat_kvã®å®Œäº† res.shape=torch.Size([1, 64, 69, 64]), res.dtype=torch.bfloat16
ğŸŸ¦ ãƒãƒªãƒ¥ãƒ¼ã‚’è¤‡è£½ value_states.shape=torch.Size([1, 64, 69, 64]), value_states.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ­ã‚¸ãƒƒãƒˆã‚’è¨ˆç®— attn_weights.shape=torch.Size([1, 64, 69, 69]), attn_weights.dtype=torch.bfloat16
ğŸŸ¦ causal_mask.shape=torch.Size([1, 1, 69, 69]), causal_mask.dtype=torch.bfloat16
ğŸŸ¦ å› æœãƒã‚¹ã‚¯é©ç”¨ attn_weights.shape=torch.Size([1, 64, 69, 69]), attn_weights.dtype=torch.bfloat16
ğŸŸ¦ sinks.shape=torch.Size([1, 64, 69, 1]), sinks.dtype=torch.bfloat16
ğŸŸ¦ ã‚·ãƒ³ã‚¯ã‚’è¿½åŠ  combined_logits.shape=torch.Size([1, 64, 69, 70]), combined_logits.dtype=torch.bfloat16
ğŸŸ¦ æœ€å¤§å€¤ã‚’æ¸›ç®— combined_logits.shape=torch.Size([1, 64, 69, 70]), combined_logits.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®— probs.shape=torch.Size([1, 64, 69, 70]), probs.dtype=torch.bfloat16
ğŸŸ¦ ã‚·ãƒ³ã‚¯ã®ç¢ºç‡ã‚’é™¤å» scores.shape=torch.Size([1, 64, 69, 69]), scores.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ attn_output.shape=torch.Size([1, 64, 69, 64]), attn_output.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’è»¢ç½® attn_output.shape=torch.Size([1, 69, 64, 64]), attn_output.dtype=torch.bfloat16
ğŸŸ© eager_attention_forwardã®å®Œäº† attn_output.shape=torch.Size([1, 69, 64, 64]), attn_output.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ attn_output.shape=torch.Size([1, 69, 64, 64])
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’å…ƒã®å½¢çŠ¶ã«æˆ»ã™ attn_output.shape=torch.Size([1, 69, 4096])
ğŸŸ© GptOssAttentionã®é †ä¼æ’­å®Œäº† attn_output.shape=torch.Size([1, 69, 2880]), attn_output.dtype=torch.bfloat16, attn_weights.shape=torch.Size([1, 64, 69, 69]), attn_weights.dtype=torch.bfloat16
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒƒãƒ—ã‚­ãƒ£ã‚¹ãƒˆ input_dtype=torch.bfloat16 hidden_states.dtype=torch.float32
ğŸŸ© RMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 69, 2880]), res.dtype=torch.bfloat16
ğŸŸ¦ MoEå±¤ã®å…¥åŠ› hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16
ğŸŸ¦ MoEå±¤ã®å‡ºåŠ› hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16
ğŸŸ© GptOssDecoderLayerã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16, position_ids.shape=torch.Size([1, 69]), past_key_values=DynamicCache(layers=[DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer]), use_cache=True, cache_position.shape=torch.Size([69]), position_embeddings[0].shape=torch.Size([1, 69, 32]), position_embeddings[1].shape=torch.Size([1, 69, 32]), attention_mask.shape=torch.Size([1, 1, 69, 69])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒƒãƒ—ã‚­ãƒ£ã‚¹ãƒˆ input_dtype=torch.bfloat16 hidden_states.dtype=torch.float32
ğŸŸ© RMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 69, 2880]), res.dtype=torch.bfloat16
ğŸŸ© GptOssAttentionã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 69, 2880]), position_embeddings[0].shape=torch.Size([1, 69, 32]), position_embeddings[1].shape=torch.Size([1, 69, 32]), past_key_values=DynamicCache(layers=[DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer]), cache_position.shape=torch.Size([69]), attention_mask.shape=torch.Size([1, 1, 69, 69])
ğŸŸ¦ input_shape=torch.Size([1, 69])
ğŸŸ¦ hidden_shape=(1, 69, -1, 64)
ğŸŸ¦ ã‚¯ã‚¨ãƒªã‚’è¨ˆç®— query_states.shape=torch.Size([1, 64, 69, 64])
ğŸŸ¦ ã‚­ãƒ¼ã‚’è¨ˆç®— key_states.shape=torch.Size([1, 8, 69, 64])
ğŸŸ¦ ãƒãƒªãƒ¥ãƒ¼ã‚’è¨ˆç®— value_states.shape=torch.Size([1, 8, 69, 64])
ğŸŸ¦ RoPEã®cosã¨sinã‚’å–å¾— cos.shape=torch.Size([1, 69, 32]), sin.shape=torch.Size([1, 69, 32])
ğŸŸ¦ RoPEã‚’é©ç”¨ query_states.shape=torch.Size([1, 64, 69, 64]), key_states.shape=torch.Size([1, 8, 69, 64])
ğŸŸ¦ ã‚­ãƒ¼ãƒãƒªãƒ¥ãƒ¼ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æ›´æ–° key_states.shape=torch.Size([1, 8, 69, 64]), value_states.shape=torch.Size([1, 8, 69, 64])
ğŸŸ¦ ä½¿ç”¨ã™ã‚‹ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å®Ÿè£…: eager_attention_forward
ğŸŸ© eager_attention_forwardã®é–‹å§‹ query.shape=torch.Size([1, 64, 69, 64]), key.shape=torch.Size([1, 8, 69, 64]), value.shape=torch.Size([1, 8, 69, 64]), scaling=0.125, dropout=0.0 attention_mask.shape=torch.Size([1, 1, 69, 69])
ğŸŸ© repeat_kvã®é–‹å§‹ hidden_states.shape=torch.Size([1, 8, 69, 64]), hidden_states.dtype=torch.bfloat16, n_rep=8
ğŸŸ¦ hidden_states.shape=torch.Size([1, 8, 8, 69, 64]), hidden_states.dtype=torch.bfloat16
ğŸŸ© repeat_kvã®å®Œäº† res.shape=torch.Size([1, 64, 69, 64]), res.dtype=torch.bfloat16
ğŸŸ¦ ã‚­ãƒ¼ã‚’è¤‡è£½ key_states.shape=torch.Size([1, 64, 69, 64]), key_states.dtype=torch.bfloat16
ğŸŸ© repeat_kvã®é–‹å§‹ hidden_states.shape=torch.Size([1, 8, 69, 64]), hidden_states.dtype=torch.bfloat16, n_rep=8
ğŸŸ¦ hidden_states.shape=torch.Size([1, 8, 8, 69, 64]), hidden_states.dtype=torch.bfloat16
ğŸŸ© repeat_kvã®å®Œäº† res.shape=torch.Size([1, 64, 69, 64]), res.dtype=torch.bfloat16
ğŸŸ¦ ãƒãƒªãƒ¥ãƒ¼ã‚’è¤‡è£½ value_states.shape=torch.Size([1, 64, 69, 64]), value_states.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ­ã‚¸ãƒƒãƒˆã‚’è¨ˆç®— attn_weights.shape=torch.Size([1, 64, 69, 69]), attn_weights.dtype=torch.bfloat16
ğŸŸ¦ causal_mask.shape=torch.Size([1, 1, 69, 69]), causal_mask.dtype=torch.bfloat16
ğŸŸ¦ å› æœãƒã‚¹ã‚¯é©ç”¨ attn_weights.shape=torch.Size([1, 64, 69, 69]), attn_weights.dtype=torch.bfloat16
ğŸŸ¦ sinks.shape=torch.Size([1, 64, 69, 1]), sinks.dtype=torch.bfloat16
ğŸŸ¦ ã‚·ãƒ³ã‚¯ã‚’è¿½åŠ  combined_logits.shape=torch.Size([1, 64, 69, 70]), combined_logits.dtype=torch.bfloat16
ğŸŸ¦ æœ€å¤§å€¤ã‚’æ¸›ç®— combined_logits.shape=torch.Size([1, 64, 69, 70]), combined_logits.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®— probs.shape=torch.Size([1, 64, 69, 70]), probs.dtype=torch.bfloat16
ğŸŸ¦ ã‚·ãƒ³ã‚¯ã®ç¢ºç‡ã‚’é™¤å» scores.shape=torch.Size([1, 64, 69, 69]), scores.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ attn_output.shape=torch.Size([1, 64, 69, 64]), attn_output.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’è»¢ç½® attn_output.shape=torch.Size([1, 69, 64, 64]), attn_output.dtype=torch.bfloat16
ğŸŸ© eager_attention_forwardã®å®Œäº† attn_output.shape=torch.Size([1, 69, 64, 64]), attn_output.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ attn_output.shape=torch.Size([1, 69, 64, 64])
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’å…ƒã®å½¢çŠ¶ã«æˆ»ã™ attn_output.shape=torch.Size([1, 69, 4096])
ğŸŸ© GptOssAttentionã®é †ä¼æ’­å®Œäº† attn_output.shape=torch.Size([1, 69, 2880]), attn_output.dtype=torch.bfloat16, attn_weights.shape=torch.Size([1, 64, 69, 69]), attn_weights.dtype=torch.bfloat16
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒƒãƒ—ã‚­ãƒ£ã‚¹ãƒˆ input_dtype=torch.bfloat16 hidden_states.dtype=torch.float32
ğŸŸ© RMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 69, 2880]), res.dtype=torch.bfloat16
ğŸŸ¦ MoEå±¤ã®å…¥åŠ› hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16
ğŸŸ¦ MoEå±¤ã®å‡ºåŠ› hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16
ğŸŸ© GptOssDecoderLayerã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16, position_ids.shape=torch.Size([1, 69]), past_key_values=DynamicCache(layers=[DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer]), use_cache=True, cache_position.shape=torch.Size([69]), position_embeddings[0].shape=torch.Size([1, 69, 32]), position_embeddings[1].shape=torch.Size([1, 69, 32]), attention_mask.shape=torch.Size([1, 1, 69, 69])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒƒãƒ—ã‚­ãƒ£ã‚¹ãƒˆ input_dtype=torch.bfloat16 hidden_states.dtype=torch.float32
ğŸŸ© RMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 69, 2880]), res.dtype=torch.bfloat16
ğŸŸ© GptOssAttentionã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 69, 2880]), position_embeddings[0].shape=torch.Size([1, 69, 32]), position_embeddings[1].shape=torch.Size([1, 69, 32]), past_key_values=DynamicCache(layers=[DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer]), cache_position.shape=torch.Size([69]), attention_mask.shape=torch.Size([1, 1, 69, 69])
ğŸŸ¦ input_shape=torch.Size([1, 69])
ğŸŸ¦ hidden_shape=(1, 69, -1, 64)
ğŸŸ¦ ã‚¯ã‚¨ãƒªã‚’è¨ˆç®— query_states.shape=torch.Size([1, 64, 69, 64])
ğŸŸ¦ ã‚­ãƒ¼ã‚’è¨ˆç®— key_states.shape=torch.Size([1, 8, 69, 64])
ğŸŸ¦ ãƒãƒªãƒ¥ãƒ¼ã‚’è¨ˆç®— value_states.shape=torch.Size([1, 8, 69, 64])
ğŸŸ¦ RoPEã®cosã¨sinã‚’å–å¾— cos.shape=torch.Size([1, 69, 32]), sin.shape=torch.Size([1, 69, 32])
ğŸŸ¦ RoPEã‚’é©ç”¨ query_states.shape=torch.Size([1, 64, 69, 64]), key_states.shape=torch.Size([1, 8, 69, 64])
ğŸŸ¦ ã‚­ãƒ¼ãƒãƒªãƒ¥ãƒ¼ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æ›´æ–° key_states.shape=torch.Size([1, 8, 69, 64]), value_states.shape=torch.Size([1, 8, 69, 64])
ğŸŸ¦ ä½¿ç”¨ã™ã‚‹ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å®Ÿè£…: eager_attention_forward
ğŸŸ© eager_attention_forwardã®é–‹å§‹ query.shape=torch.Size([1, 64, 69, 64]), key.shape=torch.Size([1, 8, 69, 64]), value.shape=torch.Size([1, 8, 69, 64]), scaling=0.125, dropout=0.0 attention_mask.shape=torch.Size([1, 1, 69, 69])
ğŸŸ© repeat_kvã®é–‹å§‹ hidden_states.shape=torch.Size([1, 8, 69, 64]), hidden_states.dtype=torch.bfloat16, n_rep=8
ğŸŸ¦ hidden_states.shape=torch.Size([1, 8, 8, 69, 64]), hidden_states.dtype=torch.bfloat16
ğŸŸ© repeat_kvã®å®Œäº† res.shape=torch.Size([1, 64, 69, 64]), res.dtype=torch.bfloat16
ğŸŸ¦ ã‚­ãƒ¼ã‚’è¤‡è£½ key_states.shape=torch.Size([1, 64, 69, 64]), key_states.dtype=torch.bfloat16
ğŸŸ© repeat_kvã®é–‹å§‹ hidden_states.shape=torch.Size([1, 8, 69, 64]), hidden_states.dtype=torch.bfloat16, n_rep=8
ğŸŸ¦ hidden_states.shape=torch.Size([1, 8, 8, 69, 64]), hidden_states.dtype=torch.bfloat16
ğŸŸ© repeat_kvã®å®Œäº† res.shape=torch.Size([1, 64, 69, 64]), res.dtype=torch.bfloat16
ğŸŸ¦ ãƒãƒªãƒ¥ãƒ¼ã‚’è¤‡è£½ value_states.shape=torch.Size([1, 64, 69, 64]), value_states.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ­ã‚¸ãƒƒãƒˆã‚’è¨ˆç®— attn_weights.shape=torch.Size([1, 64, 69, 69]), attn_weights.dtype=torch.bfloat16
ğŸŸ¦ causal_mask.shape=torch.Size([1, 1, 69, 69]), causal_mask.dtype=torch.bfloat16
ğŸŸ¦ å› æœãƒã‚¹ã‚¯é©ç”¨ attn_weights.shape=torch.Size([1, 64, 69, 69]), attn_weights.dtype=torch.bfloat16
ğŸŸ¦ sinks.shape=torch.Size([1, 64, 69, 1]), sinks.dtype=torch.bfloat16
ğŸŸ¦ ã‚·ãƒ³ã‚¯ã‚’è¿½åŠ  combined_logits.shape=torch.Size([1, 64, 69, 70]), combined_logits.dtype=torch.bfloat16
ğŸŸ¦ æœ€å¤§å€¤ã‚’æ¸›ç®— combined_logits.shape=torch.Size([1, 64, 69, 70]), combined_logits.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®— probs.shape=torch.Size([1, 64, 69, 70]), probs.dtype=torch.bfloat16
ğŸŸ¦ ã‚·ãƒ³ã‚¯ã®ç¢ºç‡ã‚’é™¤å» scores.shape=torch.Size([1, 64, 69, 69]), scores.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ attn_output.shape=torch.Size([1, 64, 69, 64]), attn_output.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’è»¢ç½® attn_output.shape=torch.Size([1, 69, 64, 64]), attn_output.dtype=torch.bfloat16
ğŸŸ© eager_attention_forwardã®å®Œäº† attn_output.shape=torch.Size([1, 69, 64, 64]), attn_output.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ attn_output.shape=torch.Size([1, 69, 64, 64])
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’å…ƒã®å½¢çŠ¶ã«æˆ»ã™ attn_output.shape=torch.Size([1, 69, 4096])
ğŸŸ© GptOssAttentionã®é †ä¼æ’­å®Œäº† attn_output.shape=torch.Size([1, 69, 2880]), attn_output.dtype=torch.bfloat16, attn_weights.shape=torch.Size([1, 64, 69, 69]), attn_weights.dtype=torch.bfloat16
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒƒãƒ—ã‚­ãƒ£ã‚¹ãƒˆ input_dtype=torch.bfloat16 hidden_states.dtype=torch.float32
ğŸŸ© RMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 69, 2880]), res.dtype=torch.bfloat16
ğŸŸ¦ MoEå±¤ã®å…¥åŠ› hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16
ğŸŸ¦ MoEå±¤ã®å‡ºåŠ› hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16
ğŸŸ© GptOssDecoderLayerã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16, position_ids.shape=torch.Size([1, 69]), past_key_values=DynamicCache(layers=[DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer]), use_cache=True, cache_position.shape=torch.Size([69]), position_embeddings[0].shape=torch.Size([1, 69, 32]), position_embeddings[1].shape=torch.Size([1, 69, 32]), attention_mask.shape=torch.Size([1, 1, 69, 69])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒƒãƒ—ã‚­ãƒ£ã‚¹ãƒˆ input_dtype=torch.bfloat16 hidden_states.dtype=torch.float32
ğŸŸ© RMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 69, 2880]), res.dtype=torch.bfloat16
ğŸŸ© GptOssAttentionã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 69, 2880]), position_embeddings[0].shape=torch.Size([1, 69, 32]), position_embeddings[1].shape=torch.Size([1, 69, 32]), past_key_values=DynamicCache(layers=[DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer]), cache_position.shape=torch.Size([69]), attention_mask.shape=torch.Size([1, 1, 69, 69])
ğŸŸ¦ input_shape=torch.Size([1, 69])
ğŸŸ¦ hidden_shape=(1, 69, -1, 64)
ğŸŸ¦ ã‚¯ã‚¨ãƒªã‚’è¨ˆç®— query_states.shape=torch.Size([1, 64, 69, 64])
ğŸŸ¦ ã‚­ãƒ¼ã‚’è¨ˆç®— key_states.shape=torch.Size([1, 8, 69, 64])
ğŸŸ¦ ãƒãƒªãƒ¥ãƒ¼ã‚’è¨ˆç®— value_states.shape=torch.Size([1, 8, 69, 64])
ğŸŸ¦ RoPEã®cosã¨sinã‚’å–å¾— cos.shape=torch.Size([1, 69, 32]), sin.shape=torch.Size([1, 69, 32])
ğŸŸ¦ RoPEã‚’é©ç”¨ query_states.shape=torch.Size([1, 64, 69, 64]), key_states.shape=torch.Size([1, 8, 69, 64])
ğŸŸ¦ ã‚­ãƒ¼ãƒãƒªãƒ¥ãƒ¼ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æ›´æ–° key_states.shape=torch.Size([1, 8, 69, 64]), value_states.shape=torch.Size([1, 8, 69, 64])
ğŸŸ¦ ä½¿ç”¨ã™ã‚‹ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å®Ÿè£…: eager_attention_forward
ğŸŸ© eager_attention_forwardã®é–‹å§‹ query.shape=torch.Size([1, 64, 69, 64]), key.shape=torch.Size([1, 8, 69, 64]), value.shape=torch.Size([1, 8, 69, 64]), scaling=0.125, dropout=0.0 attention_mask.shape=torch.Size([1, 1, 69, 69])
ğŸŸ© repeat_kvã®é–‹å§‹ hidden_states.shape=torch.Size([1, 8, 69, 64]), hidden_states.dtype=torch.bfloat16, n_rep=8
ğŸŸ¦ hidden_states.shape=torch.Size([1, 8, 8, 69, 64]), hidden_states.dtype=torch.bfloat16
ğŸŸ© repeat_kvã®å®Œäº† res.shape=torch.Size([1, 64, 69, 64]), res.dtype=torch.bfloat16
ğŸŸ¦ ã‚­ãƒ¼ã‚’è¤‡è£½ key_states.shape=torch.Size([1, 64, 69, 64]), key_states.dtype=torch.bfloat16
ğŸŸ© repeat_kvã®é–‹å§‹ hidden_states.shape=torch.Size([1, 8, 69, 64]), hidden_states.dtype=torch.bfloat16, n_rep=8
ğŸŸ¦ hidden_states.shape=torch.Size([1, 8, 8, 69, 64]), hidden_states.dtype=torch.bfloat16
ğŸŸ© repeat_kvã®å®Œäº† res.shape=torch.Size([1, 64, 69, 64]), res.dtype=torch.bfloat16
ğŸŸ¦ ãƒãƒªãƒ¥ãƒ¼ã‚’è¤‡è£½ value_states.shape=torch.Size([1, 64, 69, 64]), value_states.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ­ã‚¸ãƒƒãƒˆã‚’è¨ˆç®— attn_weights.shape=torch.Size([1, 64, 69, 69]), attn_weights.dtype=torch.bfloat16
ğŸŸ¦ causal_mask.shape=torch.Size([1, 1, 69, 69]), causal_mask.dtype=torch.bfloat16
ğŸŸ¦ å› æœãƒã‚¹ã‚¯é©ç”¨ attn_weights.shape=torch.Size([1, 64, 69, 69]), attn_weights.dtype=torch.bfloat16
ğŸŸ¦ sinks.shape=torch.Size([1, 64, 69, 1]), sinks.dtype=torch.bfloat16
ğŸŸ¦ ã‚·ãƒ³ã‚¯ã‚’è¿½åŠ  combined_logits.shape=torch.Size([1, 64, 69, 70]), combined_logits.dtype=torch.bfloat16
ğŸŸ¦ æœ€å¤§å€¤ã‚’æ¸›ç®— combined_logits.shape=torch.Size([1, 64, 69, 70]), combined_logits.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®— probs.shape=torch.Size([1, 64, 69, 70]), probs.dtype=torch.bfloat16
ğŸŸ¦ ã‚·ãƒ³ã‚¯ã®ç¢ºç‡ã‚’é™¤å» scores.shape=torch.Size([1, 64, 69, 69]), scores.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ attn_output.shape=torch.Size([1, 64, 69, 64]), attn_output.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’è»¢ç½® attn_output.shape=torch.Size([1, 69, 64, 64]), attn_output.dtype=torch.bfloat16
ğŸŸ© eager_attention_forwardã®å®Œäº† attn_output.shape=torch.Size([1, 69, 64, 64]), attn_output.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ attn_output.shape=torch.Size([1, 69, 64, 64])
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’å…ƒã®å½¢çŠ¶ã«æˆ»ã™ attn_output.shape=torch.Size([1, 69, 4096])
ğŸŸ© GptOssAttentionã®é †ä¼æ’­å®Œäº† attn_output.shape=torch.Size([1, 69, 2880]), attn_output.dtype=torch.bfloat16, attn_weights.shape=torch.Size([1, 64, 69, 69]), attn_weights.dtype=torch.bfloat16
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒƒãƒ—ã‚­ãƒ£ã‚¹ãƒˆ input_dtype=torch.bfloat16 hidden_states.dtype=torch.float32
ğŸŸ© RMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 69, 2880]), res.dtype=torch.bfloat16
ğŸŸ¦ MoEå±¤ã®å…¥åŠ› hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16
ğŸŸ¦ MoEå±¤ã®å‡ºåŠ› hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16
ğŸŸ© GptOssDecoderLayerã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16, position_ids.shape=torch.Size([1, 69]), past_key_values=DynamicCache(layers=[DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer]), use_cache=True, cache_position.shape=torch.Size([69]), position_embeddings[0].shape=torch.Size([1, 69, 32]), position_embeddings[1].shape=torch.Size([1, 69, 32]), attention_mask.shape=torch.Size([1, 1, 69, 69])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒƒãƒ—ã‚­ãƒ£ã‚¹ãƒˆ input_dtype=torch.bfloat16 hidden_states.dtype=torch.float32
ğŸŸ© RMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 69, 2880]), res.dtype=torch.bfloat16
ğŸŸ© GptOssAttentionã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 69, 2880]), position_embeddings[0].shape=torch.Size([1, 69, 32]), position_embeddings[1].shape=torch.Size([1, 69, 32]), past_key_values=DynamicCache(layers=[DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer]), cache_position.shape=torch.Size([69]), attention_mask.shape=torch.Size([1, 1, 69, 69])
ğŸŸ¦ input_shape=torch.Size([1, 69])
ğŸŸ¦ hidden_shape=(1, 69, -1, 64)
ğŸŸ¦ ã‚¯ã‚¨ãƒªã‚’è¨ˆç®— query_states.shape=torch.Size([1, 64, 69, 64])
ğŸŸ¦ ã‚­ãƒ¼ã‚’è¨ˆç®— key_states.shape=torch.Size([1, 8, 69, 64])
ğŸŸ¦ ãƒãƒªãƒ¥ãƒ¼ã‚’è¨ˆç®— value_states.shape=torch.Size([1, 8, 69, 64])
ğŸŸ¦ RoPEã®cosã¨sinã‚’å–å¾— cos.shape=torch.Size([1, 69, 32]), sin.shape=torch.Size([1, 69, 32])
ğŸŸ¦ RoPEã‚’é©ç”¨ query_states.shape=torch.Size([1, 64, 69, 64]), key_states.shape=torch.Size([1, 8, 69, 64])
ğŸŸ¦ ã‚­ãƒ¼ãƒãƒªãƒ¥ãƒ¼ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æ›´æ–° key_states.shape=torch.Size([1, 8, 69, 64]), value_states.shape=torch.Size([1, 8, 69, 64])
ğŸŸ¦ ä½¿ç”¨ã™ã‚‹ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å®Ÿè£…: eager_attention_forward
ğŸŸ© eager_attention_forwardã®é–‹å§‹ query.shape=torch.Size([1, 64, 69, 64]), key.shape=torch.Size([1, 8, 69, 64]), value.shape=torch.Size([1, 8, 69, 64]), scaling=0.125, dropout=0.0 attention_mask.shape=torch.Size([1, 1, 69, 69])
ğŸŸ© repeat_kvã®é–‹å§‹ hidden_states.shape=torch.Size([1, 8, 69, 64]), hidden_states.dtype=torch.bfloat16, n_rep=8
ğŸŸ¦ hidden_states.shape=torch.Size([1, 8, 8, 69, 64]), hidden_states.dtype=torch.bfloat16
ğŸŸ© repeat_kvã®å®Œäº† res.shape=torch.Size([1, 64, 69, 64]), res.dtype=torch.bfloat16
ğŸŸ¦ ã‚­ãƒ¼ã‚’è¤‡è£½ key_states.shape=torch.Size([1, 64, 69, 64]), key_states.dtype=torch.bfloat16
ğŸŸ© repeat_kvã®é–‹å§‹ hidden_states.shape=torch.Size([1, 8, 69, 64]), hidden_states.dtype=torch.bfloat16, n_rep=8
ğŸŸ¦ hidden_states.shape=torch.Size([1, 8, 8, 69, 64]), hidden_states.dtype=torch.bfloat16
ğŸŸ© repeat_kvã®å®Œäº† res.shape=torch.Size([1, 64, 69, 64]), res.dtype=torch.bfloat16
ğŸŸ¦ ãƒãƒªãƒ¥ãƒ¼ã‚’è¤‡è£½ value_states.shape=torch.Size([1, 64, 69, 64]), value_states.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ­ã‚¸ãƒƒãƒˆã‚’è¨ˆç®— attn_weights.shape=torch.Size([1, 64, 69, 69]), attn_weights.dtype=torch.bfloat16
ğŸŸ¦ causal_mask.shape=torch.Size([1, 1, 69, 69]), causal_mask.dtype=torch.bfloat16
ğŸŸ¦ å› æœãƒã‚¹ã‚¯é©ç”¨ attn_weights.shape=torch.Size([1, 64, 69, 69]), attn_weights.dtype=torch.bfloat16
ğŸŸ¦ sinks.shape=torch.Size([1, 64, 69, 1]), sinks.dtype=torch.bfloat16
ğŸŸ¦ ã‚·ãƒ³ã‚¯ã‚’è¿½åŠ  combined_logits.shape=torch.Size([1, 64, 69, 70]), combined_logits.dtype=torch.bfloat16
ğŸŸ¦ æœ€å¤§å€¤ã‚’æ¸›ç®— combined_logits.shape=torch.Size([1, 64, 69, 70]), combined_logits.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®— probs.shape=torch.Size([1, 64, 69, 70]), probs.dtype=torch.bfloat16
ğŸŸ¦ ã‚·ãƒ³ã‚¯ã®ç¢ºç‡ã‚’é™¤å» scores.shape=torch.Size([1, 64, 69, 69]), scores.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ attn_output.shape=torch.Size([1, 64, 69, 64]), attn_output.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’è»¢ç½® attn_output.shape=torch.Size([1, 69, 64, 64]), attn_output.dtype=torch.bfloat16
ğŸŸ© eager_attention_forwardã®å®Œäº† attn_output.shape=torch.Size([1, 69, 64, 64]), attn_output.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ attn_output.shape=torch.Size([1, 69, 64, 64])
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’å…ƒã®å½¢çŠ¶ã«æˆ»ã™ attn_output.shape=torch.Size([1, 69, 4096])
ğŸŸ© GptOssAttentionã®é †ä¼æ’­å®Œäº† attn_output.shape=torch.Size([1, 69, 2880]), attn_output.dtype=torch.bfloat16, attn_weights.shape=torch.Size([1, 64, 69, 69]), attn_weights.dtype=torch.bfloat16
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒƒãƒ—ã‚­ãƒ£ã‚¹ãƒˆ input_dtype=torch.bfloat16 hidden_states.dtype=torch.float32
ğŸŸ© RMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 69, 2880]), res.dtype=torch.bfloat16
ğŸŸ¦ MoEå±¤ã®å…¥åŠ› hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16
ğŸŸ¦ MoEå±¤ã®å‡ºåŠ› hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16
ğŸŸ© GptOssDecoderLayerã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16, position_ids.shape=torch.Size([1, 69]), past_key_values=DynamicCache(layers=[DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer]), use_cache=True, cache_position.shape=torch.Size([69]), position_embeddings[0].shape=torch.Size([1, 69, 32]), position_embeddings[1].shape=torch.Size([1, 69, 32]), attention_mask.shape=torch.Size([1, 1, 69, 69])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒƒãƒ—ã‚­ãƒ£ã‚¹ãƒˆ input_dtype=torch.bfloat16 hidden_states.dtype=torch.float32
ğŸŸ© RMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 69, 2880]), res.dtype=torch.bfloat16
ğŸŸ© GptOssAttentionã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 69, 2880]), position_embeddings[0].shape=torch.Size([1, 69, 32]), position_embeddings[1].shape=torch.Size([1, 69, 32]), past_key_values=DynamicCache(layers=[DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer]), cache_position.shape=torch.Size([69]), attention_mask.shape=torch.Size([1, 1, 69, 69])
ğŸŸ¦ input_shape=torch.Size([1, 69])
ğŸŸ¦ hidden_shape=(1, 69, -1, 64)
ğŸŸ¦ ã‚¯ã‚¨ãƒªã‚’è¨ˆç®— query_states.shape=torch.Size([1, 64, 69, 64])
ğŸŸ¦ ã‚­ãƒ¼ã‚’è¨ˆç®— key_states.shape=torch.Size([1, 8, 69, 64])
ğŸŸ¦ ãƒãƒªãƒ¥ãƒ¼ã‚’è¨ˆç®— value_states.shape=torch.Size([1, 8, 69, 64])
ğŸŸ¦ RoPEã®cosã¨sinã‚’å–å¾— cos.shape=torch.Size([1, 69, 32]), sin.shape=torch.Size([1, 69, 32])
ğŸŸ¦ RoPEã‚’é©ç”¨ query_states.shape=torch.Size([1, 64, 69, 64]), key_states.shape=torch.Size([1, 8, 69, 64])
ğŸŸ¦ ã‚­ãƒ¼ãƒãƒªãƒ¥ãƒ¼ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æ›´æ–° key_states.shape=torch.Size([1, 8, 69, 64]), value_states.shape=torch.Size([1, 8, 69, 64])
ğŸŸ¦ ä½¿ç”¨ã™ã‚‹ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å®Ÿè£…: eager_attention_forward
ğŸŸ© eager_attention_forwardã®é–‹å§‹ query.shape=torch.Size([1, 64, 69, 64]), key.shape=torch.Size([1, 8, 69, 64]), value.shape=torch.Size([1, 8, 69, 64]), scaling=0.125, dropout=0.0 attention_mask.shape=torch.Size([1, 1, 69, 69])
ğŸŸ© repeat_kvã®é–‹å§‹ hidden_states.shape=torch.Size([1, 8, 69, 64]), hidden_states.dtype=torch.bfloat16, n_rep=8
ğŸŸ¦ hidden_states.shape=torch.Size([1, 8, 8, 69, 64]), hidden_states.dtype=torch.bfloat16
ğŸŸ© repeat_kvã®å®Œäº† res.shape=torch.Size([1, 64, 69, 64]), res.dtype=torch.bfloat16
ğŸŸ¦ ã‚­ãƒ¼ã‚’è¤‡è£½ key_states.shape=torch.Size([1, 64, 69, 64]), key_states.dtype=torch.bfloat16
ğŸŸ© repeat_kvã®é–‹å§‹ hidden_states.shape=torch.Size([1, 8, 69, 64]), hidden_states.dtype=torch.bfloat16, n_rep=8
ğŸŸ¦ hidden_states.shape=torch.Size([1, 8, 8, 69, 64]), hidden_states.dtype=torch.bfloat16
ğŸŸ© repeat_kvã®å®Œäº† res.shape=torch.Size([1, 64, 69, 64]), res.dtype=torch.bfloat16
ğŸŸ¦ ãƒãƒªãƒ¥ãƒ¼ã‚’è¤‡è£½ value_states.shape=torch.Size([1, 64, 69, 64]), value_states.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ­ã‚¸ãƒƒãƒˆã‚’è¨ˆç®— attn_weights.shape=torch.Size([1, 64, 69, 69]), attn_weights.dtype=torch.bfloat16
ğŸŸ¦ causal_mask.shape=torch.Size([1, 1, 69, 69]), causal_mask.dtype=torch.bfloat16
ğŸŸ¦ å› æœãƒã‚¹ã‚¯é©ç”¨ attn_weights.shape=torch.Size([1, 64, 69, 69]), attn_weights.dtype=torch.bfloat16
ğŸŸ¦ sinks.shape=torch.Size([1, 64, 69, 1]), sinks.dtype=torch.bfloat16
ğŸŸ¦ ã‚·ãƒ³ã‚¯ã‚’è¿½åŠ  combined_logits.shape=torch.Size([1, 64, 69, 70]), combined_logits.dtype=torch.bfloat16
ğŸŸ¦ æœ€å¤§å€¤ã‚’æ¸›ç®— combined_logits.shape=torch.Size([1, 64, 69, 70]), combined_logits.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®— probs.shape=torch.Size([1, 64, 69, 70]), probs.dtype=torch.bfloat16
ğŸŸ¦ ã‚·ãƒ³ã‚¯ã®ç¢ºç‡ã‚’é™¤å» scores.shape=torch.Size([1, 64, 69, 69]), scores.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ attn_output.shape=torch.Size([1, 64, 69, 64]), attn_output.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’è»¢ç½® attn_output.shape=torch.Size([1, 69, 64, 64]), attn_output.dtype=torch.bfloat16
ğŸŸ© eager_attention_forwardã®å®Œäº† attn_output.shape=torch.Size([1, 69, 64, 64]), attn_output.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ attn_output.shape=torch.Size([1, 69, 64, 64])
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’å…ƒã®å½¢çŠ¶ã«æˆ»ã™ attn_output.shape=torch.Size([1, 69, 4096])
ğŸŸ© GptOssAttentionã®é †ä¼æ’­å®Œäº† attn_output.shape=torch.Size([1, 69, 2880]), attn_output.dtype=torch.bfloat16, attn_weights.shape=torch.Size([1, 64, 69, 69]), attn_weights.dtype=torch.bfloat16
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒƒãƒ—ã‚­ãƒ£ã‚¹ãƒˆ input_dtype=torch.bfloat16 hidden_states.dtype=torch.float32
ğŸŸ© RMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 69, 2880]), res.dtype=torch.bfloat16
ğŸŸ¦ MoEå±¤ã®å…¥åŠ› hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16
ğŸŸ¦ MoEå±¤ã®å‡ºåŠ› hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16
ğŸŸ© GptOssDecoderLayerã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16, position_ids.shape=torch.Size([1, 69]), past_key_values=DynamicCache(layers=[DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer]), use_cache=True, cache_position.shape=torch.Size([69]), position_embeddings[0].shape=torch.Size([1, 69, 32]), position_embeddings[1].shape=torch.Size([1, 69, 32]), attention_mask.shape=torch.Size([1, 1, 69, 69])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒƒãƒ—ã‚­ãƒ£ã‚¹ãƒˆ input_dtype=torch.bfloat16 hidden_states.dtype=torch.float32
ğŸŸ© RMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 69, 2880]), res.dtype=torch.bfloat16
ğŸŸ© GptOssAttentionã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 69, 2880]), position_embeddings[0].shape=torch.Size([1, 69, 32]), position_embeddings[1].shape=torch.Size([1, 69, 32]), past_key_values=DynamicCache(layers=[DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer]), cache_position.shape=torch.Size([69]), attention_mask.shape=torch.Size([1, 1, 69, 69])
ğŸŸ¦ input_shape=torch.Size([1, 69])
ğŸŸ¦ hidden_shape=(1, 69, -1, 64)
ğŸŸ¦ ã‚¯ã‚¨ãƒªã‚’è¨ˆç®— query_states.shape=torch.Size([1, 64, 69, 64])
ğŸŸ¦ ã‚­ãƒ¼ã‚’è¨ˆç®— key_states.shape=torch.Size([1, 8, 69, 64])
ğŸŸ¦ ãƒãƒªãƒ¥ãƒ¼ã‚’è¨ˆç®— value_states.shape=torch.Size([1, 8, 69, 64])
ğŸŸ¦ RoPEã®cosã¨sinã‚’å–å¾— cos.shape=torch.Size([1, 69, 32]), sin.shape=torch.Size([1, 69, 32])
ğŸŸ¦ RoPEã‚’é©ç”¨ query_states.shape=torch.Size([1, 64, 69, 64]), key_states.shape=torch.Size([1, 8, 69, 64])
ğŸŸ¦ ã‚­ãƒ¼ãƒãƒªãƒ¥ãƒ¼ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æ›´æ–° key_states.shape=torch.Size([1, 8, 69, 64]), value_states.shape=torch.Size([1, 8, 69, 64])
ğŸŸ¦ ä½¿ç”¨ã™ã‚‹ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å®Ÿè£…: eager_attention_forward
ğŸŸ© eager_attention_forwardã®é–‹å§‹ query.shape=torch.Size([1, 64, 69, 64]), key.shape=torch.Size([1, 8, 69, 64]), value.shape=torch.Size([1, 8, 69, 64]), scaling=0.125, dropout=0.0 attention_mask.shape=torch.Size([1, 1, 69, 69])
ğŸŸ© repeat_kvã®é–‹å§‹ hidden_states.shape=torch.Size([1, 8, 69, 64]), hidden_states.dtype=torch.bfloat16, n_rep=8
ğŸŸ¦ hidden_states.shape=torch.Size([1, 8, 8, 69, 64]), hidden_states.dtype=torch.bfloat16
ğŸŸ© repeat_kvã®å®Œäº† res.shape=torch.Size([1, 64, 69, 64]), res.dtype=torch.bfloat16
ğŸŸ¦ ã‚­ãƒ¼ã‚’è¤‡è£½ key_states.shape=torch.Size([1, 64, 69, 64]), key_states.dtype=torch.bfloat16
ğŸŸ© repeat_kvã®é–‹å§‹ hidden_states.shape=torch.Size([1, 8, 69, 64]), hidden_states.dtype=torch.bfloat16, n_rep=8
ğŸŸ¦ hidden_states.shape=torch.Size([1, 8, 8, 69, 64]), hidden_states.dtype=torch.bfloat16
ğŸŸ© repeat_kvã®å®Œäº† res.shape=torch.Size([1, 64, 69, 64]), res.dtype=torch.bfloat16
ğŸŸ¦ ãƒãƒªãƒ¥ãƒ¼ã‚’è¤‡è£½ value_states.shape=torch.Size([1, 64, 69, 64]), value_states.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ­ã‚¸ãƒƒãƒˆã‚’è¨ˆç®— attn_weights.shape=torch.Size([1, 64, 69, 69]), attn_weights.dtype=torch.bfloat16
ğŸŸ¦ causal_mask.shape=torch.Size([1, 1, 69, 69]), causal_mask.dtype=torch.bfloat16
ğŸŸ¦ å› æœãƒã‚¹ã‚¯é©ç”¨ attn_weights.shape=torch.Size([1, 64, 69, 69]), attn_weights.dtype=torch.bfloat16
ğŸŸ¦ sinks.shape=torch.Size([1, 64, 69, 1]), sinks.dtype=torch.bfloat16
ğŸŸ¦ ã‚·ãƒ³ã‚¯ã‚’è¿½åŠ  combined_logits.shape=torch.Size([1, 64, 69, 70]), combined_logits.dtype=torch.bfloat16
ğŸŸ¦ æœ€å¤§å€¤ã‚’æ¸›ç®— combined_logits.shape=torch.Size([1, 64, 69, 70]), combined_logits.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®— probs.shape=torch.Size([1, 64, 69, 70]), probs.dtype=torch.bfloat16
ğŸŸ¦ ã‚·ãƒ³ã‚¯ã®ç¢ºç‡ã‚’é™¤å» scores.shape=torch.Size([1, 64, 69, 69]), scores.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ attn_output.shape=torch.Size([1, 64, 69, 64]), attn_output.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’è»¢ç½® attn_output.shape=torch.Size([1, 69, 64, 64]), attn_output.dtype=torch.bfloat16
ğŸŸ© eager_attention_forwardã®å®Œäº† attn_output.shape=torch.Size([1, 69, 64, 64]), attn_output.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ attn_output.shape=torch.Size([1, 69, 64, 64])
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’å…ƒã®å½¢çŠ¶ã«æˆ»ã™ attn_output.shape=torch.Size([1, 69, 4096])
ğŸŸ© GptOssAttentionã®é †ä¼æ’­å®Œäº† attn_output.shape=torch.Size([1, 69, 2880]), attn_output.dtype=torch.bfloat16, attn_weights.shape=torch.Size([1, 64, 69, 69]), attn_weights.dtype=torch.bfloat16
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒƒãƒ—ã‚­ãƒ£ã‚¹ãƒˆ input_dtype=torch.bfloat16 hidden_states.dtype=torch.float32
ğŸŸ© RMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 69, 2880]), res.dtype=torch.bfloat16
ğŸŸ¦ MoEå±¤ã®å…¥åŠ› hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16
ğŸŸ¦ MoEå±¤ã®å‡ºåŠ› hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16
ğŸŸ© GptOssDecoderLayerã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16, position_ids.shape=torch.Size([1, 69]), past_key_values=DynamicCache(layers=[DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer]), use_cache=True, cache_position.shape=torch.Size([69]), position_embeddings[0].shape=torch.Size([1, 69, 32]), position_embeddings[1].shape=torch.Size([1, 69, 32]), attention_mask.shape=torch.Size([1, 1, 69, 69])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒƒãƒ—ã‚­ãƒ£ã‚¹ãƒˆ input_dtype=torch.bfloat16 hidden_states.dtype=torch.float32
ğŸŸ© RMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 69, 2880]), res.dtype=torch.bfloat16
ğŸŸ© GptOssAttentionã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 69, 2880]), position_embeddings[0].shape=torch.Size([1, 69, 32]), position_embeddings[1].shape=torch.Size([1, 69, 32]), past_key_values=DynamicCache(layers=[DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer]), cache_position.shape=torch.Size([69]), attention_mask.shape=torch.Size([1, 1, 69, 69])
ğŸŸ¦ input_shape=torch.Size([1, 69])
ğŸŸ¦ hidden_shape=(1, 69, -1, 64)
ğŸŸ¦ ã‚¯ã‚¨ãƒªã‚’è¨ˆç®— query_states.shape=torch.Size([1, 64, 69, 64])
ğŸŸ¦ ã‚­ãƒ¼ã‚’è¨ˆç®— key_states.shape=torch.Size([1, 8, 69, 64])
ğŸŸ¦ ãƒãƒªãƒ¥ãƒ¼ã‚’è¨ˆç®— value_states.shape=torch.Size([1, 8, 69, 64])
ğŸŸ¦ RoPEã®cosã¨sinã‚’å–å¾— cos.shape=torch.Size([1, 69, 32]), sin.shape=torch.Size([1, 69, 32])
ğŸŸ¦ RoPEã‚’é©ç”¨ query_states.shape=torch.Size([1, 64, 69, 64]), key_states.shape=torch.Size([1, 8, 69, 64])
ğŸŸ¦ ã‚­ãƒ¼ãƒãƒªãƒ¥ãƒ¼ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æ›´æ–° key_states.shape=torch.Size([1, 8, 69, 64]), value_states.shape=torch.Size([1, 8, 69, 64])
ğŸŸ¦ ä½¿ç”¨ã™ã‚‹ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å®Ÿè£…: eager_attention_forward
ğŸŸ© eager_attention_forwardã®é–‹å§‹ query.shape=torch.Size([1, 64, 69, 64]), key.shape=torch.Size([1, 8, 69, 64]), value.shape=torch.Size([1, 8, 69, 64]), scaling=0.125, dropout=0.0 attention_mask.shape=torch.Size([1, 1, 69, 69])
ğŸŸ© repeat_kvã®é–‹å§‹ hidden_states.shape=torch.Size([1, 8, 69, 64]), hidden_states.dtype=torch.bfloat16, n_rep=8
ğŸŸ¦ hidden_states.shape=torch.Size([1, 8, 8, 69, 64]), hidden_states.dtype=torch.bfloat16
ğŸŸ© repeat_kvã®å®Œäº† res.shape=torch.Size([1, 64, 69, 64]), res.dtype=torch.bfloat16
ğŸŸ¦ ã‚­ãƒ¼ã‚’è¤‡è£½ key_states.shape=torch.Size([1, 64, 69, 64]), key_states.dtype=torch.bfloat16
ğŸŸ© repeat_kvã®é–‹å§‹ hidden_states.shape=torch.Size([1, 8, 69, 64]), hidden_states.dtype=torch.bfloat16, n_rep=8
ğŸŸ¦ hidden_states.shape=torch.Size([1, 8, 8, 69, 64]), hidden_states.dtype=torch.bfloat16
ğŸŸ© repeat_kvã®å®Œäº† res.shape=torch.Size([1, 64, 69, 64]), res.dtype=torch.bfloat16
ğŸŸ¦ ãƒãƒªãƒ¥ãƒ¼ã‚’è¤‡è£½ value_states.shape=torch.Size([1, 64, 69, 64]), value_states.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ­ã‚¸ãƒƒãƒˆã‚’è¨ˆç®— attn_weights.shape=torch.Size([1, 64, 69, 69]), attn_weights.dtype=torch.bfloat16
ğŸŸ¦ causal_mask.shape=torch.Size([1, 1, 69, 69]), causal_mask.dtype=torch.bfloat16
ğŸŸ¦ å› æœãƒã‚¹ã‚¯é©ç”¨ attn_weights.shape=torch.Size([1, 64, 69, 69]), attn_weights.dtype=torch.bfloat16
ğŸŸ¦ sinks.shape=torch.Size([1, 64, 69, 1]), sinks.dtype=torch.bfloat16
ğŸŸ¦ ã‚·ãƒ³ã‚¯ã‚’è¿½åŠ  combined_logits.shape=torch.Size([1, 64, 69, 70]), combined_logits.dtype=torch.bfloat16
ğŸŸ¦ æœ€å¤§å€¤ã‚’æ¸›ç®— combined_logits.shape=torch.Size([1, 64, 69, 70]), combined_logits.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®— probs.shape=torch.Size([1, 64, 69, 70]), probs.dtype=torch.bfloat16
ğŸŸ¦ ã‚·ãƒ³ã‚¯ã®ç¢ºç‡ã‚’é™¤å» scores.shape=torch.Size([1, 64, 69, 69]), scores.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ attn_output.shape=torch.Size([1, 64, 69, 64]), attn_output.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’è»¢ç½® attn_output.shape=torch.Size([1, 69, 64, 64]), attn_output.dtype=torch.bfloat16
ğŸŸ© eager_attention_forwardã®å®Œäº† attn_output.shape=torch.Size([1, 69, 64, 64]), attn_output.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ attn_output.shape=torch.Size([1, 69, 64, 64])
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’å…ƒã®å½¢çŠ¶ã«æˆ»ã™ attn_output.shape=torch.Size([1, 69, 4096])
ğŸŸ© GptOssAttentionã®é †ä¼æ’­å®Œäº† attn_output.shape=torch.Size([1, 69, 2880]), attn_output.dtype=torch.bfloat16, attn_weights.shape=torch.Size([1, 64, 69, 69]), attn_weights.dtype=torch.bfloat16
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒƒãƒ—ã‚­ãƒ£ã‚¹ãƒˆ input_dtype=torch.bfloat16 hidden_states.dtype=torch.float32
ğŸŸ© RMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 69, 2880]), res.dtype=torch.bfloat16
ğŸŸ¦ MoEå±¤ã®å…¥åŠ› hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16
ğŸŸ¦ MoEå±¤ã®å‡ºåŠ› hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16
ğŸŸ© GptOssDecoderLayerã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16, position_ids.shape=torch.Size([1, 69]), past_key_values=DynamicCache(layers=[DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer]), use_cache=True, cache_position.shape=torch.Size([69]), position_embeddings[0].shape=torch.Size([1, 69, 32]), position_embeddings[1].shape=torch.Size([1, 69, 32]), attention_mask.shape=torch.Size([1, 1, 69, 69])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒƒãƒ—ã‚­ãƒ£ã‚¹ãƒˆ input_dtype=torch.bfloat16 hidden_states.dtype=torch.float32
ğŸŸ© RMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 69, 2880]), res.dtype=torch.bfloat16
ğŸŸ© GptOssAttentionã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 69, 2880]), position_embeddings[0].shape=torch.Size([1, 69, 32]), position_embeddings[1].shape=torch.Size([1, 69, 32]), past_key_values=DynamicCache(layers=[DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer]), cache_position.shape=torch.Size([69]), attention_mask.shape=torch.Size([1, 1, 69, 69])
ğŸŸ¦ input_shape=torch.Size([1, 69])
ğŸŸ¦ hidden_shape=(1, 69, -1, 64)
ğŸŸ¦ ã‚¯ã‚¨ãƒªã‚’è¨ˆç®— query_states.shape=torch.Size([1, 64, 69, 64])
ğŸŸ¦ ã‚­ãƒ¼ã‚’è¨ˆç®— key_states.shape=torch.Size([1, 8, 69, 64])
ğŸŸ¦ ãƒãƒªãƒ¥ãƒ¼ã‚’è¨ˆç®— value_states.shape=torch.Size([1, 8, 69, 64])
ğŸŸ¦ RoPEã®cosã¨sinã‚’å–å¾— cos.shape=torch.Size([1, 69, 32]), sin.shape=torch.Size([1, 69, 32])
ğŸŸ¦ RoPEã‚’é©ç”¨ query_states.shape=torch.Size([1, 64, 69, 64]), key_states.shape=torch.Size([1, 8, 69, 64])
ğŸŸ¦ ã‚­ãƒ¼ãƒãƒªãƒ¥ãƒ¼ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æ›´æ–° key_states.shape=torch.Size([1, 8, 69, 64]), value_states.shape=torch.Size([1, 8, 69, 64])
ğŸŸ¦ ä½¿ç”¨ã™ã‚‹ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å®Ÿè£…: eager_attention_forward
ğŸŸ© eager_attention_forwardã®é–‹å§‹ query.shape=torch.Size([1, 64, 69, 64]), key.shape=torch.Size([1, 8, 69, 64]), value.shape=torch.Size([1, 8, 69, 64]), scaling=0.125, dropout=0.0 attention_mask.shape=torch.Size([1, 1, 69, 69])
ğŸŸ© repeat_kvã®é–‹å§‹ hidden_states.shape=torch.Size([1, 8, 69, 64]), hidden_states.dtype=torch.bfloat16, n_rep=8
ğŸŸ¦ hidden_states.shape=torch.Size([1, 8, 8, 69, 64]), hidden_states.dtype=torch.bfloat16
ğŸŸ© repeat_kvã®å®Œäº† res.shape=torch.Size([1, 64, 69, 64]), res.dtype=torch.bfloat16
ğŸŸ¦ ã‚­ãƒ¼ã‚’è¤‡è£½ key_states.shape=torch.Size([1, 64, 69, 64]), key_states.dtype=torch.bfloat16
ğŸŸ© repeat_kvã®é–‹å§‹ hidden_states.shape=torch.Size([1, 8, 69, 64]), hidden_states.dtype=torch.bfloat16, n_rep=8
ğŸŸ¦ hidden_states.shape=torch.Size([1, 8, 8, 69, 64]), hidden_states.dtype=torch.bfloat16
ğŸŸ© repeat_kvã®å®Œäº† res.shape=torch.Size([1, 64, 69, 64]), res.dtype=torch.bfloat16
ğŸŸ¦ ãƒãƒªãƒ¥ãƒ¼ã‚’è¤‡è£½ value_states.shape=torch.Size([1, 64, 69, 64]), value_states.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ­ã‚¸ãƒƒãƒˆã‚’è¨ˆç®— attn_weights.shape=torch.Size([1, 64, 69, 69]), attn_weights.dtype=torch.bfloat16
ğŸŸ¦ causal_mask.shape=torch.Size([1, 1, 69, 69]), causal_mask.dtype=torch.bfloat16
ğŸŸ¦ å› æœãƒã‚¹ã‚¯é©ç”¨ attn_weights.shape=torch.Size([1, 64, 69, 69]), attn_weights.dtype=torch.bfloat16
ğŸŸ¦ sinks.shape=torch.Size([1, 64, 69, 1]), sinks.dtype=torch.bfloat16
ğŸŸ¦ ã‚·ãƒ³ã‚¯ã‚’è¿½åŠ  combined_logits.shape=torch.Size([1, 64, 69, 70]), combined_logits.dtype=torch.bfloat16
ğŸŸ¦ æœ€å¤§å€¤ã‚’æ¸›ç®— combined_logits.shape=torch.Size([1, 64, 69, 70]), combined_logits.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®— probs.shape=torch.Size([1, 64, 69, 70]), probs.dtype=torch.bfloat16
ğŸŸ¦ ã‚·ãƒ³ã‚¯ã®ç¢ºç‡ã‚’é™¤å» scores.shape=torch.Size([1, 64, 69, 69]), scores.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ attn_output.shape=torch.Size([1, 64, 69, 64]), attn_output.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’è»¢ç½® attn_output.shape=torch.Size([1, 69, 64, 64]), attn_output.dtype=torch.bfloat16
ğŸŸ© eager_attention_forwardã®å®Œäº† attn_output.shape=torch.Size([1, 69, 64, 64]), attn_output.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ attn_output.shape=torch.Size([1, 69, 64, 64])
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’å…ƒã®å½¢çŠ¶ã«æˆ»ã™ attn_output.shape=torch.Size([1, 69, 4096])
ğŸŸ© GptOssAttentionã®é †ä¼æ’­å®Œäº† attn_output.shape=torch.Size([1, 69, 2880]), attn_output.dtype=torch.bfloat16, attn_weights.shape=torch.Size([1, 64, 69, 69]), attn_weights.dtype=torch.bfloat16
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒƒãƒ—ã‚­ãƒ£ã‚¹ãƒˆ input_dtype=torch.bfloat16 hidden_states.dtype=torch.float32
ğŸŸ© RMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 69, 2880]), res.dtype=torch.bfloat16
ğŸŸ¦ MoEå±¤ã®å…¥åŠ› hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16
ğŸŸ¦ MoEå±¤ã®å‡ºåŠ› hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16
ğŸŸ© GptOssDecoderLayerã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16, position_ids.shape=torch.Size([1, 69]), past_key_values=DynamicCache(layers=[DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer]), use_cache=True, cache_position.shape=torch.Size([69]), position_embeddings[0].shape=torch.Size([1, 69, 32]), position_embeddings[1].shape=torch.Size([1, 69, 32]), attention_mask.shape=torch.Size([1, 1, 69, 69])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒƒãƒ—ã‚­ãƒ£ã‚¹ãƒˆ input_dtype=torch.bfloat16 hidden_states.dtype=torch.float32
ğŸŸ© RMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 69, 2880]), res.dtype=torch.bfloat16
ğŸŸ© GptOssAttentionã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 69, 2880]), position_embeddings[0].shape=torch.Size([1, 69, 32]), position_embeddings[1].shape=torch.Size([1, 69, 32]), past_key_values=DynamicCache(layers=[DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer]), cache_position.shape=torch.Size([69]), attention_mask.shape=torch.Size([1, 1, 69, 69])
ğŸŸ¦ input_shape=torch.Size([1, 69])
ğŸŸ¦ hidden_shape=(1, 69, -1, 64)
ğŸŸ¦ ã‚¯ã‚¨ãƒªã‚’è¨ˆç®— query_states.shape=torch.Size([1, 64, 69, 64])
ğŸŸ¦ ã‚­ãƒ¼ã‚’è¨ˆç®— key_states.shape=torch.Size([1, 8, 69, 64])
ğŸŸ¦ ãƒãƒªãƒ¥ãƒ¼ã‚’è¨ˆç®— value_states.shape=torch.Size([1, 8, 69, 64])
ğŸŸ¦ RoPEã®cosã¨sinã‚’å–å¾— cos.shape=torch.Size([1, 69, 32]), sin.shape=torch.Size([1, 69, 32])
ğŸŸ¦ RoPEã‚’é©ç”¨ query_states.shape=torch.Size([1, 64, 69, 64]), key_states.shape=torch.Size([1, 8, 69, 64])
ğŸŸ¦ ã‚­ãƒ¼ãƒãƒªãƒ¥ãƒ¼ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æ›´æ–° key_states.shape=torch.Size([1, 8, 69, 64]), value_states.shape=torch.Size([1, 8, 69, 64])
ğŸŸ¦ ä½¿ç”¨ã™ã‚‹ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å®Ÿè£…: eager_attention_forward
ğŸŸ© eager_attention_forwardã®é–‹å§‹ query.shape=torch.Size([1, 64, 69, 64]), key.shape=torch.Size([1, 8, 69, 64]), value.shape=torch.Size([1, 8, 69, 64]), scaling=0.125, dropout=0.0 attention_mask.shape=torch.Size([1, 1, 69, 69])
ğŸŸ© repeat_kvã®é–‹å§‹ hidden_states.shape=torch.Size([1, 8, 69, 64]), hidden_states.dtype=torch.bfloat16, n_rep=8
ğŸŸ¦ hidden_states.shape=torch.Size([1, 8, 8, 69, 64]), hidden_states.dtype=torch.bfloat16
ğŸŸ© repeat_kvã®å®Œäº† res.shape=torch.Size([1, 64, 69, 64]), res.dtype=torch.bfloat16
ğŸŸ¦ ã‚­ãƒ¼ã‚’è¤‡è£½ key_states.shape=torch.Size([1, 64, 69, 64]), key_states.dtype=torch.bfloat16
ğŸŸ© repeat_kvã®é–‹å§‹ hidden_states.shape=torch.Size([1, 8, 69, 64]), hidden_states.dtype=torch.bfloat16, n_rep=8
ğŸŸ¦ hidden_states.shape=torch.Size([1, 8, 8, 69, 64]), hidden_states.dtype=torch.bfloat16
ğŸŸ© repeat_kvã®å®Œäº† res.shape=torch.Size([1, 64, 69, 64]), res.dtype=torch.bfloat16
ğŸŸ¦ ãƒãƒªãƒ¥ãƒ¼ã‚’è¤‡è£½ value_states.shape=torch.Size([1, 64, 69, 64]), value_states.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ­ã‚¸ãƒƒãƒˆã‚’è¨ˆç®— attn_weights.shape=torch.Size([1, 64, 69, 69]), attn_weights.dtype=torch.bfloat16
ğŸŸ¦ causal_mask.shape=torch.Size([1, 1, 69, 69]), causal_mask.dtype=torch.bfloat16
ğŸŸ¦ å› æœãƒã‚¹ã‚¯é©ç”¨ attn_weights.shape=torch.Size([1, 64, 69, 69]), attn_weights.dtype=torch.bfloat16
ğŸŸ¦ sinks.shape=torch.Size([1, 64, 69, 1]), sinks.dtype=torch.bfloat16
ğŸŸ¦ ã‚·ãƒ³ã‚¯ã‚’è¿½åŠ  combined_logits.shape=torch.Size([1, 64, 69, 70]), combined_logits.dtype=torch.bfloat16
ğŸŸ¦ æœ€å¤§å€¤ã‚’æ¸›ç®— combined_logits.shape=torch.Size([1, 64, 69, 70]), combined_logits.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®— probs.shape=torch.Size([1, 64, 69, 70]), probs.dtype=torch.bfloat16
ğŸŸ¦ ã‚·ãƒ³ã‚¯ã®ç¢ºç‡ã‚’é™¤å» scores.shape=torch.Size([1, 64, 69, 69]), scores.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ attn_output.shape=torch.Size([1, 64, 69, 64]), attn_output.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’è»¢ç½® attn_output.shape=torch.Size([1, 69, 64, 64]), attn_output.dtype=torch.bfloat16
ğŸŸ© eager_attention_forwardã®å®Œäº† attn_output.shape=torch.Size([1, 69, 64, 64]), attn_output.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ attn_output.shape=torch.Size([1, 69, 64, 64])
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’å…ƒã®å½¢çŠ¶ã«æˆ»ã™ attn_output.shape=torch.Size([1, 69, 4096])
ğŸŸ© GptOssAttentionã®é †ä¼æ’­å®Œäº† attn_output.shape=torch.Size([1, 69, 2880]), attn_output.dtype=torch.bfloat16, attn_weights.shape=torch.Size([1, 64, 69, 69]), attn_weights.dtype=torch.bfloat16
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒƒãƒ—ã‚­ãƒ£ã‚¹ãƒˆ input_dtype=torch.bfloat16 hidden_states.dtype=torch.float32
ğŸŸ© RMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 69, 2880]), res.dtype=torch.bfloat16
ğŸŸ¦ MoEå±¤ã®å…¥åŠ› hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16
ğŸŸ¦ MoEå±¤ã®å‡ºåŠ› hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16
ğŸŸ© GptOssDecoderLayerã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16, position_ids.shape=torch.Size([1, 69]), past_key_values=DynamicCache(layers=[DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer]), use_cache=True, cache_position.shape=torch.Size([69]), position_embeddings[0].shape=torch.Size([1, 69, 32]), position_embeddings[1].shape=torch.Size([1, 69, 32]), attention_mask.shape=torch.Size([1, 1, 69, 69])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒƒãƒ—ã‚­ãƒ£ã‚¹ãƒˆ input_dtype=torch.bfloat16 hidden_states.dtype=torch.float32
ğŸŸ© RMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 69, 2880]), res.dtype=torch.bfloat16
ğŸŸ© GptOssAttentionã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 69, 2880]), position_embeddings[0].shape=torch.Size([1, 69, 32]), position_embeddings[1].shape=torch.Size([1, 69, 32]), past_key_values=DynamicCache(layers=[DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer]), cache_position.shape=torch.Size([69]), attention_mask.shape=torch.Size([1, 1, 69, 69])
ğŸŸ¦ input_shape=torch.Size([1, 69])
ğŸŸ¦ hidden_shape=(1, 69, -1, 64)
ğŸŸ¦ ã‚¯ã‚¨ãƒªã‚’è¨ˆç®— query_states.shape=torch.Size([1, 64, 69, 64])
ğŸŸ¦ ã‚­ãƒ¼ã‚’è¨ˆç®— key_states.shape=torch.Size([1, 8, 69, 64])
ğŸŸ¦ ãƒãƒªãƒ¥ãƒ¼ã‚’è¨ˆç®— value_states.shape=torch.Size([1, 8, 69, 64])
ğŸŸ¦ RoPEã®cosã¨sinã‚’å–å¾— cos.shape=torch.Size([1, 69, 32]), sin.shape=torch.Size([1, 69, 32])
ğŸŸ¦ RoPEã‚’é©ç”¨ query_states.shape=torch.Size([1, 64, 69, 64]), key_states.shape=torch.Size([1, 8, 69, 64])
ğŸŸ¦ ã‚­ãƒ¼ãƒãƒªãƒ¥ãƒ¼ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æ›´æ–° key_states.shape=torch.Size([1, 8, 69, 64]), value_states.shape=torch.Size([1, 8, 69, 64])
ğŸŸ¦ ä½¿ç”¨ã™ã‚‹ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å®Ÿè£…: eager_attention_forward
ğŸŸ© eager_attention_forwardã®é–‹å§‹ query.shape=torch.Size([1, 64, 69, 64]), key.shape=torch.Size([1, 8, 69, 64]), value.shape=torch.Size([1, 8, 69, 64]), scaling=0.125, dropout=0.0 attention_mask.shape=torch.Size([1, 1, 69, 69])
ğŸŸ© repeat_kvã®é–‹å§‹ hidden_states.shape=torch.Size([1, 8, 69, 64]), hidden_states.dtype=torch.bfloat16, n_rep=8
ğŸŸ¦ hidden_states.shape=torch.Size([1, 8, 8, 69, 64]), hidden_states.dtype=torch.bfloat16
ğŸŸ© repeat_kvã®å®Œäº† res.shape=torch.Size([1, 64, 69, 64]), res.dtype=torch.bfloat16
ğŸŸ¦ ã‚­ãƒ¼ã‚’è¤‡è£½ key_states.shape=torch.Size([1, 64, 69, 64]), key_states.dtype=torch.bfloat16
ğŸŸ© repeat_kvã®é–‹å§‹ hidden_states.shape=torch.Size([1, 8, 69, 64]), hidden_states.dtype=torch.bfloat16, n_rep=8
ğŸŸ¦ hidden_states.shape=torch.Size([1, 8, 8, 69, 64]), hidden_states.dtype=torch.bfloat16
ğŸŸ© repeat_kvã®å®Œäº† res.shape=torch.Size([1, 64, 69, 64]), res.dtype=torch.bfloat16
ğŸŸ¦ ãƒãƒªãƒ¥ãƒ¼ã‚’è¤‡è£½ value_states.shape=torch.Size([1, 64, 69, 64]), value_states.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ­ã‚¸ãƒƒãƒˆã‚’è¨ˆç®— attn_weights.shape=torch.Size([1, 64, 69, 69]), attn_weights.dtype=torch.bfloat16
ğŸŸ¦ causal_mask.shape=torch.Size([1, 1, 69, 69]), causal_mask.dtype=torch.bfloat16
ğŸŸ¦ å› æœãƒã‚¹ã‚¯é©ç”¨ attn_weights.shape=torch.Size([1, 64, 69, 69]), attn_weights.dtype=torch.bfloat16
ğŸŸ¦ sinks.shape=torch.Size([1, 64, 69, 1]), sinks.dtype=torch.bfloat16
ğŸŸ¦ ã‚·ãƒ³ã‚¯ã‚’è¿½åŠ  combined_logits.shape=torch.Size([1, 64, 69, 70]), combined_logits.dtype=torch.bfloat16
ğŸŸ¦ æœ€å¤§å€¤ã‚’æ¸›ç®— combined_logits.shape=torch.Size([1, 64, 69, 70]), combined_logits.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®— probs.shape=torch.Size([1, 64, 69, 70]), probs.dtype=torch.bfloat16
ğŸŸ¦ ã‚·ãƒ³ã‚¯ã®ç¢ºç‡ã‚’é™¤å» scores.shape=torch.Size([1, 64, 69, 69]), scores.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ attn_output.shape=torch.Size([1, 64, 69, 64]), attn_output.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’è»¢ç½® attn_output.shape=torch.Size([1, 69, 64, 64]), attn_output.dtype=torch.bfloat16
ğŸŸ© eager_attention_forwardã®å®Œäº† attn_output.shape=torch.Size([1, 69, 64, 64]), attn_output.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ attn_output.shape=torch.Size([1, 69, 64, 64])
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’å…ƒã®å½¢çŠ¶ã«æˆ»ã™ attn_output.shape=torch.Size([1, 69, 4096])
ğŸŸ© GptOssAttentionã®é †ä¼æ’­å®Œäº† attn_output.shape=torch.Size([1, 69, 2880]), attn_output.dtype=torch.bfloat16, attn_weights.shape=torch.Size([1, 64, 69, 69]), attn_weights.dtype=torch.bfloat16
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒƒãƒ—ã‚­ãƒ£ã‚¹ãƒˆ input_dtype=torch.bfloat16 hidden_states.dtype=torch.float32
ğŸŸ© RMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 69, 2880]), res.dtype=torch.bfloat16
ğŸŸ¦ MoEå±¤ã®å…¥åŠ› hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16
ğŸŸ¦ MoEå±¤ã®å‡ºåŠ› hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16
ğŸŸ© GptOssDecoderLayerã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16, position_ids.shape=torch.Size([1, 69]), past_key_values=DynamicCache(layers=[DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer]), use_cache=True, cache_position.shape=torch.Size([69]), position_embeddings[0].shape=torch.Size([1, 69, 32]), position_embeddings[1].shape=torch.Size([1, 69, 32]), attention_mask.shape=torch.Size([1, 1, 69, 69])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒƒãƒ—ã‚­ãƒ£ã‚¹ãƒˆ input_dtype=torch.bfloat16 hidden_states.dtype=torch.float32
ğŸŸ© RMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 69, 2880]), res.dtype=torch.bfloat16
ğŸŸ© GptOssAttentionã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 69, 2880]), position_embeddings[0].shape=torch.Size([1, 69, 32]), position_embeddings[1].shape=torch.Size([1, 69, 32]), past_key_values=DynamicCache(layers=[DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer]), cache_position.shape=torch.Size([69]), attention_mask.shape=torch.Size([1, 1, 69, 69])
ğŸŸ¦ input_shape=torch.Size([1, 69])
ğŸŸ¦ hidden_shape=(1, 69, -1, 64)
ğŸŸ¦ ã‚¯ã‚¨ãƒªã‚’è¨ˆç®— query_states.shape=torch.Size([1, 64, 69, 64])
ğŸŸ¦ ã‚­ãƒ¼ã‚’è¨ˆç®— key_states.shape=torch.Size([1, 8, 69, 64])
ğŸŸ¦ ãƒãƒªãƒ¥ãƒ¼ã‚’è¨ˆç®— value_states.shape=torch.Size([1, 8, 69, 64])
ğŸŸ¦ RoPEã®cosã¨sinã‚’å–å¾— cos.shape=torch.Size([1, 69, 32]), sin.shape=torch.Size([1, 69, 32])
ğŸŸ¦ RoPEã‚’é©ç”¨ query_states.shape=torch.Size([1, 64, 69, 64]), key_states.shape=torch.Size([1, 8, 69, 64])
ğŸŸ¦ ã‚­ãƒ¼ãƒãƒªãƒ¥ãƒ¼ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æ›´æ–° key_states.shape=torch.Size([1, 8, 69, 64]), value_states.shape=torch.Size([1, 8, 69, 64])
ğŸŸ¦ ä½¿ç”¨ã™ã‚‹ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å®Ÿè£…: eager_attention_forward
ğŸŸ© eager_attention_forwardã®é–‹å§‹ query.shape=torch.Size([1, 64, 69, 64]), key.shape=torch.Size([1, 8, 69, 64]), value.shape=torch.Size([1, 8, 69, 64]), scaling=0.125, dropout=0.0 attention_mask.shape=torch.Size([1, 1, 69, 69])
ğŸŸ© repeat_kvã®é–‹å§‹ hidden_states.shape=torch.Size([1, 8, 69, 64]), hidden_states.dtype=torch.bfloat16, n_rep=8
ğŸŸ¦ hidden_states.shape=torch.Size([1, 8, 8, 69, 64]), hidden_states.dtype=torch.bfloat16
ğŸŸ© repeat_kvã®å®Œäº† res.shape=torch.Size([1, 64, 69, 64]), res.dtype=torch.bfloat16
ğŸŸ¦ ã‚­ãƒ¼ã‚’è¤‡è£½ key_states.shape=torch.Size([1, 64, 69, 64]), key_states.dtype=torch.bfloat16
ğŸŸ© repeat_kvã®é–‹å§‹ hidden_states.shape=torch.Size([1, 8, 69, 64]), hidden_states.dtype=torch.bfloat16, n_rep=8
ğŸŸ¦ hidden_states.shape=torch.Size([1, 8, 8, 69, 64]), hidden_states.dtype=torch.bfloat16
ğŸŸ© repeat_kvã®å®Œäº† res.shape=torch.Size([1, 64, 69, 64]), res.dtype=torch.bfloat16
ğŸŸ¦ ãƒãƒªãƒ¥ãƒ¼ã‚’è¤‡è£½ value_states.shape=torch.Size([1, 64, 69, 64]), value_states.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ­ã‚¸ãƒƒãƒˆã‚’è¨ˆç®— attn_weights.shape=torch.Size([1, 64, 69, 69]), attn_weights.dtype=torch.bfloat16
ğŸŸ¦ causal_mask.shape=torch.Size([1, 1, 69, 69]), causal_mask.dtype=torch.bfloat16
ğŸŸ¦ å› æœãƒã‚¹ã‚¯é©ç”¨ attn_weights.shape=torch.Size([1, 64, 69, 69]), attn_weights.dtype=torch.bfloat16
ğŸŸ¦ sinks.shape=torch.Size([1, 64, 69, 1]), sinks.dtype=torch.bfloat16
ğŸŸ¦ ã‚·ãƒ³ã‚¯ã‚’è¿½åŠ  combined_logits.shape=torch.Size([1, 64, 69, 70]), combined_logits.dtype=torch.bfloat16
ğŸŸ¦ æœ€å¤§å€¤ã‚’æ¸›ç®— combined_logits.shape=torch.Size([1, 64, 69, 70]), combined_logits.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®— probs.shape=torch.Size([1, 64, 69, 70]), probs.dtype=torch.bfloat16
ğŸŸ¦ ã‚·ãƒ³ã‚¯ã®ç¢ºç‡ã‚’é™¤å» scores.shape=torch.Size([1, 64, 69, 69]), scores.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ attn_output.shape=torch.Size([1, 64, 69, 64]), attn_output.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’è»¢ç½® attn_output.shape=torch.Size([1, 69, 64, 64]), attn_output.dtype=torch.bfloat16
ğŸŸ© eager_attention_forwardã®å®Œäº† attn_output.shape=torch.Size([1, 69, 64, 64]), attn_output.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ attn_output.shape=torch.Size([1, 69, 64, 64])
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’å…ƒã®å½¢çŠ¶ã«æˆ»ã™ attn_output.shape=torch.Size([1, 69, 4096])
ğŸŸ© GptOssAttentionã®é †ä¼æ’­å®Œäº† attn_output.shape=torch.Size([1, 69, 2880]), attn_output.dtype=torch.bfloat16, attn_weights.shape=torch.Size([1, 64, 69, 69]), attn_weights.dtype=torch.bfloat16
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒƒãƒ—ã‚­ãƒ£ã‚¹ãƒˆ input_dtype=torch.bfloat16 hidden_states.dtype=torch.float32
ğŸŸ© RMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 69, 2880]), res.dtype=torch.bfloat16
ğŸŸ¦ MoEå±¤ã®å…¥åŠ› hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16
ğŸŸ¦ MoEå±¤ã®å‡ºåŠ› hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16
ğŸŸ© GptOssDecoderLayerã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16, position_ids.shape=torch.Size([1, 69]), past_key_values=DynamicCache(layers=[DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer]), use_cache=True, cache_position.shape=torch.Size([69]), position_embeddings[0].shape=torch.Size([1, 69, 32]), position_embeddings[1].shape=torch.Size([1, 69, 32]), attention_mask.shape=torch.Size([1, 1, 69, 69])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒƒãƒ—ã‚­ãƒ£ã‚¹ãƒˆ input_dtype=torch.bfloat16 hidden_states.dtype=torch.float32
ğŸŸ© RMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 69, 2880]), res.dtype=torch.bfloat16
ğŸŸ© GptOssAttentionã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 69, 2880]), position_embeddings[0].shape=torch.Size([1, 69, 32]), position_embeddings[1].shape=torch.Size([1, 69, 32]), past_key_values=DynamicCache(layers=[DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer]), cache_position.shape=torch.Size([69]), attention_mask.shape=torch.Size([1, 1, 69, 69])
ğŸŸ¦ input_shape=torch.Size([1, 69])
ğŸŸ¦ hidden_shape=(1, 69, -1, 64)
ğŸŸ¦ ã‚¯ã‚¨ãƒªã‚’è¨ˆç®— query_states.shape=torch.Size([1, 64, 69, 64])
ğŸŸ¦ ã‚­ãƒ¼ã‚’è¨ˆç®— key_states.shape=torch.Size([1, 8, 69, 64])
ğŸŸ¦ ãƒãƒªãƒ¥ãƒ¼ã‚’è¨ˆç®— value_states.shape=torch.Size([1, 8, 69, 64])
ğŸŸ¦ RoPEã®cosã¨sinã‚’å–å¾— cos.shape=torch.Size([1, 69, 32]), sin.shape=torch.Size([1, 69, 32])
ğŸŸ¦ RoPEã‚’é©ç”¨ query_states.shape=torch.Size([1, 64, 69, 64]), key_states.shape=torch.Size([1, 8, 69, 64])
ğŸŸ¦ ã‚­ãƒ¼ãƒãƒªãƒ¥ãƒ¼ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æ›´æ–° key_states.shape=torch.Size([1, 8, 69, 64]), value_states.shape=torch.Size([1, 8, 69, 64])
ğŸŸ¦ ä½¿ç”¨ã™ã‚‹ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å®Ÿè£…: eager_attention_forward
ğŸŸ© eager_attention_forwardã®é–‹å§‹ query.shape=torch.Size([1, 64, 69, 64]), key.shape=torch.Size([1, 8, 69, 64]), value.shape=torch.Size([1, 8, 69, 64]), scaling=0.125, dropout=0.0 attention_mask.shape=torch.Size([1, 1, 69, 69])
ğŸŸ© repeat_kvã®é–‹å§‹ hidden_states.shape=torch.Size([1, 8, 69, 64]), hidden_states.dtype=torch.bfloat16, n_rep=8
ğŸŸ¦ hidden_states.shape=torch.Size([1, 8, 8, 69, 64]), hidden_states.dtype=torch.bfloat16
ğŸŸ© repeat_kvã®å®Œäº† res.shape=torch.Size([1, 64, 69, 64]), res.dtype=torch.bfloat16
ğŸŸ¦ ã‚­ãƒ¼ã‚’è¤‡è£½ key_states.shape=torch.Size([1, 64, 69, 64]), key_states.dtype=torch.bfloat16
ğŸŸ© repeat_kvã®é–‹å§‹ hidden_states.shape=torch.Size([1, 8, 69, 64]), hidden_states.dtype=torch.bfloat16, n_rep=8
ğŸŸ¦ hidden_states.shape=torch.Size([1, 8, 8, 69, 64]), hidden_states.dtype=torch.bfloat16
ğŸŸ© repeat_kvã®å®Œäº† res.shape=torch.Size([1, 64, 69, 64]), res.dtype=torch.bfloat16
ğŸŸ¦ ãƒãƒªãƒ¥ãƒ¼ã‚’è¤‡è£½ value_states.shape=torch.Size([1, 64, 69, 64]), value_states.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ­ã‚¸ãƒƒãƒˆã‚’è¨ˆç®— attn_weights.shape=torch.Size([1, 64, 69, 69]), attn_weights.dtype=torch.bfloat16
ğŸŸ¦ causal_mask.shape=torch.Size([1, 1, 69, 69]), causal_mask.dtype=torch.bfloat16
ğŸŸ¦ å› æœãƒã‚¹ã‚¯é©ç”¨ attn_weights.shape=torch.Size([1, 64, 69, 69]), attn_weights.dtype=torch.bfloat16
ğŸŸ¦ sinks.shape=torch.Size([1, 64, 69, 1]), sinks.dtype=torch.bfloat16
ğŸŸ¦ ã‚·ãƒ³ã‚¯ã‚’è¿½åŠ  combined_logits.shape=torch.Size([1, 64, 69, 70]), combined_logits.dtype=torch.bfloat16
ğŸŸ¦ æœ€å¤§å€¤ã‚’æ¸›ç®— combined_logits.shape=torch.Size([1, 64, 69, 70]), combined_logits.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®— probs.shape=torch.Size([1, 64, 69, 70]), probs.dtype=torch.bfloat16
ğŸŸ¦ ã‚·ãƒ³ã‚¯ã®ç¢ºç‡ã‚’é™¤å» scores.shape=torch.Size([1, 64, 69, 69]), scores.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ attn_output.shape=torch.Size([1, 64, 69, 64]), attn_output.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’è»¢ç½® attn_output.shape=torch.Size([1, 69, 64, 64]), attn_output.dtype=torch.bfloat16
ğŸŸ© eager_attention_forwardã®å®Œäº† attn_output.shape=torch.Size([1, 69, 64, 64]), attn_output.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ attn_output.shape=torch.Size([1, 69, 64, 64])
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’å…ƒã®å½¢çŠ¶ã«æˆ»ã™ attn_output.shape=torch.Size([1, 69, 4096])
ğŸŸ© GptOssAttentionã®é †ä¼æ’­å®Œäº† attn_output.shape=torch.Size([1, 69, 2880]), attn_output.dtype=torch.bfloat16, attn_weights.shape=torch.Size([1, 64, 69, 69]), attn_weights.dtype=torch.bfloat16
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒƒãƒ—ã‚­ãƒ£ã‚¹ãƒˆ input_dtype=torch.bfloat16 hidden_states.dtype=torch.float32
ğŸŸ© RMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 69, 2880]), res.dtype=torch.bfloat16
ğŸŸ¦ MoEå±¤ã®å…¥åŠ› hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16
ğŸŸ¦ MoEå±¤ã®å‡ºåŠ› hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16
ğŸŸ© GptOssDecoderLayerã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16, position_ids.shape=torch.Size([1, 69]), past_key_values=DynamicCache(layers=[DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer]), use_cache=True, cache_position.shape=torch.Size([69]), position_embeddings[0].shape=torch.Size([1, 69, 32]), position_embeddings[1].shape=torch.Size([1, 69, 32]), attention_mask.shape=torch.Size([1, 1, 69, 69])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒƒãƒ—ã‚­ãƒ£ã‚¹ãƒˆ input_dtype=torch.bfloat16 hidden_states.dtype=torch.float32
ğŸŸ© RMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 69, 2880]), res.dtype=torch.bfloat16
ğŸŸ© GptOssAttentionã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 69, 2880]), position_embeddings[0].shape=torch.Size([1, 69, 32]), position_embeddings[1].shape=torch.Size([1, 69, 32]), past_key_values=DynamicCache(layers=[DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer]), cache_position.shape=torch.Size([69]), attention_mask.shape=torch.Size([1, 1, 69, 69])
ğŸŸ¦ input_shape=torch.Size([1, 69])
ğŸŸ¦ hidden_shape=(1, 69, -1, 64)
ğŸŸ¦ ã‚¯ã‚¨ãƒªã‚’è¨ˆç®— query_states.shape=torch.Size([1, 64, 69, 64])
ğŸŸ¦ ã‚­ãƒ¼ã‚’è¨ˆç®— key_states.shape=torch.Size([1, 8, 69, 64])
ğŸŸ¦ ãƒãƒªãƒ¥ãƒ¼ã‚’è¨ˆç®— value_states.shape=torch.Size([1, 8, 69, 64])
ğŸŸ¦ RoPEã®cosã¨sinã‚’å–å¾— cos.shape=torch.Size([1, 69, 32]), sin.shape=torch.Size([1, 69, 32])
ğŸŸ¦ RoPEã‚’é©ç”¨ query_states.shape=torch.Size([1, 64, 69, 64]), key_states.shape=torch.Size([1, 8, 69, 64])
ğŸŸ¦ ã‚­ãƒ¼ãƒãƒªãƒ¥ãƒ¼ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æ›´æ–° key_states.shape=torch.Size([1, 8, 69, 64]), value_states.shape=torch.Size([1, 8, 69, 64])
ğŸŸ¦ ä½¿ç”¨ã™ã‚‹ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å®Ÿè£…: eager_attention_forward
ğŸŸ© eager_attention_forwardã®é–‹å§‹ query.shape=torch.Size([1, 64, 69, 64]), key.shape=torch.Size([1, 8, 69, 64]), value.shape=torch.Size([1, 8, 69, 64]), scaling=0.125, dropout=0.0 attention_mask.shape=torch.Size([1, 1, 69, 69])
ğŸŸ© repeat_kvã®é–‹å§‹ hidden_states.shape=torch.Size([1, 8, 69, 64]), hidden_states.dtype=torch.bfloat16, n_rep=8
ğŸŸ¦ hidden_states.shape=torch.Size([1, 8, 8, 69, 64]), hidden_states.dtype=torch.bfloat16
ğŸŸ© repeat_kvã®å®Œäº† res.shape=torch.Size([1, 64, 69, 64]), res.dtype=torch.bfloat16
ğŸŸ¦ ã‚­ãƒ¼ã‚’è¤‡è£½ key_states.shape=torch.Size([1, 64, 69, 64]), key_states.dtype=torch.bfloat16
ğŸŸ© repeat_kvã®é–‹å§‹ hidden_states.shape=torch.Size([1, 8, 69, 64]), hidden_states.dtype=torch.bfloat16, n_rep=8
ğŸŸ¦ hidden_states.shape=torch.Size([1, 8, 8, 69, 64]), hidden_states.dtype=torch.bfloat16
ğŸŸ© repeat_kvã®å®Œäº† res.shape=torch.Size([1, 64, 69, 64]), res.dtype=torch.bfloat16
ğŸŸ¦ ãƒãƒªãƒ¥ãƒ¼ã‚’è¤‡è£½ value_states.shape=torch.Size([1, 64, 69, 64]), value_states.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ­ã‚¸ãƒƒãƒˆã‚’è¨ˆç®— attn_weights.shape=torch.Size([1, 64, 69, 69]), attn_weights.dtype=torch.bfloat16
ğŸŸ¦ causal_mask.shape=torch.Size([1, 1, 69, 69]), causal_mask.dtype=torch.bfloat16
ğŸŸ¦ å› æœãƒã‚¹ã‚¯é©ç”¨ attn_weights.shape=torch.Size([1, 64, 69, 69]), attn_weights.dtype=torch.bfloat16
ğŸŸ¦ sinks.shape=torch.Size([1, 64, 69, 1]), sinks.dtype=torch.bfloat16
ğŸŸ¦ ã‚·ãƒ³ã‚¯ã‚’è¿½åŠ  combined_logits.shape=torch.Size([1, 64, 69, 70]), combined_logits.dtype=torch.bfloat16
ğŸŸ¦ æœ€å¤§å€¤ã‚’æ¸›ç®— combined_logits.shape=torch.Size([1, 64, 69, 70]), combined_logits.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®— probs.shape=torch.Size([1, 64, 69, 70]), probs.dtype=torch.bfloat16
ğŸŸ¦ ã‚·ãƒ³ã‚¯ã®ç¢ºç‡ã‚’é™¤å» scores.shape=torch.Size([1, 64, 69, 69]), scores.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ attn_output.shape=torch.Size([1, 64, 69, 64]), attn_output.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’è»¢ç½® attn_output.shape=torch.Size([1, 69, 64, 64]), attn_output.dtype=torch.bfloat16
ğŸŸ© eager_attention_forwardã®å®Œäº† attn_output.shape=torch.Size([1, 69, 64, 64]), attn_output.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ attn_output.shape=torch.Size([1, 69, 64, 64])
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’å…ƒã®å½¢çŠ¶ã«æˆ»ã™ attn_output.shape=torch.Size([1, 69, 4096])
ğŸŸ© GptOssAttentionã®é †ä¼æ’­å®Œäº† attn_output.shape=torch.Size([1, 69, 2880]), attn_output.dtype=torch.bfloat16, attn_weights.shape=torch.Size([1, 64, 69, 69]), attn_weights.dtype=torch.bfloat16
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒƒãƒ—ã‚­ãƒ£ã‚¹ãƒˆ input_dtype=torch.bfloat16 hidden_states.dtype=torch.float32
ğŸŸ© RMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 69, 2880]), res.dtype=torch.bfloat16
ğŸŸ¦ MoEå±¤ã®å…¥åŠ› hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16
ğŸŸ¦ MoEå±¤ã®å‡ºåŠ› hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16
ğŸŸ© GptOssDecoderLayerã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16, position_ids.shape=torch.Size([1, 69]), past_key_values=DynamicCache(layers=[DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer]), use_cache=True, cache_position.shape=torch.Size([69]), position_embeddings[0].shape=torch.Size([1, 69, 32]), position_embeddings[1].shape=torch.Size([1, 69, 32]), attention_mask.shape=torch.Size([1, 1, 69, 69])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒƒãƒ—ã‚­ãƒ£ã‚¹ãƒˆ input_dtype=torch.bfloat16 hidden_states.dtype=torch.float32
ğŸŸ© RMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 69, 2880]), res.dtype=torch.bfloat16
ğŸŸ© GptOssAttentionã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 69, 2880]), position_embeddings[0].shape=torch.Size([1, 69, 32]), position_embeddings[1].shape=torch.Size([1, 69, 32]), past_key_values=DynamicCache(layers=[DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer]), cache_position.shape=torch.Size([69]), attention_mask.shape=torch.Size([1, 1, 69, 69])
ğŸŸ¦ input_shape=torch.Size([1, 69])
ğŸŸ¦ hidden_shape=(1, 69, -1, 64)
ğŸŸ¦ ã‚¯ã‚¨ãƒªã‚’è¨ˆç®— query_states.shape=torch.Size([1, 64, 69, 64])
ğŸŸ¦ ã‚­ãƒ¼ã‚’è¨ˆç®— key_states.shape=torch.Size([1, 8, 69, 64])
ğŸŸ¦ ãƒãƒªãƒ¥ãƒ¼ã‚’è¨ˆç®— value_states.shape=torch.Size([1, 8, 69, 64])
ğŸŸ¦ RoPEã®cosã¨sinã‚’å–å¾— cos.shape=torch.Size([1, 69, 32]), sin.shape=torch.Size([1, 69, 32])
ğŸŸ¦ RoPEã‚’é©ç”¨ query_states.shape=torch.Size([1, 64, 69, 64]), key_states.shape=torch.Size([1, 8, 69, 64])
ğŸŸ¦ ã‚­ãƒ¼ãƒãƒªãƒ¥ãƒ¼ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æ›´æ–° key_states.shape=torch.Size([1, 8, 69, 64]), value_states.shape=torch.Size([1, 8, 69, 64])
ğŸŸ¦ ä½¿ç”¨ã™ã‚‹ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å®Ÿè£…: eager_attention_forward
ğŸŸ© eager_attention_forwardã®é–‹å§‹ query.shape=torch.Size([1, 64, 69, 64]), key.shape=torch.Size([1, 8, 69, 64]), value.shape=torch.Size([1, 8, 69, 64]), scaling=0.125, dropout=0.0 attention_mask.shape=torch.Size([1, 1, 69, 69])
ğŸŸ© repeat_kvã®é–‹å§‹ hidden_states.shape=torch.Size([1, 8, 69, 64]), hidden_states.dtype=torch.bfloat16, n_rep=8
ğŸŸ¦ hidden_states.shape=torch.Size([1, 8, 8, 69, 64]), hidden_states.dtype=torch.bfloat16
ğŸŸ© repeat_kvã®å®Œäº† res.shape=torch.Size([1, 64, 69, 64]), res.dtype=torch.bfloat16
ğŸŸ¦ ã‚­ãƒ¼ã‚’è¤‡è£½ key_states.shape=torch.Size([1, 64, 69, 64]), key_states.dtype=torch.bfloat16
ğŸŸ© repeat_kvã®é–‹å§‹ hidden_states.shape=torch.Size([1, 8, 69, 64]), hidden_states.dtype=torch.bfloat16, n_rep=8
ğŸŸ¦ hidden_states.shape=torch.Size([1, 8, 8, 69, 64]), hidden_states.dtype=torch.bfloat16
ğŸŸ© repeat_kvã®å®Œäº† res.shape=torch.Size([1, 64, 69, 64]), res.dtype=torch.bfloat16
ğŸŸ¦ ãƒãƒªãƒ¥ãƒ¼ã‚’è¤‡è£½ value_states.shape=torch.Size([1, 64, 69, 64]), value_states.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ­ã‚¸ãƒƒãƒˆã‚’è¨ˆç®— attn_weights.shape=torch.Size([1, 64, 69, 69]), attn_weights.dtype=torch.bfloat16
ğŸŸ¦ causal_mask.shape=torch.Size([1, 1, 69, 69]), causal_mask.dtype=torch.bfloat16
ğŸŸ¦ å› æœãƒã‚¹ã‚¯é©ç”¨ attn_weights.shape=torch.Size([1, 64, 69, 69]), attn_weights.dtype=torch.bfloat16
ğŸŸ¦ sinks.shape=torch.Size([1, 64, 69, 1]), sinks.dtype=torch.bfloat16
ğŸŸ¦ ã‚·ãƒ³ã‚¯ã‚’è¿½åŠ  combined_logits.shape=torch.Size([1, 64, 69, 70]), combined_logits.dtype=torch.bfloat16
ğŸŸ¦ æœ€å¤§å€¤ã‚’æ¸›ç®— combined_logits.shape=torch.Size([1, 64, 69, 70]), combined_logits.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®— probs.shape=torch.Size([1, 64, 69, 70]), probs.dtype=torch.bfloat16
ğŸŸ¦ ã‚·ãƒ³ã‚¯ã®ç¢ºç‡ã‚’é™¤å» scores.shape=torch.Size([1, 64, 69, 69]), scores.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ attn_output.shape=torch.Size([1, 64, 69, 64]), attn_output.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’è»¢ç½® attn_output.shape=torch.Size([1, 69, 64, 64]), attn_output.dtype=torch.bfloat16
ğŸŸ© eager_attention_forwardã®å®Œäº† attn_output.shape=torch.Size([1, 69, 64, 64]), attn_output.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ attn_output.shape=torch.Size([1, 69, 64, 64])
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’å…ƒã®å½¢çŠ¶ã«æˆ»ã™ attn_output.shape=torch.Size([1, 69, 4096])
ğŸŸ© GptOssAttentionã®é †ä¼æ’­å®Œäº† attn_output.shape=torch.Size([1, 69, 2880]), attn_output.dtype=torch.bfloat16, attn_weights.shape=torch.Size([1, 64, 69, 69]), attn_weights.dtype=torch.bfloat16
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒƒãƒ—ã‚­ãƒ£ã‚¹ãƒˆ input_dtype=torch.bfloat16 hidden_states.dtype=torch.float32
ğŸŸ© RMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 69, 2880]), res.dtype=torch.bfloat16
ğŸŸ¦ MoEå±¤ã®å…¥åŠ› hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16
ğŸŸ¦ MoEå±¤ã®å‡ºåŠ› hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16
ğŸŸ© GptOssDecoderLayerã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16, position_ids.shape=torch.Size([1, 69]), past_key_values=DynamicCache(layers=[DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer]), use_cache=True, cache_position.shape=torch.Size([69]), position_embeddings[0].shape=torch.Size([1, 69, 32]), position_embeddings[1].shape=torch.Size([1, 69, 32]), attention_mask.shape=torch.Size([1, 1, 69, 69])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒƒãƒ—ã‚­ãƒ£ã‚¹ãƒˆ input_dtype=torch.bfloat16 hidden_states.dtype=torch.float32
ğŸŸ© RMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 69, 2880]), res.dtype=torch.bfloat16
ğŸŸ© GptOssAttentionã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 69, 2880]), position_embeddings[0].shape=torch.Size([1, 69, 32]), position_embeddings[1].shape=torch.Size([1, 69, 32]), past_key_values=DynamicCache(layers=[DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer]), cache_position.shape=torch.Size([69]), attention_mask.shape=torch.Size([1, 1, 69, 69])
ğŸŸ¦ input_shape=torch.Size([1, 69])
ğŸŸ¦ hidden_shape=(1, 69, -1, 64)
ğŸŸ¦ ã‚¯ã‚¨ãƒªã‚’è¨ˆç®— query_states.shape=torch.Size([1, 64, 69, 64])
ğŸŸ¦ ã‚­ãƒ¼ã‚’è¨ˆç®— key_states.shape=torch.Size([1, 8, 69, 64])
ğŸŸ¦ ãƒãƒªãƒ¥ãƒ¼ã‚’è¨ˆç®— value_states.shape=torch.Size([1, 8, 69, 64])
ğŸŸ¦ RoPEã®cosã¨sinã‚’å–å¾— cos.shape=torch.Size([1, 69, 32]), sin.shape=torch.Size([1, 69, 32])
ğŸŸ¦ RoPEã‚’é©ç”¨ query_states.shape=torch.Size([1, 64, 69, 64]), key_states.shape=torch.Size([1, 8, 69, 64])
ğŸŸ¦ ã‚­ãƒ¼ãƒãƒªãƒ¥ãƒ¼ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æ›´æ–° key_states.shape=torch.Size([1, 8, 69, 64]), value_states.shape=torch.Size([1, 8, 69, 64])
ğŸŸ¦ ä½¿ç”¨ã™ã‚‹ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å®Ÿè£…: eager_attention_forward
ğŸŸ© eager_attention_forwardã®é–‹å§‹ query.shape=torch.Size([1, 64, 69, 64]), key.shape=torch.Size([1, 8, 69, 64]), value.shape=torch.Size([1, 8, 69, 64]), scaling=0.125, dropout=0.0 attention_mask.shape=torch.Size([1, 1, 69, 69])
ğŸŸ© repeat_kvã®é–‹å§‹ hidden_states.shape=torch.Size([1, 8, 69, 64]), hidden_states.dtype=torch.bfloat16, n_rep=8
ğŸŸ¦ hidden_states.shape=torch.Size([1, 8, 8, 69, 64]), hidden_states.dtype=torch.bfloat16
ğŸŸ© repeat_kvã®å®Œäº† res.shape=torch.Size([1, 64, 69, 64]), res.dtype=torch.bfloat16
ğŸŸ¦ ã‚­ãƒ¼ã‚’è¤‡è£½ key_states.shape=torch.Size([1, 64, 69, 64]), key_states.dtype=torch.bfloat16
ğŸŸ© repeat_kvã®é–‹å§‹ hidden_states.shape=torch.Size([1, 8, 69, 64]), hidden_states.dtype=torch.bfloat16, n_rep=8
ğŸŸ¦ hidden_states.shape=torch.Size([1, 8, 8, 69, 64]), hidden_states.dtype=torch.bfloat16
ğŸŸ© repeat_kvã®å®Œäº† res.shape=torch.Size([1, 64, 69, 64]), res.dtype=torch.bfloat16
ğŸŸ¦ ãƒãƒªãƒ¥ãƒ¼ã‚’è¤‡è£½ value_states.shape=torch.Size([1, 64, 69, 64]), value_states.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ­ã‚¸ãƒƒãƒˆã‚’è¨ˆç®— attn_weights.shape=torch.Size([1, 64, 69, 69]), attn_weights.dtype=torch.bfloat16
ğŸŸ¦ causal_mask.shape=torch.Size([1, 1, 69, 69]), causal_mask.dtype=torch.bfloat16
ğŸŸ¦ å› æœãƒã‚¹ã‚¯é©ç”¨ attn_weights.shape=torch.Size([1, 64, 69, 69]), attn_weights.dtype=torch.bfloat16
ğŸŸ¦ sinks.shape=torch.Size([1, 64, 69, 1]), sinks.dtype=torch.bfloat16
ğŸŸ¦ ã‚·ãƒ³ã‚¯ã‚’è¿½åŠ  combined_logits.shape=torch.Size([1, 64, 69, 70]), combined_logits.dtype=torch.bfloat16
ğŸŸ¦ æœ€å¤§å€¤ã‚’æ¸›ç®— combined_logits.shape=torch.Size([1, 64, 69, 70]), combined_logits.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®— probs.shape=torch.Size([1, 64, 69, 70]), probs.dtype=torch.bfloat16
ğŸŸ¦ ã‚·ãƒ³ã‚¯ã®ç¢ºç‡ã‚’é™¤å» scores.shape=torch.Size([1, 64, 69, 69]), scores.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ attn_output.shape=torch.Size([1, 64, 69, 64]), attn_output.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’è»¢ç½® attn_output.shape=torch.Size([1, 69, 64, 64]), attn_output.dtype=torch.bfloat16
ğŸŸ© eager_attention_forwardã®å®Œäº† attn_output.shape=torch.Size([1, 69, 64, 64]), attn_output.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ attn_output.shape=torch.Size([1, 69, 64, 64])
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’å…ƒã®å½¢çŠ¶ã«æˆ»ã™ attn_output.shape=torch.Size([1, 69, 4096])
ğŸŸ© GptOssAttentionã®é †ä¼æ’­å®Œäº† attn_output.shape=torch.Size([1, 69, 2880]), attn_output.dtype=torch.bfloat16, attn_weights.shape=torch.Size([1, 64, 69, 69]), attn_weights.dtype=torch.bfloat16
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒƒãƒ—ã‚­ãƒ£ã‚¹ãƒˆ input_dtype=torch.bfloat16 hidden_states.dtype=torch.float32
ğŸŸ© RMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 69, 2880]), res.dtype=torch.bfloat16
ğŸŸ¦ MoEå±¤ã®å…¥åŠ› hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16
ğŸŸ¦ MoEå±¤ã®å‡ºåŠ› hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16
ğŸŸ© GptOssDecoderLayerã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16, position_ids.shape=torch.Size([1, 69]), past_key_values=DynamicCache(layers=[DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer]), use_cache=True, cache_position.shape=torch.Size([69]), position_embeddings[0].shape=torch.Size([1, 69, 32]), position_embeddings[1].shape=torch.Size([1, 69, 32]), attention_mask.shape=torch.Size([1, 1, 69, 69])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒƒãƒ—ã‚­ãƒ£ã‚¹ãƒˆ input_dtype=torch.bfloat16 hidden_states.dtype=torch.float32
ğŸŸ© RMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 69, 2880]), res.dtype=torch.bfloat16
ğŸŸ© GptOssAttentionã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 69, 2880]), position_embeddings[0].shape=torch.Size([1, 69, 32]), position_embeddings[1].shape=torch.Size([1, 69, 32]), past_key_values=DynamicCache(layers=[DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer]), cache_position.shape=torch.Size([69]), attention_mask.shape=torch.Size([1, 1, 69, 69])
ğŸŸ¦ input_shape=torch.Size([1, 69])
ğŸŸ¦ hidden_shape=(1, 69, -1, 64)
ğŸŸ¦ ã‚¯ã‚¨ãƒªã‚’è¨ˆç®— query_states.shape=torch.Size([1, 64, 69, 64])
ğŸŸ¦ ã‚­ãƒ¼ã‚’è¨ˆç®— key_states.shape=torch.Size([1, 8, 69, 64])
ğŸŸ¦ ãƒãƒªãƒ¥ãƒ¼ã‚’è¨ˆç®— value_states.shape=torch.Size([1, 8, 69, 64])
ğŸŸ¦ RoPEã®cosã¨sinã‚’å–å¾— cos.shape=torch.Size([1, 69, 32]), sin.shape=torch.Size([1, 69, 32])
ğŸŸ¦ RoPEã‚’é©ç”¨ query_states.shape=torch.Size([1, 64, 69, 64]), key_states.shape=torch.Size([1, 8, 69, 64])
ğŸŸ¦ ã‚­ãƒ¼ãƒãƒªãƒ¥ãƒ¼ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æ›´æ–° key_states.shape=torch.Size([1, 8, 69, 64]), value_states.shape=torch.Size([1, 8, 69, 64])
ğŸŸ¦ ä½¿ç”¨ã™ã‚‹ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å®Ÿè£…: eager_attention_forward
ğŸŸ© eager_attention_forwardã®é–‹å§‹ query.shape=torch.Size([1, 64, 69, 64]), key.shape=torch.Size([1, 8, 69, 64]), value.shape=torch.Size([1, 8, 69, 64]), scaling=0.125, dropout=0.0 attention_mask.shape=torch.Size([1, 1, 69, 69])
ğŸŸ© repeat_kvã®é–‹å§‹ hidden_states.shape=torch.Size([1, 8, 69, 64]), hidden_states.dtype=torch.bfloat16, n_rep=8
ğŸŸ¦ hidden_states.shape=torch.Size([1, 8, 8, 69, 64]), hidden_states.dtype=torch.bfloat16
ğŸŸ© repeat_kvã®å®Œäº† res.shape=torch.Size([1, 64, 69, 64]), res.dtype=torch.bfloat16
ğŸŸ¦ ã‚­ãƒ¼ã‚’è¤‡è£½ key_states.shape=torch.Size([1, 64, 69, 64]), key_states.dtype=torch.bfloat16
ğŸŸ© repeat_kvã®é–‹å§‹ hidden_states.shape=torch.Size([1, 8, 69, 64]), hidden_states.dtype=torch.bfloat16, n_rep=8
ğŸŸ¦ hidden_states.shape=torch.Size([1, 8, 8, 69, 64]), hidden_states.dtype=torch.bfloat16
ğŸŸ© repeat_kvã®å®Œäº† res.shape=torch.Size([1, 64, 69, 64]), res.dtype=torch.bfloat16
ğŸŸ¦ ãƒãƒªãƒ¥ãƒ¼ã‚’è¤‡è£½ value_states.shape=torch.Size([1, 64, 69, 64]), value_states.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ­ã‚¸ãƒƒãƒˆã‚’è¨ˆç®— attn_weights.shape=torch.Size([1, 64, 69, 69]), attn_weights.dtype=torch.bfloat16
ğŸŸ¦ causal_mask.shape=torch.Size([1, 1, 69, 69]), causal_mask.dtype=torch.bfloat16
ğŸŸ¦ å› æœãƒã‚¹ã‚¯é©ç”¨ attn_weights.shape=torch.Size([1, 64, 69, 69]), attn_weights.dtype=torch.bfloat16
ğŸŸ¦ sinks.shape=torch.Size([1, 64, 69, 1]), sinks.dtype=torch.bfloat16
ğŸŸ¦ ã‚·ãƒ³ã‚¯ã‚’è¿½åŠ  combined_logits.shape=torch.Size([1, 64, 69, 70]), combined_logits.dtype=torch.bfloat16
ğŸŸ¦ æœ€å¤§å€¤ã‚’æ¸›ç®— combined_logits.shape=torch.Size([1, 64, 69, 70]), combined_logits.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®— probs.shape=torch.Size([1, 64, 69, 70]), probs.dtype=torch.bfloat16
ğŸŸ¦ ã‚·ãƒ³ã‚¯ã®ç¢ºç‡ã‚’é™¤å» scores.shape=torch.Size([1, 64, 69, 69]), scores.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ attn_output.shape=torch.Size([1, 64, 69, 64]), attn_output.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’è»¢ç½® attn_output.shape=torch.Size([1, 69, 64, 64]), attn_output.dtype=torch.bfloat16
ğŸŸ© eager_attention_forwardã®å®Œäº† attn_output.shape=torch.Size([1, 69, 64, 64]), attn_output.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ attn_output.shape=torch.Size([1, 69, 64, 64])
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’å…ƒã®å½¢çŠ¶ã«æˆ»ã™ attn_output.shape=torch.Size([1, 69, 4096])
ğŸŸ© GptOssAttentionã®é †ä¼æ’­å®Œäº† attn_output.shape=torch.Size([1, 69, 2880]), attn_output.dtype=torch.bfloat16, attn_weights.shape=torch.Size([1, 64, 69, 69]), attn_weights.dtype=torch.bfloat16
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒƒãƒ—ã‚­ãƒ£ã‚¹ãƒˆ input_dtype=torch.bfloat16 hidden_states.dtype=torch.float32
ğŸŸ© RMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 69, 2880]), res.dtype=torch.bfloat16
ğŸŸ¦ MoEå±¤ã®å…¥åŠ› hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16
ğŸŸ¦ MoEå±¤ã®å‡ºåŠ› hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16
ğŸŸ© GptOssDecoderLayerã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16, position_ids.shape=torch.Size([1, 69]), past_key_values=DynamicCache(layers=[DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer]), use_cache=True, cache_position.shape=torch.Size([69]), position_embeddings[0].shape=torch.Size([1, 69, 32]), position_embeddings[1].shape=torch.Size([1, 69, 32]), attention_mask.shape=torch.Size([1, 1, 69, 69])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒƒãƒ—ã‚­ãƒ£ã‚¹ãƒˆ input_dtype=torch.bfloat16 hidden_states.dtype=torch.float32
ğŸŸ© RMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 69, 2880]), res.dtype=torch.bfloat16
ğŸŸ© GptOssAttentionã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 69, 2880]), position_embeddings[0].shape=torch.Size([1, 69, 32]), position_embeddings[1].shape=torch.Size([1, 69, 32]), past_key_values=DynamicCache(layers=[DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer]), cache_position.shape=torch.Size([69]), attention_mask.shape=torch.Size([1, 1, 69, 69])
ğŸŸ¦ input_shape=torch.Size([1, 69])
ğŸŸ¦ hidden_shape=(1, 69, -1, 64)
ğŸŸ¦ ã‚¯ã‚¨ãƒªã‚’è¨ˆç®— query_states.shape=torch.Size([1, 64, 69, 64])
ğŸŸ¦ ã‚­ãƒ¼ã‚’è¨ˆç®— key_states.shape=torch.Size([1, 8, 69, 64])
ğŸŸ¦ ãƒãƒªãƒ¥ãƒ¼ã‚’è¨ˆç®— value_states.shape=torch.Size([1, 8, 69, 64])
ğŸŸ¦ RoPEã®cosã¨sinã‚’å–å¾— cos.shape=torch.Size([1, 69, 32]), sin.shape=torch.Size([1, 69, 32])
ğŸŸ¦ RoPEã‚’é©ç”¨ query_states.shape=torch.Size([1, 64, 69, 64]), key_states.shape=torch.Size([1, 8, 69, 64])
ğŸŸ¦ ã‚­ãƒ¼ãƒãƒªãƒ¥ãƒ¼ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æ›´æ–° key_states.shape=torch.Size([1, 8, 69, 64]), value_states.shape=torch.Size([1, 8, 69, 64])
ğŸŸ¦ ä½¿ç”¨ã™ã‚‹ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å®Ÿè£…: eager_attention_forward
ğŸŸ© eager_attention_forwardã®é–‹å§‹ query.shape=torch.Size([1, 64, 69, 64]), key.shape=torch.Size([1, 8, 69, 64]), value.shape=torch.Size([1, 8, 69, 64]), scaling=0.125, dropout=0.0 attention_mask.shape=torch.Size([1, 1, 69, 69])
ğŸŸ© repeat_kvã®é–‹å§‹ hidden_states.shape=torch.Size([1, 8, 69, 64]), hidden_states.dtype=torch.bfloat16, n_rep=8
ğŸŸ¦ hidden_states.shape=torch.Size([1, 8, 8, 69, 64]), hidden_states.dtype=torch.bfloat16
ğŸŸ© repeat_kvã®å®Œäº† res.shape=torch.Size([1, 64, 69, 64]), res.dtype=torch.bfloat16
ğŸŸ¦ ã‚­ãƒ¼ã‚’è¤‡è£½ key_states.shape=torch.Size([1, 64, 69, 64]), key_states.dtype=torch.bfloat16
ğŸŸ© repeat_kvã®é–‹å§‹ hidden_states.shape=torch.Size([1, 8, 69, 64]), hidden_states.dtype=torch.bfloat16, n_rep=8
ğŸŸ¦ hidden_states.shape=torch.Size([1, 8, 8, 69, 64]), hidden_states.dtype=torch.bfloat16
ğŸŸ© repeat_kvã®å®Œäº† res.shape=torch.Size([1, 64, 69, 64]), res.dtype=torch.bfloat16
ğŸŸ¦ ãƒãƒªãƒ¥ãƒ¼ã‚’è¤‡è£½ value_states.shape=torch.Size([1, 64, 69, 64]), value_states.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ­ã‚¸ãƒƒãƒˆã‚’è¨ˆç®— attn_weights.shape=torch.Size([1, 64, 69, 69]), attn_weights.dtype=torch.bfloat16
ğŸŸ¦ causal_mask.shape=torch.Size([1, 1, 69, 69]), causal_mask.dtype=torch.bfloat16
ğŸŸ¦ å› æœãƒã‚¹ã‚¯é©ç”¨ attn_weights.shape=torch.Size([1, 64, 69, 69]), attn_weights.dtype=torch.bfloat16
ğŸŸ¦ sinks.shape=torch.Size([1, 64, 69, 1]), sinks.dtype=torch.bfloat16
ğŸŸ¦ ã‚·ãƒ³ã‚¯ã‚’è¿½åŠ  combined_logits.shape=torch.Size([1, 64, 69, 70]), combined_logits.dtype=torch.bfloat16
ğŸŸ¦ æœ€å¤§å€¤ã‚’æ¸›ç®— combined_logits.shape=torch.Size([1, 64, 69, 70]), combined_logits.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®— probs.shape=torch.Size([1, 64, 69, 70]), probs.dtype=torch.bfloat16
ğŸŸ¦ ã‚·ãƒ³ã‚¯ã®ç¢ºç‡ã‚’é™¤å» scores.shape=torch.Size([1, 64, 69, 69]), scores.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ attn_output.shape=torch.Size([1, 64, 69, 64]), attn_output.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’è»¢ç½® attn_output.shape=torch.Size([1, 69, 64, 64]), attn_output.dtype=torch.bfloat16
ğŸŸ© eager_attention_forwardã®å®Œäº† attn_output.shape=torch.Size([1, 69, 64, 64]), attn_output.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ attn_output.shape=torch.Size([1, 69, 64, 64])
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’å…ƒã®å½¢çŠ¶ã«æˆ»ã™ attn_output.shape=torch.Size([1, 69, 4096])
ğŸŸ© GptOssAttentionã®é †ä¼æ’­å®Œäº† attn_output.shape=torch.Size([1, 69, 2880]), attn_output.dtype=torch.bfloat16, attn_weights.shape=torch.Size([1, 64, 69, 69]), attn_weights.dtype=torch.bfloat16
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒƒãƒ—ã‚­ãƒ£ã‚¹ãƒˆ input_dtype=torch.bfloat16 hidden_states.dtype=torch.float32
ğŸŸ© RMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 69, 2880]), res.dtype=torch.bfloat16
ğŸŸ¦ MoEå±¤ã®å…¥åŠ› hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16
ğŸŸ¦ MoEå±¤ã®å‡ºåŠ› hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16
ğŸŸ© GptOssDecoderLayerã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16, position_ids.shape=torch.Size([1, 69]), past_key_values=DynamicCache(layers=[DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer]), use_cache=True, cache_position.shape=torch.Size([69]), position_embeddings[0].shape=torch.Size([1, 69, 32]), position_embeddings[1].shape=torch.Size([1, 69, 32]), attention_mask.shape=torch.Size([1, 1, 69, 69])
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒƒãƒ—ã‚­ãƒ£ã‚¹ãƒˆ input_dtype=torch.bfloat16 hidden_states.dtype=torch.float32
ğŸŸ© RMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 69, 2880]), res.dtype=torch.bfloat16
ğŸŸ© GptOssAttentionã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 69, 2880]), position_embeddings[0].shape=torch.Size([1, 69, 32]), position_embeddings[1].shape=torch.Size([1, 69, 32]), past_key_values=DynamicCache(layers=[DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer]), cache_position.shape=torch.Size([69]), attention_mask.shape=torch.Size([1, 1, 69, 69])
ğŸŸ¦ input_shape=torch.Size([1, 69])
ğŸŸ¦ hidden_shape=(1, 69, -1, 64)
ğŸŸ¦ ã‚¯ã‚¨ãƒªã‚’è¨ˆç®— query_states.shape=torch.Size([1, 64, 69, 64])
ğŸŸ¦ ã‚­ãƒ¼ã‚’è¨ˆç®— key_states.shape=torch.Size([1, 8, 69, 64])
ğŸŸ¦ ãƒãƒªãƒ¥ãƒ¼ã‚’è¨ˆç®— value_states.shape=torch.Size([1, 8, 69, 64])
ğŸŸ¦ RoPEã®cosã¨sinã‚’å–å¾— cos.shape=torch.Size([1, 69, 32]), sin.shape=torch.Size([1, 69, 32])
ğŸŸ¦ RoPEã‚’é©ç”¨ query_states.shape=torch.Size([1, 64, 69, 64]), key_states.shape=torch.Size([1, 8, 69, 64])
ğŸŸ¦ ã‚­ãƒ¼ãƒãƒªãƒ¥ãƒ¼ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æ›´æ–° key_states.shape=torch.Size([1, 8, 69, 64]), value_states.shape=torch.Size([1, 8, 69, 64])
ğŸŸ¦ ä½¿ç”¨ã™ã‚‹ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å®Ÿè£…: eager_attention_forward
ğŸŸ© eager_attention_forwardã®é–‹å§‹ query.shape=torch.Size([1, 64, 69, 64]), key.shape=torch.Size([1, 8, 69, 64]), value.shape=torch.Size([1, 8, 69, 64]), scaling=0.125, dropout=0.0 attention_mask.shape=torch.Size([1, 1, 69, 69])
ğŸŸ© repeat_kvã®é–‹å§‹ hidden_states.shape=torch.Size([1, 8, 69, 64]), hidden_states.dtype=torch.bfloat16, n_rep=8
ğŸŸ¦ hidden_states.shape=torch.Size([1, 8, 8, 69, 64]), hidden_states.dtype=torch.bfloat16
ğŸŸ© repeat_kvã®å®Œäº† res.shape=torch.Size([1, 64, 69, 64]), res.dtype=torch.bfloat16
ğŸŸ¦ ã‚­ãƒ¼ã‚’è¤‡è£½ key_states.shape=torch.Size([1, 64, 69, 64]), key_states.dtype=torch.bfloat16
ğŸŸ© repeat_kvã®é–‹å§‹ hidden_states.shape=torch.Size([1, 8, 69, 64]), hidden_states.dtype=torch.bfloat16, n_rep=8
ğŸŸ¦ hidden_states.shape=torch.Size([1, 8, 8, 69, 64]), hidden_states.dtype=torch.bfloat16
ğŸŸ© repeat_kvã®å®Œäº† res.shape=torch.Size([1, 64, 69, 64]), res.dtype=torch.bfloat16
ğŸŸ¦ ãƒãƒªãƒ¥ãƒ¼ã‚’è¤‡è£½ value_states.shape=torch.Size([1, 64, 69, 64]), value_states.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ­ã‚¸ãƒƒãƒˆã‚’è¨ˆç®— attn_weights.shape=torch.Size([1, 64, 69, 69]), attn_weights.dtype=torch.bfloat16
ğŸŸ¦ causal_mask.shape=torch.Size([1, 1, 69, 69]), causal_mask.dtype=torch.bfloat16
ğŸŸ¦ å› æœãƒã‚¹ã‚¯é©ç”¨ attn_weights.shape=torch.Size([1, 64, 69, 69]), attn_weights.dtype=torch.bfloat16
ğŸŸ¦ sinks.shape=torch.Size([1, 64, 69, 1]), sinks.dtype=torch.bfloat16
ğŸŸ¦ ã‚·ãƒ³ã‚¯ã‚’è¿½åŠ  combined_logits.shape=torch.Size([1, 64, 69, 70]), combined_logits.dtype=torch.bfloat16
ğŸŸ¦ æœ€å¤§å€¤ã‚’æ¸›ç®— combined_logits.shape=torch.Size([1, 64, 69, 70]), combined_logits.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®— probs.shape=torch.Size([1, 64, 69, 70]), probs.dtype=torch.bfloat16
ğŸŸ¦ ã‚·ãƒ³ã‚¯ã®ç¢ºç‡ã‚’é™¤å» scores.shape=torch.Size([1, 64, 69, 69]), scores.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ attn_output.shape=torch.Size([1, 64, 69, 64]), attn_output.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’è»¢ç½® attn_output.shape=torch.Size([1, 69, 64, 64]), attn_output.dtype=torch.bfloat16
ğŸŸ© eager_attention_forwardã®å®Œäº† attn_output.shape=torch.Size([1, 69, 64, 64]), attn_output.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ attn_output.shape=torch.Size([1, 69, 64, 64])
ğŸŸ¦ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’å…ƒã®å½¢çŠ¶ã«æˆ»ã™ attn_output.shape=torch.Size([1, 69, 4096])
ğŸŸ© GptOssAttentionã®é †ä¼æ’­å®Œäº† attn_output.shape=torch.Size([1, 69, 2880]), attn_output.dtype=torch.bfloat16, attn_weights.shape=torch.Size([1, 64, 69, 69]), attn_weights.dtype=torch.bfloat16
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒƒãƒ—ã‚­ãƒ£ã‚¹ãƒˆ input_dtype=torch.bfloat16 hidden_states.dtype=torch.float32
ğŸŸ© RMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 69, 2880]), res.dtype=torch.bfloat16
ğŸŸ¦ MoEå±¤ã®å…¥åŠ› hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16
ğŸŸ¦ MoEå±¤ã®å‡ºåŠ› hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16
ğŸŸ© RMSNormã®é †ä¼æ’­é–‹å§‹ hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16
ğŸŸ¦ ã‚¢ãƒƒãƒ—ã‚­ãƒ£ã‚¹ãƒˆ input_dtype=torch.bfloat16 hidden_states.dtype=torch.float32
ğŸŸ© RMSNormã®é †ä¼æ’­å®Œäº† res.shape=torch.Size([1, 69, 2880]), res.dtype=torch.bfloat16
ğŸŸ© GptOssModelã®é †ä¼æ’­å®Œäº† hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16
ğŸŸ¦ æœ€çµ‚éš ã‚ŒçŠ¶æ…‹ã‚’å–å¾— hidden_states.shape=torch.Size([1, 69, 2880]), hidden_states.dtype=torch.bfloat16
ğŸŸ¦ ã‚¹ãƒ©ã‚¤ã‚¹ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’è¨ˆç®— slice_indices=slice(-1, None, None)
ğŸŸ¦ ãƒ­ã‚¸ãƒƒãƒˆã‚’è¨ˆç®— logits.shape=torch.Size([1, 1, 201088]), logits.dtype=torch.bfloat16
